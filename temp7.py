
import os
import csv
import requests
import pandas as pd
import numpy as np
import time
import math
import matplotlib.pyplot as plt
from datetime import datetime
import pytz
import json
from typing import Dict, List, Tuple, Any, Optional, Union
import traceback
from concurrent.futures import ThreadPoolExecutor
import warnings
import logging
from functools import wraps
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from scipy.signal import find_peaks

# ===== LOGGING CONFIGURATION =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crypto_analyzer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Suppress specific pandas warnings
warnings.filterwarnings("ignore", category=pd.errors.SettingWithCopyWarning)

class CryptoTradeAnalyzer:
    def __init__(self, min_success_rate=70, api_key: Optional[str] = None):
        """
        Initialize the CryptoTradeAnalyzer with API key and default settings

        Args:
            min_success_rate: Minimum success rate percentage for signals (default: 70)
            api_key: CryptoCompare API key. If not provided, will try to load from
                    environment variable CRYPTOCOMPARE_API_KEY

        Raises:
            ValueError: If API key is not provided and not found in environment

        Security Note:
            NEVER hardcode API keys in source code. Use environment variables instead.
            Set CRYPTOCOMPARE_API_KEY in your environment or pass it to this constructor.
        """
        # ===== SECURE API KEY MANAGEMENT =====
        self.api_key = api_key or os.getenv('CRYPTOCOMPARE_API_KEY')

        if not self.api_key:
            error_msg = (
                "API key not found! Please provide API key via:\n"
                "  1. Environment variable: export CRYPTOCOMPARE_API_KEY='your_key'\n"
                "  2. Constructor parameter: CryptoTradeAnalyzer(api_key='your_key')\n"
                "Get your free API key at: https://min-api.cryptocompare.com/"
            )
            logger.error(error_msg)
            raise ValueError(error_msg)

        if len(self.api_key) < 16:
            logger.warning("API key seems too short. Please verify it's correct.")

        logger.info("CryptoTradeAnalyzer initialized successfully")

        self.min_success_rate = min_success_rate
        # List of crypto symbols to analyze as specified
        self.symbols = [
            "BTC", "ETH", "ETC", "XRP", "SOL", "BNB", "DOGE", "ADA", "EOS", "TRX",
            "AVAX", "LINK", "XLM", "HBAR", "SHIB", "TON", "DOT", "LTC", "BCH", "UNI",
            "S", "AXS", "NEAR", "APT", "AAVE", "POL", "XMR", "RENDER", "FIL", "PEPE",
            "GMT", "ATOM", "SAND", "FLOKI", "APE", "CAKE", "CATI", "CVX", "XAUT", "CHZ",
            "CRV", "LDO", "DYDX", "API3", "ONE", "STORJ", "SNT", "ZRX", "SLP", "T", "GRASS",
            "ARB", "WLD", "X", "WIF", "CELR", "FET", "PENGU", "ALGO", "VET", "OP", "INJ", 
            "ICP", "SEI", "SUI", "ENA", "JUP", "PUMP", "TURBO", "MOG", "HYPE", "PYTH", "FORM"   
        ]

        # self.symbols = [
        #     "EOS", "ETH", "ARB", "AAVE", "RENDER", "XMR", "LINK" 
        # ]
       

        # List of top 150 crypto symbols
        # self.symbols = [
        #     "BTC", "ETH", "USDT", "BNB", "SOL", "XRP", "USDC", "DOGE", "ADA", "TRX",
        #     "TON", "AVAX", "SHIB", "LINK", "DOT", "BCH", "LTC", "NEAR", "LEO", "DAI",
        #     "MATIC", "PEPE", "APT", "UNI", "ICP", "XLM", "ETC", "XMR", "FET", "HBAR",
        #     "RNDR", "CRO", "OKB", "ARB", "IMX", "INJ", "SUI", "MNT", "AAVE", "OP",
        #     "WIF", "TAO", "HEDERA", "VET", "MKR", "AR", "FLOKI", "FTM", "THETA", "RUNE",
        #     "BONK", "ALGO", "SEI", "JUP", "PYTH", "TIA", "LDO", "ONDO", "BRETT", "CORE",
        #     "EGLD", "STRK", "KAS", "QNT", "GALA", "ENS", "AXS", "BEAM", "NEO", "PENDLE",
        #     "ORDI", "XAUt", "CHZ", "FLOW", "NOT", "SAND", "WLD", "GT", "ZEC", "DYDX",
        #     "CFX", "XTZ", "MANA", "CRV", "GNO", "KCS", "ILV", "NEXO", "PAXG", "IOTA",
        #     "OSMO", "CAKE", "W", "ZRO", "LUNC", "MEW", "ZIL", "TURBO", "1INCH", "BOME",
        #     "ENJ", "COMP", "RVN", "HNT", "SNX", "BAT", "ELF", "ANKR", "IOST", "DCR",
        #     "HOT", "GLMR", "T", "KAVA", "RSR", "XDC", "SSV", "GMT", "PROM", "LRC", "CATI",
        #     "MEME", "ZRX", "JTO", "BLUR", "ROSE", "BAL", "ASTR", "DASH", "SXP", "SC",
        #     "WAXP", "API3", "COTI", "PUNDIX", "TRB", "STORJ", "SLP", "SNT", "CVC", "BAND",
        #     "PENGU", "DGB", "KNC", "LSK", "MOG", "ACH", "CTSI", "XEM", "BICO", "ONG",
        #     "ZENT", "WAVES", "POWR", "VTHO", "BONE", "ARK", "DENT", "SYS"
        # ]




        self.btc_trend = None
        self.btc_dominance_trend = None
        
        # API endpoints and settings
        self.base_url = "https://min-api.cryptocompare.com/data"
        self.alternative_base_url = "https://api.coingecko.com/api/v3"
        self.signals_path = "crypto_signals.csv"
        
        # Timeframe mappings for API calls
        self.timeframes = {
            "1m": "histominute",
            "5m": "histominute",
            "15m": "histominute",
            "30m": "histominute",
            "1h": "histohour",
            "4h": "histohour",
            "12h": "histohour",
            "1d": "histoday",
            "1w": "histoday"
        }
        
        # Multipliers for timeframes
        self.timeframe_multipliers = {
            "1m": 1,
            "5m": 5,
            "15m": 15,
            "30m": 30,
            "1h": 1,
            "4h": 4,
            "12h": 12,
            "1d": 1,
            "1w": 7
        }
        
        # Initialize market data cache
        self.btc_dominance = None
        self.correlation_matrix = {}
        self.market_regime = None
        
        # Rate limiting variables
        self.last_api_call = 0
        self.api_call_limit = 15  # Allow up to 15 calls per minute
        
        # Create signals CSV file if it doesn't exist
        self._initialize_signals_csv()

    def update_btc_trend(self):
        """Update Bitcoin trend information"""
        try:
            # Get BTC data for different timeframes
            btc_daily = self.get_historical_data("BTC", "1d")
            if btc_daily is not None:
                btc_daily = self.calculate_technical_indicators(btc_daily)
                self.btc_trend = self.analyze_trend(btc_daily, "1d")

                # Calculate BTC dominance trend
                dominance_history = []
                for i in range(min(14, len(btc_daily))):
                    dominance = self.get_btc_dominance()  # This will use cached value after first call
                    dominance_history.append(dominance)

                # Simple trend based on recent dominance changes
                if len(dominance_history) >= 2:
                    current = dominance_history[-1]
                    previous = dominance_history[0]

                    if current > previous * 1.03:  # 3% increase
                        self.btc_dominance_trend = {"trend": "BULLISH", "strength": 70}
                    elif current < previous * 0.97:  # 3% decrease
                        self.btc_dominance_trend = {"trend": "BEARISH", "strength": 70}
                    else:
                        self.btc_dominance_trend = {"trend": "NEUTRAL", "strength": 50}

                logger.info(f"BTC trend updated. Current trend: {self.btc_trend.get('trend')}")
        except Exception as e:
            logger.error(f"Error updating BTC trend: {e}", exc_info=True)

    
    def _initialize_signals_csv(self):
        """
        Create the signals CSV file with headers if it doesn't exist

        This creates a standardized CSV file for storing trading signals with
        all necessary fields including entry ranges, targets, and stop losses.
        """
        try:
            if not os.path.exists(self.signals_path):
                with open(self.signals_path, 'w', newline='', encoding='utf-8') as csvfile:
                    field_names = [
                        'quality_score', 'confidence', 'blank', 'signal_date', 'signal_time',
                        'crypto_pair', 'direction', 'detected_pattern', 'detected_strategy',
                        'startentryrange', 'endentryrange', 'middleofentryrange'
                    ]
                    # Add targets (up to 10)
                    for i in range(1, 11):
                        field_names.append(f'Target{i}')

                    # Add stop loss
                    field_names.append('StopLoss')

                    # Create writer and write header
                    writer = csv.DictWriter(csvfile, fieldnames=field_names)
                    writer.writeheader()
                    logger.info(f"Created new signals file: {self.signals_path}")
            else:
                logger.debug(f"Signals file already exists: {self.signals_path}")
        except IOError as e:
            logger.error(f"Failed to create signals CSV file: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"Unexpected error creating signals CSV: {e}", exc_info=True)
            raise
    
    def rate_limit_api_call(self):
        """
        Implement rate limiting for API calls to avoid hitting limits
        This ensures we don't exceed the allowed number of calls per minute
        """
        current_time = time.time()
        elapsed = current_time - self.last_api_call
        
        # If we're making calls too quickly, wait a bit
        if elapsed < 0.5:  # Ensure at least 0.5 seconds between calls
            time.sleep(0.5 - elapsed)
        
        self.last_api_call = time.time()
    
    def get_historical_data(self, symbol: str, timeframe: str, limit: int = 200, end_date: Optional[datetime] = None) -> Optional[pd.DataFrame]:
        """
        Fetch historical price data from CryptoCompare API with retry logic

        Args:
            symbol: The cryptocurrency symbol (e.g., 'BTC')
            timeframe: The timeframe to use (e.g., '1d', '4h', '1h')
            limit: Number of data points to retrieve
            end_date: If specified, fetch data up to this date

        Returns:
            DataFrame with historical price data or None if request failed

        Raises:
            ValueError: If invalid timeframe provided
        """
        if timeframe not in self.timeframes:
            logger.error(f"Invalid timeframe: {timeframe}. Valid options: {list(self.timeframes.keys())}")
            raise ValueError(f"Invalid timeframe: {timeframe}")

        max_retries = 3
        retry_delay = 1  # seconds

        for attempt in range(max_retries):
            try:
                # Apply rate limiting
                self.rate_limit_api_call()

                # Select appropriate endpoint and params based on timeframe
                tf_category = self.timeframes.get(timeframe, "histoday")
                tf_multiplier = self.timeframe_multipliers.get(timeframe, 1)

                url = f"{self.base_url}/{tf_category}"
                params = {
                    "fsym": symbol,
                    "tsym": "USDT",
                    "limit": limit,
                    "api_key": self.api_key
                }

                if end_date:
                    params['toTs'] = int(end_date.timestamp())

                # Add multiplier for appropriate timeframes
                if tf_category == "histominute" and tf_multiplier > 1:
                    params["aggregate"] = tf_multiplier
                elif tf_category == "histohour" and tf_multiplier > 1:
                    params["aggregate"] = tf_multiplier
                elif tf_category == "histoday" and tf_multiplier > 1:
                    params["aggregate"] = tf_multiplier

                logger.debug(f"Fetching {symbol} data for timeframe {timeframe} (attempt {attempt + 1}/{max_retries})")

                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()  # Raise exception for HTTP errors
                data = response.json()

                # Check for API-specific errors
                if data.get('Response') == 'Error':
                    logger.warning(f"API error for {symbol}: {data.get('Message')}")
                    return self._fallback_to_alternative_api(symbol, timeframe, limit)

                if 'Data' not in data or not data['Data']:
                    logger.warning(f"No data found for {symbol}")
                    return self._fallback_to_alternative_api(symbol, timeframe, limit)

                # Convert to DataFrame
                df = pd.DataFrame(data['Data'])

                # Convert timestamp to datetime
                df['time'] = pd.to_datetime(df['time'], unit='s')

                # Ensure all required columns exist
                required_columns = ['open', 'high', 'low', 'close', 'volumefrom', 'volumeto']
                for col in required_columns:
                    if col not in df.columns:
                        if col == 'volumefrom' or col == 'volumeto':
                            df[col] = 0
                        else:
                            logger.error(f"Missing required column {col} for {symbol}")
                            return None

                # Calculate volume (some APIs might name it differently)
                df['volume'] = df['volumefrom']

                # Remove rows with 0 values for open, high, low, close
                df = df[(df['open'] > 0) & (df['high'] > 0) & (df['low'] > 0) & (df['close'] > 0)]

                if len(df) == 0:
                    logger.warning(f"All data filtered out for {symbol} - no valid price data")
                    return None

                logger.debug(f"Successfully fetched {len(df)} candles for {symbol}")
                return df

            except requests.exceptions.Timeout:
                logger.warning(f"Timeout fetching {symbol} (attempt {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))  # Exponential backoff
                    continue

            except requests.exceptions.RequestException as e:
                logger.error(f"Network error fetching {symbol}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                    continue

            except Exception as e:
                logger.error(f"Unexpected error fetching data for {symbol}: {e}", exc_info=True)
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue

        # All retries failed, try fallback
        logger.info(f"All retries failed for {symbol}, attempting fallback API")
        return self._fallback_to_alternative_api(symbol, timeframe, limit)
    
    def _fallback_to_alternative_api(self, symbol: str, timeframe: str, limit: int) -> pd.DataFrame:
        """
        Fallback method to use alternative API (CoinGecko) if primary fails
        
        Args:
            symbol: The cryptocurrency symbol (e.g., 'BTC')
            timeframe: The timeframe to use
            limit: Number of data points to retrieve
            
        Returns:
            DataFrame with historical price data or None if request failed
        """
        try:
            print(f"Attempting to fetch {symbol} data from alternative API...")
            
            # CoinGecko uses different timeframe format and has different limits
            # Convert our timeframe to CoinGecko format
            days = 30  # Default
            if timeframe == "1d":
                days = limit
            elif timeframe == "1h" or timeframe == "4h":
                days = min(90, limit // 24 + 1)  # CoinGecko hourly data is limited to ~90 days
            
            # CoinGecko uses different symbol format (lowercase)
            coin_id = symbol.lower()
            
            # Make API request
            url = f"{self.alternative_base_url}/coins/{coin_id}/market_chart"
            params = {
                "vs_currency": "usd",
                "days": days,
                "interval": "daily" if timeframe == "1d" else "hourly"
            }
            
            response = requests.get(url, params=params)
            data = response.json()
            
            if 'prices' not in data:
                print(f"Alternative API: No data found for {symbol}")
                return None
            
            # Convert to DataFrame
            prices = data['prices']  # [timestamp, price] pairs
            volumes = data['total_volumes']  # [timestamp, volume] pairs
            
            # Create DataFrame
            df = pd.DataFrame(data=prices, columns=['timestamp', 'close'])
            df['time'] = pd.to_datetime(df['timestamp'], unit='ms')
            
            # Add volume data
            volume_df = pd.DataFrame(data=volumes, columns=['timestamp', 'volume'])
            df = pd.merge(df, volume_df, on='timestamp')
            
            # For simplicity, set OHLC to close price (this is a limitation when using CoinGecko)
            df['open'] = df['close']
            df['high'] = df['close']
            df['low'] = df['close']
            
            # Limit to requested number of data points
            df = df.tail(limit)
            
            return df
            
        except Exception as e:
            print(f"Alternative API failed for {symbol}: {str(e)}")
            return None
    
    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate technical indicators for a given DataFrame of price data
        
        Args:
            df: DataFrame with OHLCV data
            
        Returns:
            DataFrame with added technical indicators
        """
        if df is None or len(df) < 30:
            print("Insufficient data for technical indicators")
            return df
        
        try:
            # Make a copy to avoid warnings
            df = df.copy()
            
            # Simple moving averages (SMA)
            for period in [5, 10, 20, 30, 50, 100, 200]:
                if len(df) >= period:
                    df[f'sma{period}'] = df['close'].rolling(window=period).mean()
            
            # Exponential moving averages (EMA)
            for period in [9, 12, 13, 26, 50, 100, 200]:
                if len(df) >= period:
                    df[f'ema{period}'] = df['close'].ewm(span=period, adjust=False).mean()
            
            # MACD (Moving Average Convergence Divergence)
            if len(df) >= 26:
                df['macd'] = df['close'].ewm(span=12, adjust=False).mean() - df['close'].ewm(span=26, adjust=False).mean()
                df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
                df['macd_hist'] = df['macd'] - df['macd_signal']
            
            # RSI (Relative Strength Index) for different periods
            for period in [9, 14, 25]:
                if len(df) >= period + 1:
                    delta = df['close'].diff()
                    gain = delta.where(delta > 0, 0)
                    loss = -delta.where(delta < 0, 0)
                    
                    avg_gain = gain.rolling(window=period).mean()
                    avg_loss = loss.rolling(window=period).mean()
                    
                    # Calculate RS and RSI
                    rs = avg_gain / avg_loss
                    df[f'rsi{period}'] = 100 - (100 / (1 + rs))
            
            # Stochastic Oscillator
            if len(df) >= 14:
                low_14 = df['low'].rolling(window=14).min()
                high_14 = df['high'].rolling(window=14).max()
                df['stoch_k'] = 100 * ((df['close'] - low_14) / (high_14 - low_14))
                df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()
                
                # Stochastic RSI
                if 'rsi14' in df.columns:
                    rsi_14 = df['rsi14']
                    stoch_rsi_k = 100 * ((rsi_14 - rsi_14.rolling(window=14).min()) / 
                                       (rsi_14.rolling(window=14).max() - rsi_14.rolling(window=14).min()))
                    df['stoch_rsi_k'] = stoch_rsi_k
                    df['stoch_rsi_d'] = stoch_rsi_k.rolling(window=3).mean()
            
            # Bollinger Bands
            if len(df) >= 20:
                df['bb_middle'] = df['close'].rolling(window=20).mean()
                df['bb_std'] = df['close'].rolling(window=20).std()
                df['bb_upper'] = df['bb_middle'] + (df['bb_std'] * 2)
                df['bb_lower'] = df['bb_middle'] - (df['bb_std'] * 2)
                df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
                
                # Price crossing Bollinger bands signals
                df['price_cross_upper_bb'] = ((df['close'] > df['bb_upper']) & 
                                            (df['close'].shift(1) <= df['bb_upper'].shift(1))).astype(int)
                df['price_cross_lower_bb'] = ((df['close'] < df['bb_lower']) & 
                                            (df['close'].shift(1) >= df['bb_lower'].shift(1))).astype(int)
            
            # Average True Range (ATR)
            if len(df) >= 14:
                high_low = df['high'] - df['low']
                high_close = (df['high'] - df['close'].shift()).abs()
                low_close = (df['low'] - df['close'].shift()).abs()
                
                true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                df['atr'] = true_range.rolling(window=14).mean()
                df['atr_percent'] = (df['atr'] / df['close']) * 100
            
            # CCI (Commodity Channel Index)
            if len(df) >= 20:
                typical_price = (df['high'] + df['low'] + df['close']) / 3
                tp_sma20 = typical_price.rolling(window=20).mean()
                tp_deviation = (typical_price - tp_sma20).abs()
                tp_deviation_mean = tp_deviation.rolling(window=20).mean()
                df['cci20'] = (typical_price - tp_sma20) / (0.015 * tp_deviation_mean)
            
            # Williams %R
            if len(df) >= 14:
                high_14 = df['high'].rolling(window=14).max()
                low_14 = df['low'].rolling(window=14).min()
                df['williams_r'] = -100 * ((high_14 - df['close']) / (high_14 - low_14))
            
            # Ultimate Oscillator
            if len(df) >= 28:
                bp = df['close'] - pd.concat([df['low'], df['close'].shift(1)], axis=1).min(axis=1)
                tr = pd.concat([df['high'] - df['low'], 
                               (df['high'] - df['close'].shift(1)).abs(), 
                               (df['low'] - df['close'].shift(1)).abs()], axis=1).max(axis=1)
                
                avg7 = bp.rolling(window=7).sum() / tr.rolling(window=7).sum()
                avg14 = bp.rolling(window=14).sum() / tr.rolling(window=14).sum()
                avg28 = bp.rolling(window=28).sum() / tr.rolling(window=28).sum()
                
                df['uo'] = 100 * ((4 * avg7) + (2 * avg14) + avg28) / 7
            
            # Momentum
            df['mom'] = df['close'].diff(14)
            
            # ADX (Average Directional Index)
            if len(df) >= 14:
                plus_dm = df['high'].diff()
                minus_dm = -df['low'].diff()
                plus_dm[plus_dm < 0] = 0
                minus_dm[minus_dm < 0] = 0
                
                # Condition for +DM: if +DM > -DM and +DM > 0
                cond1 = (plus_dm > minus_dm) & (plus_dm > 0)
                plus_dm[~cond1] = 0
                
                # Condition for -DM: if -DM > +DM and -DM > 0
                cond2 = (minus_dm > plus_dm) & (minus_dm > 0)
                minus_dm[~cond2] = 0
                
                tr = pd.concat([
                    (df['high'] - df['low']).abs(),
                    (df['high'] - df['close'].shift()).abs(),
                    (df['low'] - df['close'].shift()).abs()
                ], axis=1).max(axis=1)
                
                plus_di14 = 100 * (plus_dm.rolling(window=14).sum() / tr.rolling(window=14).sum())
                minus_di14 = 100 * (minus_dm.rolling(window=14).sum() / tr.rolling(window=14).sum())
                
                dx = 100 * (abs(plus_di14 - minus_di14) / (plus_di14 + minus_di14))
                df['adx'] = pd.Series(dx).rolling(window=14).mean()
                
                # Store +DI and -DI for divergence detection
                df['plus_di'] = plus_di14
                df['minus_di'] = minus_di14
            
            # Volume indicators
            # On Balance Volume (OBV)
            obv = np.zeros(len(df))
            for i in range(1, len(df)):
                if df['close'].iloc[i] > df['close'].iloc[i-1]:
                    obv[i] = obv[i-1] + df['volume'].iloc[i]
                elif df['close'].iloc[i] < df['close'].iloc[i-1]:
                    obv[i] = obv[i-1] - df['volume'].iloc[i]
                else:
                    obv[i] = obv[i-1]
            df['obv'] = obv
            
            # Unusual volume detection (compare to 20-day average)
            df['volume_sma20'] = df['volume'].rolling(window=20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_sma20']
            df['unusual_volume'] = (df['volume_ratio'] > 2).astype(int)
            df['rvol'] = df['volume'] / df['volume'].rolling(20).mean()
            
            # Trend detection based on EMAs (using multiple EMAs to determine trend strength)
            if all(col in df.columns for col in ['ema9', 'ema50', 'ema200']):
                # Short-term trend
                df['short_term_trend'] = np.where(df['ema9'] > df['ema50'], 1, 
                                                np.where(df['ema9'] < df['ema50'], -1, 0))
                
                # Medium-term trend
                df['medium_term_trend'] = np.where(df['ema50'] > df['ema200'], 1, 
                                                np.where(df['ema50'] < df['ema200'], -1, 0))
                
                # Trend strength (combination of short and medium trends)
                df['trend_strength'] = df['short_term_trend'] + df['medium_term_trend']

            # VWAP (Volume-Weighted Average Price)
            df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()
            
            # Detect MA crossovers
            for fast, slow in [(5, 10), (10, 20), (20, 50), (50, 200)]:
                fast_col = f'sma{fast}'
                slow_col = f'sma{slow}'
                crossover_col = f'sma{fast}_{slow}_crossover'
                
                if fast_col in df.columns and slow_col in df.columns:
                    # 1 for bullish crossover, -1 for bearish, 0 for no crossover
                    df[crossover_col] = 0
                    
                    # Bullish crossover (fast crosses above slow)
                    bullish = (df[fast_col].shift(1) <= df[slow_col].shift(1)) & (df[fast_col] > df[slow_col])
                    df.loc[bullish, crossover_col] = 1
                    
                    # Bearish crossover (fast crosses below slow)
                    bearish = (df[fast_col].shift(1) >= df[slow_col].shift(1)) & (df[fast_col] < df[slow_col])
                    df.loc[bearish, crossover_col] = -1
            
            # Do the same for EMA crossovers
            for fast, slow in [(9, 12), (12, 26), (26, 50), (50, 200)]:
                fast_col = f'ema{fast}'
                slow_col = f'ema{slow}'
                crossover_col = f'ema{fast}_{slow}_crossover'
                
                if fast_col in df.columns and slow_col in df.columns:
                    df[crossover_col] = 0
                    
                    # Bullish crossover
                    bullish = (df[fast_col].shift(1) <= df[slow_col].shift(1)) & (df[fast_col] > df[slow_col])
                    df.loc[bullish, crossover_col] = 1
                    
                    # Bearish crossover
                    bearish = (df[fast_col].shift(1) >= df[slow_col].shift(1)) & (df[fast_col] < df[slow_col])
                    df.loc[bearish, crossover_col] = -1

            # Ichimoku Cloud
            high_9 = df['high'].rolling(window=9).max()
            low_9 = df['low'].rolling(window=9).min()
            df['tenkan_sen'] = (high_9 + low_9) / 2

            high_26 = df['high'].rolling(window=26).max()
            low_26 = df['low'].rolling(window=26).min()
            df['kijun_sen'] = (high_26 + low_26) / 2

            df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(26)

            high_52 = df['high'].rolling(window=52).max()
            low_52 = df['low'].rolling(window=52).min()
            df['senkou_span_b'] = ((high_52 + low_52) / 2).shift(26)

            df['chikou_span'] = df['close'].shift(-26)
            
            return df
            
        except Exception as e:
            print(f"Error calculating indicators: {str(e)}")
            return df
    
    def _calculate_fibonacci_levels(self, df: pd.DataFrame) -> List[float]:
        """
        ENHANCED: Calculate Fibonacci retracement and extension levels with crypto-specific improvements
        Uses dynamic swing point detection and volume-weighted significance
        """
        try:
            if len(df) < 50:
                return []
            
            # CRYPTO-SPECIFIC: Use multiple timeframe analysis for swing points
            lookback_periods = [21, 50, 100]  # Short, medium, long-term swings
            all_fib_levels = []
            
            for lookback in lookback_periods:
                if len(df) >= lookback:
                    period_df = df.tail(lookback)
                    
                    # ENHANCED: Find significant swing points with volume confirmation
                    swing_highs, swing_lows = self._find_volume_confirmed_swings(period_df)
                    
                    if len(swing_highs) >= 1 and len(swing_lows) >= 1:
                        # Get most recent and significant swings
                        major_high = max(swing_highs, key=lambda x: x['volume_strength'])
                        major_low = min(swing_lows, key=lambda x: x['price'])
                        
                        # Ensure we have a meaningful range
                        price_range = major_high['price'] - major_low['price']
                        if price_range > 0:
                            # CRYPTO-OPTIMIZED: Fibonacci levels with crypto bias
                            # Standard retracements
                            fib_retracements = [0.236, 0.382, 0.5, 0.618, 0.786]
                            # Crypto-specific extensions (more aggressive)
                            fib_extensions = [1.272, 1.414, 1.618, 2.0, 2.618, 3.618]
                            
                            # Calculate retracement levels
                            for fib in fib_retracements:
                                level = major_high['price'] - (price_range * fib)
                                if level > 0:
                                    # Weight by swing significance and recency
                                    time_weight = 1.0 if lookback == 21 else 0.7 if lookback == 50 else 0.5
                                    volume_weight = (major_high['volume_strength'] + major_low['volume_strength']) / 2
                                    level_weight = time_weight * volume_weight
                                    
                                    all_fib_levels.append({
                                        'price': level,
                                        'type': 'retracement',
                                        'fib_ratio': fib,
                                        'weight': level_weight,
                                        'timeframe': f'{lookback}d'
                                    })
                            
                            # Calculate extension levels (for breakout targets)
                            for fib in fib_extensions:
                                # Upside extensions
                                level_up = major_high['price'] + (price_range * (fib - 1.0))
                                # Downside extensions  
                                level_down = major_low['price'] - (price_range * (fib - 1.0))
                                
                                if level_up > 0:
                                    time_weight = 1.0 if lookback == 21 else 0.7 if lookback == 50 else 0.5
                                    volume_weight = major_high['volume_strength']
                                    
                                    all_fib_levels.append({
                                        'price': level_up,
                                        'type': 'extension_up',
                                        'fib_ratio': fib,
                                        'weight': time_weight * volume_weight * 0.8,  # Extensions less certain
                                        'timeframe': f'{lookback}d'
                                    })
                                
                                if level_down > 0:
                                    time_weight = 1.0 if lookback == 21 else 0.7 if lookback == 50 else 0.5
                                    volume_weight = major_low['volume_strength']
                                    
                                    all_fib_levels.append({
                                        'price': level_down,
                                        'type': 'extension_down',
                                        'fib_ratio': fib,
                                        'weight': time_weight * volume_weight * 0.8,
                                        'timeframe': f'{lookback}d'
                                    })
            
            # ENHANCED: Cluster and weight similar levels
            if not all_fib_levels:
                return []
            
            # Sort by price for clustering
            all_fib_levels.sort(key=lambda x: x['price'])
            
            # Cluster similar levels (within 1% of each other)
            clustered_levels = []
            current_cluster = [all_fib_levels[0]]
            
            for level in all_fib_levels[1:]:
                cluster_avg = sum(l['price'] for l in current_cluster) / len(current_cluster)
                
                if abs(level['price'] - cluster_avg) / cluster_avg < 0.01:  # Within 1%
                    current_cluster.append(level)
                else:
                    # Finalize current cluster
                    if current_cluster:
                        cluster_price = sum(l['price'] * l['weight'] for l in current_cluster) / sum(l['weight'] for l in current_cluster)
                        cluster_weight = sum(l['weight'] for l in current_cluster)
                        
                        # Only include significant levels
                        if cluster_weight > 0.5:  # Minimum significance threshold
                            clustered_levels.append(cluster_price)
                    
                    current_cluster = [level]
            
            # Don't forget the last cluster
            if current_cluster:
                cluster_price = sum(l['price'] * l['weight'] for l in current_cluster) / sum(l['weight'] for l in current_cluster)
                cluster_weight = sum(l['weight'] for l in current_cluster)
                if cluster_weight > 0.5:
                    clustered_levels.append(cluster_price)
            
            # Filter out levels too close to current price (less than 0.5% away)
            current_price = df['close'].iloc[-1]
            filtered_levels = []
            for level in clustered_levels:
                distance = abs(level - current_price) / current_price
                if distance > 0.005:  # At least 0.5% away
                    filtered_levels.append(level)
            
            # Return top 12 most significant levels
            return sorted(filtered_levels)[:12]
            
        except Exception as e:
            print(f"Error in fibonacci calculation: {e}")
            return []
    
    def _find_volume_confirmed_swings(self, df: pd.DataFrame) -> Tuple[List[Dict], List[Dict]]:
        """
        CRYPTO-SPECIFIC: Find swing highs and lows confirmed by volume
        Returns significant swing points with volume strength weighting
        """
        try:
            if len(df) < 10:
                return [], []
            
            # Find potential swing points using a more aggressive approach for crypto
            window = max(3, len(df) // 15)  # Dynamic window based on data length
            
            highs = []
            lows = []
            
            # Calculate volume metrics
            avg_volume = df['volume'].mean()
            volume_std = df['volume'].std()
            
            for i in range(window, len(df) - window):
                # Check for swing high
                is_swing_high = all(
                    df['high'].iloc[i] >= df['high'].iloc[i-j] for j in range(1, window+1)
                ) and all(
                    df['high'].iloc[i] >= df['high'].iloc[i+j] for j in range(1, window+1)
                )
                
                if is_swing_high:
                    # Volume confirmation for swing high
                    volume_around_swing = df['volume'].iloc[i-1:i+2].max()  # Peak volume around swing
                    volume_strength = min(3.0, volume_around_swing / avg_volume) if avg_volume > 0 else 1.0
                    
                    # Price significance (how much higher than surrounding areas)
                    price_context = df['high'].iloc[max(0, i-window*2):i+window*2+1]
                    price_percentile = (df['high'].iloc[i] - price_context.min()) / (price_context.max() - price_context.min()) if price_context.max() > price_context.min() else 0.5
                    
                    highs.append({
                        'price': df['high'].iloc[i],
                        'index': i,
                        'volume': volume_around_swing,
                        'volume_strength': volume_strength,
                        'price_significance': price_percentile,
                        'timestamp': df.index[i] if hasattr(df.index, '__getitem__') else i
                    })
                
                # Check for swing low
                is_swing_low = all(
                    df['low'].iloc[i] <= df['low'].iloc[i-j] for j in range(1, window+1)
                ) and all(
                    df['low'].iloc[i] <= df['low'].iloc[i+j] for j in range(1, window+1)
                )
                
                if is_swing_low:
                    # Volume confirmation for swing low
                    volume_around_swing = df['volume'].iloc[i-1:i+2].max()
                    volume_strength = min(3.0, volume_around_swing / avg_volume) if avg_volume > 0 else 1.0
                    
                    # Price significance
                    price_context = df['low'].iloc[max(0, i-window*2):i+window*2+1]
                    price_percentile = (price_context.max() - df['low'].iloc[i]) / (price_context.max() - price_context.min()) if price_context.max() > price_context.min() else 0.5
                    
                    lows.append({
                        'price': df['low'].iloc[i],
                        'index': i,
                        'volume': volume_around_swing,
                        'volume_strength': volume_strength,
                        'price_significance': price_percentile,
                        'timestamp': df.index[i] if hasattr(df.index, '__getitem__') else i
                    })
            
            # Filter and rank by significance (combination of volume and price importance)
            def calculate_significance(swing):
                return (swing['volume_strength'] * 0.6 + swing['price_significance'] * 0.4)
            
            # Sort by significance and keep most important swings
            highs.sort(key=calculate_significance, reverse=True)
            lows.sort(key=calculate_significance, reverse=True)
            
            # Keep top swings but ensure we have at least one recent swing
            max_swings = min(5, max(1, len(df) // 20))  # Dynamic based on data length
            
            # Ensure we include recent swings
            recent_threshold = len(df) * 0.7  # Last 30% of data
            
            filtered_highs = []
            filtered_lows = []
            
            # Add most significant swings
            for swing in highs[:max_swings]:
                filtered_highs.append(swing)
            
            for swing in lows[:max_swings]:
                filtered_lows.append(swing)
            
            # Ensure we have at least one recent swing if available
            recent_highs = [s for s in highs if s['index'] >= recent_threshold]
            recent_lows = [s for s in lows if s['index'] >= recent_threshold]
            
            if recent_highs and not any(s['index'] >= recent_threshold for s in filtered_highs):
                filtered_highs.append(recent_highs[0])
            
            if recent_lows and not any(s['index'] >= recent_threshold for s in filtered_lows):
                filtered_lows.append(recent_lows[0])
            
            return filtered_highs, filtered_lows
        except Exception as e:
            print(f"Error in volume-confirmed swing detection: {e}")
            return [], []

    def _find_structural_levels(self, df: pd.DataFrame, lookback: int) -> List[Tuple[float, str]]:
        """
        Helper method to find statistical and swing point support/resistance levels.
        """
        if df is None or len(df) < lookback:
            return []

        potential_levels = []

        # --- Statistical Support/Resistance ---
        try:
            for i in range(lookback, len(df) - 5):
                low_price = df['low'].iloc[i]
                if i + 5 < len(df):
                    recovery = df['close'].iloc[i:i+5].min() > low_price * 1.005
                    if recovery:
                        test_count = 0
                        for j in range(max(0, i-lookback), min(len(df), i+lookback)):
                            if abs(df['low'].iloc[j] - low_price) / low_price < 0.01:
                                test_count += 1
                        if test_count >= 2:
                            potential_levels.append((low_price, 'statistical'))

                high_price = df['high'].iloc[i]
                if i + 5 < len(df):
                    rejection = df['close'].iloc[i:i+5].max() < high_price * 0.995
                    if rejection:
                        test_count = 0
                        for j in range(max(0, i-lookback), min(len(df), i+lookback)):
                            if abs(df['high'].iloc[j] - high_price) / high_price < 0.01:
                                test_count += 1
                        if test_count >= 2:
                            potential_levels.append((high_price, 'statistical'))
        except Exception as e:
            print(f"Statistical levels calculation error: {e}")

        # --- Previous significant highs/lows ---
        try:
            for i in range(lookback, len(df) - lookback):
                is_high = all(df['high'].iloc[i] >= df['high'].iloc[i-j] for j in range(1, lookback + 1)) and \
                          all(df['high'].iloc[i] >= df['high'].iloc[i+j] for j in range(1, lookback + 1))
                if is_high:
                    potential_levels.append((df['high'].iloc[i], 'swing_point'))

                is_low = all(df['low'].iloc[i] <= df['low'].iloc[i-j] for j in range(1, lookback + 1)) and \
                         all(df['low'].iloc[i] <= df['low'].iloc[i+j] for j in range(1, lookback + 1))
                if is_low:
                    potential_levels.append((df['low'].iloc[i], 'swing_point'))
        except Exception as e:
            print(f"Historical levels calculation error: {e}")

        return potential_levels

    def identify_support_resistance(self, all_timeframes: Dict[str, pd.DataFrame]) -> Tuple[List[Tuple[float, int]], List[Tuple[float, int]]]:
        """
        Overhauled support and resistance identification using a professional-grade Volume Profile 
        as the primary source and a confluence-based scoring system.
        
        Args:
            all_timeframes: Dictionary of DataFrames for different timeframes
            
        Returns:
            Tuple of (support_levels, resistance_levels), each a list of (price, strength_score) tuples
        """
        df_daily = all_timeframes.get("1d")
        
        if df_daily is None or df_daily.empty:
            return [], []  # Daily data is required for this analysis

        try:
            # Initialize empty list for potential levels
            potential_levels = []
            
            # Get current price
            current_price = df_daily['close'].iloc[-1]
            
            # --- 1. Use the new professional Volume Profile as primary source ---
            volume_profile_result = self.get_professional_volume_profile(df_daily)
            
            # Add POC, VAH, and VAL to potential_levels with their sources
            poc_price = volume_profile_result.get("poc")
            vah_price = volume_profile_result.get("vah")
            val_price = volume_profile_result.get("val")
            
            if poc_price is not None:
                potential_levels.append((poc_price, 'volume_poc'))
            if vah_price is not None:
                potential_levels.append((vah_price, 'volume_vah'))
            if val_price is not None:
                potential_levels.append((val_price, 'volume_val'))

            # --- 2. Call existing helper methods to get other levels ---
            
            # Get structural levels from daily timeframe
            daily_structural_levels = self._find_structural_levels(df_daily, lookback=30)
            for level, source in daily_structural_levels:
                potential_levels.append((level, source + '_1d'))
                
            # Get structural levels from 4H timeframe if available
            df_4h = all_timeframes.get("4h")
            if df_4h is not None and not df_4h.empty:
                four_hour_structural_levels = self._find_structural_levels(df_4h, lookback=15)
                for level, source in four_hour_structural_levels:
                    potential_levels.append((level, source + '_4h'))
            
            # Get Fibonacci levels
            fib_levels = self._calculate_fibonacci_levels(df_daily)
            for level in fib_levels:
                potential_levels.append((level, 'fibonacci'))
                
            # Get pivot points
            if len(df_daily) >= 2:
                try:
                    prev_high = df_daily['high'].iloc[-2]
                    prev_low = df_daily['low'].iloc[-2]
                    prev_close = df_daily['close'].iloc[-2]
                    pivot = (prev_high + prev_low + prev_close) / 3
                    
                    # Calculate pivot levels
                    pivot_levels = [
                        pivot,  # Central pivot
                        (2 * pivot) - prev_low,  # R1
                        (2 * pivot) - prev_high,  # S1
                        pivot + (prev_high - prev_low),  # R2
                        pivot - (prev_high - prev_low),  # S2
                        pivot + 2 * (prev_high - prev_low),  # R3
                        pivot - 2 * (prev_high - prev_low)  # S3
                    ]
                    
                    for level in pivot_levels:
                        if level > 0:  # Only add positive levels
                            potential_levels.append((level, 'pivot'))
                except Exception:
                    pass  # Skip pivot calculation if there's an error
                    
            # Add psychological levels
            magnitude = 10 ** (len(str(int(current_price))) - 1)
            psychological_multipliers = [0.1, 0.2, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 2.5, 5.0, 7.5, 10.0]
            for multiplier in psychological_multipliers:
                level = magnitude * multiplier
                if level > 0 and level != current_price:
                    potential_levels.append((level, 'psychological'))

            # --- 3. Define source scores for confluence-based scoring ---
            source_scores = {
                'volume_poc': 10,       #  Highest importance
                'volume_vah': 8,        # Value Area is critical
                'volume_val': 8,
                'swing_point_1d': 6,    # Daily structure is primary
                'statistical_1d': 5,
                'fibonacci': 4,         # Fibonacci confluence is strong
                'swing_point_4h': 3,    # 4H structure is secondary
                'statistical_4h': 2,
                'pivot': 2,
                'psychological': 1      # Lowest importance
            }

            # --- 4. Cluster and Score Levels ---
            
            # Sort potential_levels by price
            potential_levels.sort(key=lambda x: x[0])
            
            # Group levels that are very close together (within 0.5% of each other)
            clustered_levels = []
            if not potential_levels:
                return [], []

            current_cluster = [potential_levels[0]]
            
            for level, source in potential_levels[1:]:
                cluster_avg_price = sum(p for p, s in current_cluster) / len(current_cluster)
                # Check if level is within 0.5% of cluster average
                if abs(level - cluster_avg_price) / cluster_avg_price < 0.005:
                    current_cluster.append((level, source))
                else:
                    # Calculate final level price (average) and final score (sum of source scores)
                    final_price = sum(p for p, s in current_cluster) / len(current_cluster)
                    final_score = sum(source_scores.get(s, 1) for p, s in current_cluster)
                    clustered_levels.append((final_price, final_score))
                    current_cluster = [(level, source)]

            # Don't forget the last cluster
            if current_cluster:
                final_price = sum(p for p, s in current_cluster) / len(current_cluster)
                final_score = sum(source_scores.get(s, 1) for p, s in current_cluster)
                clustered_levels.append((final_price, final_score))

            # --- 5. Separate into support and resistance ---
            support_levels = [(price, score) for price, score in clustered_levels if price < current_price]
            resistance_levels = [(price, score) for price, score in clustered_levels if price > current_price]
            
            # Sort supports in descending order of price and resistances in ascending order
            support_levels.sort(key=lambda x: x[0], reverse=True)
            resistance_levels.sort(key=lambda x: x[0])
            
            # Return top 8 of each
            return support_levels[:8], resistance_levels[:8]

        except Exception as e:
            print(f"Error in identify_support_resistance: {e}")
            # Fallback to simple support/resistance calculation
            return [], []
    
    def _simple_support_resistance_fallback(self, df: pd.DataFrame, current_price: float) -> Tuple[List[float], List[float]]:
        """Simple fallback support/resistance calculation"""
        try:
            support_levels = []
            resistance_levels = []
            
            # Use recent lows as support
            recent_lows = df['low'].iloc[-50:] if len(df) >= 50 else df['low']
            for low in recent_lows:
                if low < current_price * 0.98:  # At least 2% below current price
                    support_levels.append(low)
            
            # Use recent highs as resistance
            recent_highs = df['high'].iloc[-50:] if len(df) >= 50 else df['high']
            for high in recent_highs:
                if high > current_price * 1.02:  # At least 2% above current price
                    resistance_levels.append(high)
            
            # Remove duplicates and limit results
            support_levels = list(set(support_levels))[:5]
            resistance_levels = list(set(resistance_levels))[:5]
            
            return support_levels, resistance_levels
            
        except Exception as e:
            print(f"Fallback calculation error: {e}")
            return [], []

    def _is_level_significant(self, price_level: float, sr_levels: List[Tuple[float, int]], tolerance_pct: float = 0.015, min_strength: int = 4) -> bool:
        """
        Checks if a given price_level aligns with a significant S/R level.

        Args:
            price_level: The price point of the pattern to check (e.g., a peak or trough).
            sr_levels: The list of pre-calculated (price, strength) tuples for S/R.
            tolerance_pct: The percentage tolerance to consider a match (e.g., 0.015 for 1.5%).
            min_strength: The minimum strength score a S/R level must have to be considered significant.

        Returns:
            True if the price_level aligns with a strong S/R zone, False otherwise.
        """
        if price_level == 0: return False
        for sr_price, sr_strength in sr_levels:
            if abs(price_level - sr_price) / sr_price <= tolerance_pct:
                if sr_strength >= min_strength:
                    return True
        return False

    def _get_volume_weighted_levels(self, df: pd.DataFrame) -> Tuple[List[float], List[float]]:
        """Get support/resistance levels from volume profile analysis"""
        try:
            # Create price-volume histogram
            price_min, price_max = df['low'].min(), df['high'].max()
            price_bins = np.linspace(price_min, price_max, 50)
            volume_profile = np.zeros(49)
            
            for i in range(len(df)):
                price = df['close'].iloc[i]
                volume = df['volume'].iloc[i]
                bin_idx = np.searchsorted(price_bins, price) - 1
                if 0 <= bin_idx < 49:
                    volume_profile[bin_idx] += volume
            
            # Find high-volume nodes (potential support/resistance)
            mean_volume = np.mean(volume_profile)
            std_volume = np.std(volume_profile)
            threshold = mean_volume + 1.5 * std_volume
            
            high_volume_indices = np.where(volume_profile > threshold)[0]
            high_volume_prices = [(price_bins[i] + price_bins[i+1]) / 2 for i in high_volume_indices]
            
            current_price = df['close'].iloc[-1]
            supports = [p for p in high_volume_prices if p < current_price]
            resistances = [p for p in high_volume_prices if p > current_price]
            
            return supports, resistances
            
        except Exception as e:
            return [], []

    def _get_statistical_levels(self, df: pd.DataFrame) -> Tuple[List[float], List[float]]:
        """Get support/resistance using statistical analysis of price rejections"""
        try:
            supports = []
            resistances = []
            current_price = df['close'].iloc[-1]
            
            # Look for price rejection patterns
            for i in range(10, len(df) - 5):
                # Check for support test (price touches low but recovers)
                low_price = df['low'].iloc[i]
                recovery = df['close'].iloc[i:i+5]['close'].min() > low_price * 1.005  # 0.5% recovery
                
                if recovery:
                    # Count how many times this level was tested
                    test_count = 0
                    for j in range(max(0, i-20), min(len(df), i+20)):
                        if abs(df['low'].iloc[j] - low_price) / low_price < 0.01:  # Within 1%
                            test_count += 1
                    
                    if test_count >= 2 and low_price < current_price:
                        supports.append(low_price)
                
                # Check for resistance test (price touches high but fails)
                high_price = df['high'].iloc[i]
                rejection = df['close'].iloc[i:i+5]['close'].max() < high_price * 0.995  # 0.5% rejection
                
                if rejection:
                    test_count = 0
                    for j in range(max(0, i-20), min(len(df), i+20)):
                        if abs(df['high'].iloc[j] - high_price) / high_price < 0.01:
                            test_count += 1
                    
                    if test_count >= 2 and high_price > current_price:
                        resistances.append(high_price)
            
            return list(set(supports)), list(set(resistances))
            
        except Exception as e:
            return [], []

    def _score_levels(self, df: pd.DataFrame, levels: List[float], level_type: str) -> List[Tuple[float, float]]:
        """Score support/resistance levels by strength"""
        scored_levels = []
        current_time = len(df)
        
        for level in levels:
            score = 0.0
            touch_count = 0
            total_volume = 0
            most_recent_touch = 0
            
            # Check each candle for touches of this level
            for i in range(len(df)):
                if level_type == 'support':
                    touch = abs(df['low'].iloc[i] - level) / level < 0.015  # Within 1.5%
                else:
                    touch = abs(df['high'].iloc[i] - level) / level < 0.015
                
                if touch:
                    touch_count += 1
                    total_volume += df['volume'].iloc[i]
                    most_recent_touch = i
                    
                    # Stronger if price bounced/rejected from level
                    if level_type == 'support':
                        bounce = df['close'].iloc[i] > df['low'].iloc[i] * 1.002
                        if bounce:
                            score += 0.2
                    else:
                        rejection = df['close'].iloc[i] < df['high'].iloc[i] * 0.998
                        if rejection:
                            score += 0.2
            
            if touch_count > 0:
                # Base score from touch count
                score += min(touch_count * 0.15, 0.6)
                
                # Volume score (higher volume = stronger level)
                avg_volume = df['volume'].mean()
                volume_strength = min((total_volume / touch_count) / avg_volume, 3.0) * 0.1
                score += volume_strength
                
                # Recency score (more recent touches are more relevant)
                recency = (most_recent_touch / current_time) * 0.3
                score += recency
                
                scored_levels.append((level, score))
        
        return sorted(scored_levels, key=lambda x: x[1], reverse=True)


    def _group_price_levels_with_volume(self, levels: List[Tuple[float, int, float]], threshold: float) -> List[float]:
        """
        Group similar price levels together, weighted by their volume
        
        Args:
            levels: List of tuples containing (price, index, volume)
            threshold: Maximum percentage difference to consider as same level
            
        Returns:
            List of consolidated price levels
        """
        if not levels:
            return []
        
        # Sort levels by price
        sorted_levels = sorted(levels, key=lambda x: x[0])
        
        # Group similar levels
        groups = []
        current_group = [sorted_levels[0]]
        
        for level in sorted_levels[1:]:
            price, _, _ = level
            reference_price, _, _ = current_group[0]
            
            # Check if this level is similar to the current group
            if abs(price - reference_price) / reference_price <= threshold:
                # Add to current group
                current_group.append(level)
            else:
                # Calculate weighted average of current group
                total_weight = sum(volume for _, _, volume in current_group)
                weighted_avg = sum(price * volume for price, _, volume in current_group) / total_weight
                
                # Start a new group
                groups.append(weighted_avg)
                current_group = [level]
        
        # Add the last group
        if current_group:
            total_weight = sum(volume for _, _, volume in current_group)
            weighted_avg = sum(price * volume for price, _, volume in current_group) / total_weight
            groups.append(weighted_avg)
        
        return groups
    
    def _group_price_levels(self, levels: List[float], threshold: float) -> List[float]:
        """
        Group similar price levels together
        
        Args:
            levels: List of price levels to group
            threshold: Maximum percentage difference to consider as same level
            
        Returns:
            List of consolidated price levels
        """
        if not levels:
            return []
        
        # Sort levels
        sorted_levels = sorted(levels)
        grouped = []
        current_group = [sorted_levels[0]]
        
        for level in sorted_levels[1:]:
            # Check if this level is similar to the current group
            reference = current_group[0]
            if abs(level - reference) / reference <= threshold:
                # Add to current group
                current_group.append(level)
            else:
                # Start a new group
                grouped.append(sum(current_group) / len(current_group))
                current_group = [level]
        
        # Add the last group
        if current_group:
            grouped.append(sum(current_group) / len(current_group))
        
        return grouped
    
    def _generate_psychological_levels(self, current_price: float, direction: str, wide_range: bool = False) -> List[float]:
        """
        Generate psychological price levels based on round numbers.
        
        Args:
            current_price: Current price to base levels on.
            direction: "LONG" or "SHORT" to filter levels accordingly.
            wide_range: If True, generate levels further away from the current price.
            
        Returns:
            List of psychological price levels.
        """
        levels = []
        
        # Determine magnitude of current price
        if current_price <= 0: return []
        magnitude = 10 ** int(np.log10(current_price))
        
        # Define multipliers based on the range required
        if wide_range:
            multipliers = [0.1, 0.2, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 4, 5]
            filter_low = 0.2 * current_price
            filter_high = 5 * current_price
        else:
            multipliers = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]
            filter_low = 0.5 * current_price
            filter_high = 2 * current_price

        # Generate levels at various points
        for factor in multipliers:
            level = round(factor * magnitude, int(-np.log10(magnitude)) + 1)
            levels.append(level)
        
        # For smaller prices, add more granular levels
        if current_price < 1:
            small_magnitude = 10 ** int(np.log10(current_price * 100)) if current_price > 0.01 else 0.1
            for factor in multipliers:
                level = round(factor * small_magnitude / 100, int(-np.log10(small_magnitude)) + 3)
                levels.append(level)
        
        # Filter out levels based on direction and range
        if direction == "LONG":
            return sorted(list(set([level for level in levels if level > current_price and filter_low <= level <= filter_high and level > 0])))
        else: # SHORT
            return sorted(list(set([level for level in levels if level < current_price and filter_low <= level <= filter_high and level > 0])), reverse=True)

    def find_trendlines(self, df: pd.DataFrame) -> Tuple[List[float], List[float]]:
        """
        Find diagonal support and resistance trendlines using linear regression.
        """
        try:
            recent_df = df.tail(30)
            highs = recent_df['high'].values
            lows = recent_df['low'].values
            x = np.arange(len(recent_df))

            # Find peaks and troughs
            peaks = [i for i in range(1, len(highs) - 1) if highs[i] > highs[i-1] and highs[i] > highs[i+1]]
            troughs = [i for i in range(1, len(lows) - 1) if lows[i] < lows[i-1] and lows[i] < lows[i+1]]

            support_trend = []
            if len(troughs) >= 2:
                x_troughs = np.array(troughs).reshape(-1, 1)
                y_troughs = lows[troughs]
                lr_support = LinearRegression().fit(x_troughs, y_troughs)
                support_trend = lr_support.predict(x.reshape(-1, 1))

            resistance_trend = []
            if len(peaks) >= 2:
                x_peaks = np.array(peaks).reshape(-1, 1)
                y_peaks = highs[peaks]
                lr_resistance = LinearRegression().fit(x_peaks, y_peaks)
                resistance_trend = lr_resistance.predict(x.reshape(-1, 1))

            return support_trend.tolist(), resistance_trend.tolist()
        except Exception as e:
            print(f"Error in find_trendlines: {e}")
            return [], []

    def get_professional_volume_profile(self, df: pd.DataFrame, bins: int = 30) -> Dict[str, Any]:
        """
        ENHANCED Professional Volume Profile Analysis for crypto markets
        Returns Point of Control, Value Area High/Low, and volume distribution insights
        Enhanced with crypto-specific accumulation/distribution detection
        """
        try:
            if 'volume' not in df.columns or len(df) < 20:
                return {"poc": None, "vah": None, "val": None, "volume_nodes": []}
            
            # Get price and volume data
            prices = df['close'].values
            volumes = df['volume'].values
            highs = df['high'].values
            lows = df['low'].values
            
            # ENHANCEMENT: Dynamic bin sizing based on volatility
            atr = np.mean(highs[-14:] - lows[-14:]) if len(df) >= 14 else (highs[-1] - lows[-1])
            price_range = highs.max() - lows.min()
            # Adjust bins for crypto volatility - more bins for stable periods
            if atr / price_range < 0.02:  # Low volatility
                bins = min(50, bins * 2)
            elif atr / price_range > 0.08:  # High volatility
                bins = max(20, bins // 2)
            
            # Create price bins using high-low range for more accuracy
            price_min = lows.min()
            price_max = highs.max()
            price_bins = np.linspace(price_min, price_max, bins + 1)
            
            # ENHANCEMENT: Volume-weighted price distribution
            volume_by_price = np.zeros(bins)
            time_weighted_volume = np.zeros(bins)  # Recent volume gets higher weight
            accumulation_zones = np.zeros(bins)    # Track accumulation vs distribution
            
            for i in range(len(df)):
                # Use VWAP-style calculation for more accurate representation
                ohlc_avg = (highs[i] + lows[i] + prices[i] + prices[i]) / 4  # Close gets double weight
                volume = volumes[i]
                
                # Time decay factor - recent data is more important in crypto
                time_weight = 1.0 + (i / len(df)) * 0.5  # Recent data gets up to 50% more weight
                
                # Find appropriate bin
                bin_index = np.searchsorted(price_bins, ohlc_avg) - 1
                if 0 <= bin_index < bins:
                    volume_by_price[bin_index] += volume
                    time_weighted_volume[bin_index] += volume * time_weight
                    
                    # CRYPTO-SPECIFIC: Detect accumulation vs distribution
                    price_change = 0 if i == 0 else (prices[i] - prices[i-1]) / prices[i-1]

                    # FIX: Safely calculate volume_ratio to prevent division by zero
                    mean_vol_slice = np.mean(volumes[max(0, i-10):i+1]) if i > 0 else 1.0
                    if mean_vol_slice > 0:
                        volume_ratio = volume / mean_vol_slice
                    else:
                        volume_ratio = 1.0 # Default to 1.0 if mean volume is zero
                    
                    # High volume + small price change = accumulation/distribution
                    if volume_ratio > 1.5 and abs(price_change) < 0.02:
                        if price_change >= 0:
                            accumulation_zones[bin_index] += volume * 0.7  # Accumulation
                        else:
                            accumulation_zones[bin_index] -= volume * 0.7  # Distribution
            
            # ENHANCED: Calculate Point of Control using time-weighted volume
            poc_index = np.argmax(time_weighted_volume)
            poc_price = (price_bins[poc_index] + price_bins[poc_index + 1]) / 2
            poc_strength = time_weighted_volume[poc_index] / np.mean(time_weighted_volume)
            
            # Calculate Value Area (70% of total volume) using time-weighted data
            total_volume = np.sum(time_weighted_volume)
            target_volume = total_volume * 0.70
            
            # Find Value Area High and Low
            sorted_indices = np.argsort(time_weighted_volume)[::-1]  # Sort by volume desc
            cumulative_volume = 0
            value_area_bins = []
            
            for idx in sorted_indices:
                cumulative_volume += time_weighted_volume[idx]
                value_area_bins.append(idx)
                if cumulative_volume >= target_volume:
                    break
            
            # Value Area High and Low
            vah_price = (price_bins[max(value_area_bins)] + price_bins[max(value_area_bins) + 1]) / 2
            val_price = (price_bins[min(value_area_bins)] + price_bins[min(value_area_bins) + 1]) / 2
            
            # CRYPTO-ENHANCED: Identify significant volume nodes with accumulation bias
            avg_volume = total_volume / bins
            volume_nodes = []
            
            for i, vol in enumerate(time_weighted_volume):
                if vol > avg_volume * 1.3:  # Lower threshold for crypto volatility
                    node_price = (price_bins[i] + price_bins[i + 1]) / 2
                    
                    # Calculate accumulation score
                    accumulation_score = accumulation_zones[i] / max(vol, 1)
                    
                    # Classify node type
                    if accumulation_score > 0.3:
                        node_type = "accumulation"  # Smart money buying
                    elif accumulation_score < -0.3:
                        node_type = "distribution"  # Smart money selling
                    else:
                        node_type = "neutral"      # Mixed activity
                    
                    # Calculate proximity to current price for relevance
                    current_price = prices[-1]
                    price_distance = abs(node_price - current_price) / current_price
                    relevance_multiplier = max(0.5, 2.0 - price_distance * 10)  # Closer = more relevant
                    
                    volume_nodes.append({
                        "price": node_price,
                        "volume": vol,
                        "strength": (vol / avg_volume) * relevance_multiplier,
                        "accumulation_score": round(accumulation_score, 3),
                        "node_type": node_type,
                        "distance_pct": round(price_distance * 100, 2)
                    })
            
            # Sort volume nodes by strength (relevance-adjusted)
            volume_nodes.sort(key=lambda x: x["strength"], reverse=True)
            
            # CRYPTO-SPECIFIC: Market structure analysis
            market_structure = self._analyze_volume_market_structure(
                time_weighted_volume, accumulation_zones, current_price, price_bins
            )
            
            return {
                "poc": poc_price,
                "poc_strength": round(poc_strength, 2),
                "vah": vah_price,
                "val": val_price,
                "volume_nodes": volume_nodes[:10],  # Top 10 most relevant nodes
                "total_volume": total_volume,
                "distribution_quality": self._assess_volume_distribution_quality(time_weighted_volume),
                "market_structure": market_structure,
                "accumulation_distribution_ratio": round(np.sum(accumulation_zones) / max(total_volume, 1), 3)
            }
            
        except Exception as e:
            print(f"Error in professional volume profile: {e}")
            return {"poc": None, "vah": None, "val": None, "volume_nodes": []}
    
    def _assess_volume_distribution_quality(self, volume_by_price):
        """Assess the quality of volume distribution for trading signals"""
        try:
            # Calculate distribution metrics
            total_volume = np.sum(volume_by_price)
            if total_volume == 0:
                return "poor"
            
            # Concentration ratio (how concentrated volume is)
            max_volume = np.max(volume_by_price)
            concentration_ratio = max_volume / total_volume
            
            # Standard deviation of volume distribution
            volume_std = np.std(volume_by_price)
            volume_mean = np.mean(volume_by_price)
            cv = volume_std / volume_mean if volume_mean > 0 else 0
            
            # Classification
            if concentration_ratio > 0.3 and cv > 1.5:
                return "excellent"  # High concentration, good for S/R levels
            elif concentration_ratio > 0.2 and cv > 1.0:
                return "good"
            elif concentration_ratio > 0.15:
                return "fair"
            else:
                return "poor"  # Too distributed, weak S/R levels
                
        except Exception:
            return "unknown"
    
    def _analyze_volume_market_structure(self, volume_by_price, accumulation_zones, current_price, price_bins):
        """
        CRYPTO-SPECIFIC: Analyze market structure from volume distribution
        Identifies key support/resistance zones and market phase
        """
        try:
            # Find current price bin
            current_bin = np.searchsorted(price_bins, current_price) - 1
            current_bin = max(0, min(current_bin, len(volume_by_price) - 1))
            
            # Analyze volume above and below current price
            volume_above = np.sum(volume_by_price[current_bin+1:]) if current_bin < len(volume_by_price) - 1 else 0
            volume_below = np.sum(volume_by_price[:current_bin]) if current_bin > 0 else 0
            total_volume = volume_above + volume_below + volume_by_price[current_bin]
            
            if total_volume == 0:
                return {"phase": "unknown", "bias": "neutral"}
            
            # Calculate volume distribution bias
            volume_above_pct = volume_above / total_volume * 100
            volume_below_pct = volume_below / total_volume * 100
            
            # Analyze accumulation distribution patterns
            acc_above = np.sum(accumulation_zones[current_bin+1:]) if current_bin < len(accumulation_zones) - 1 else 0
            acc_below = np.sum(accumulation_zones[:current_bin]) if current_bin > 0 else 0
            
            # Determine market phase
            if volume_above_pct > 65:
                if acc_above > 0:
                    phase = "accumulation_above"  # Strength above, likely to move up
                else:
                    phase = "resistance_above"   # Heavy selling above
            elif volume_below_pct > 65:
                if acc_below > 0:
                    phase = "support_below"      # Strong support below
                else:
                    phase = "distribution_below" # Weak support below
            else:
                phase = "balanced"             # Relatively balanced
            
            # Overall market bias
            if acc_above + acc_below > 0:
                bias = "bullish_accumulation"
            elif acc_above + acc_below < 0:
                bias = "bearish_distribution"
            else:
                bias = "neutral"
            
            return {
                "phase": phase,
                "bias": bias,
                "volume_above_pct": round(volume_above_pct, 1),
                "volume_below_pct": round(volume_below_pct, 1),
                "net_accumulation": round(acc_above + acc_below, 2)
            }
            
        except Exception:
            return {"phase": "unknown", "bias": "neutral"}
    
    def calculate_money_flow_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        PROFESSIONAL Money Flow Analysis for crypto market insights
        Analyzes buying/selling pressure and institutional activity
        """
        try:
            if len(df) < 20:
                return {"status": "insufficient_data"}
            
            # Calculate Money Flow Multiplier and Money Flow Volume
            typical_price = (df['high'] + df['low'] + df['close']) / 3
            
            # Money Flow Multiplier
            mf_multiplier = []
            for i in range(len(df)):
                if i == 0:
                    mf_multiplier.append(0)
                else:
                    high_low_range = df['high'].iloc[i] - df['low'].iloc[i]
                    if high_low_range == 0:
                        mf_multiplier.append(0)
                    else:
                        multiplier = ((df['close'].iloc[i] - df['low'].iloc[i]) - 
                                    (df['high'].iloc[i] - df['close'].iloc[i])) / high_low_range
                        mf_multiplier.append(multiplier)
            
            # Money Flow Volume
            mf_volume = [mult * vol for mult, vol in zip(mf_multiplier, df['volume'])]
            
            # Positive and Negative Money Flow
            positive_mf = [vol if mult > 0 else 0 for mult, vol in zip(mf_multiplier, df['volume'])]
            negative_mf = [vol if mult < 0 else 0 for mult, vol in zip(mf_multiplier, df['volume'])]
            
            # Money Flow Index (14-period)
            period = min(14, len(df) - 1)
            if period < 5:
                return {"status": "insufficient_data"}
            
            positive_sum = sum(positive_mf[-period:])
            negative_sum = sum(negative_mf[-period:])
            
            if negative_sum == 0:
                mfi = 100
            else:
                money_ratio = positive_sum / negative_sum
                mfi = 100 - (100 / (1 + money_ratio))
            
            # Buying/Selling Pressure Analysis
            recent_positive = sum(positive_mf[-5:])  # Last 5 periods
            recent_negative = sum(negative_mf[-5:])
            total_recent = recent_positive + recent_negative
            
            if total_recent > 0:
                buying_pressure = (recent_positive / total_recent) * 100
                selling_pressure = (recent_negative / total_recent) * 100
            else:
                buying_pressure = selling_pressure = 50
            
            # Volume-Price Trend (VPT) for trend confirmation
            vpt = [0]
            for i in range(1, len(df)):
                price_change = (df['close'].iloc[i] - df['close'].iloc[i-1]) / df['close'].iloc[i-1]
                vpt_change = price_change * df['volume'].iloc[i]
                vpt.append(vpt[-1] + vpt_change)
            
            # Smart Money Activity Detection
            smart_money_signals = self._detect_smart_money_activity(df, mf_volume)
            
            return {
                "status": "success",
                "mfi": round(mfi, 2),
                "buying_pressure": round(buying_pressure, 2),
                "selling_pressure": round(selling_pressure, 2),
                "money_flow_trend": "bullish" if buying_pressure > 60 else "bearish" if selling_pressure > 60 else "neutral",
                "vpt_current": vpt[-1],
                "vpt_trend": "rising" if len(vpt) > 5 and vpt[-1] > vpt[-6] else "falling",
                "smart_money_signals": smart_money_signals
            }
            
        except Exception as e:
            print(f"Error in money flow analysis: {e}")
            return {"status": "error"}
    
    def _detect_smart_money_activity(self, df: pd.DataFrame, mf_volume: List[float]) -> Dict[str, Any]:
        """ENHANCED: Detect potential smart money/institutional activity with crypto-specific patterns"""
        try:
            signals = {
                "accumulation": False,
                "distribution": False,
                "unusual_activity": False,
                "stealth_buying": False,
                "whale_activity": False,
                "strength": 0,
                "confidence": 0
            }
            
            if len(df) < 15 or len(mf_volume) < 15:
                return signals
            
            # Enhanced volume analysis
            recent_volume = df['volume'].iloc[-5:]
            avg_volume_20 = df['volume'].iloc[-20:].mean()
            avg_volume_10 = df['volume'].iloc[-10:].mean()
            
            # Price action analysis
            recent_prices = df['close'].iloc[-10:]
            price_change_5d = (df['close'].iloc[-1] - df['close'].iloc[-6]) / df['close'].iloc[-6]
            price_volatility = recent_prices.std() / recent_prices.mean()
            
            # Money flow analysis  
            recent_mf = np.mean(mf_volume[-5:]) if len(mf_volume) >= 5 else 0
            avg_mf_15 = np.mean(mf_volume[-15:]) if len(mf_volume) >= 15 else 0
            mf_trend = recent_mf - avg_mf_15
            
            # CRYPTO-SPECIFIC: Stealth accumulation pattern
            # Consistent above-average volume with price remaining in tight range
            volume_consistency = np.sum(recent_volume > avg_volume_20 * 1.1) / len(recent_volume)
            if (volume_consistency > 0.6 and 
                price_volatility < 0.03 and  # Low volatility
                abs(price_change_5d) < 0.05 and  # Small net price change
                recent_mf > 0):  # Positive money flow
                signals["stealth_buying"] = True
                signals["strength"] += 4
                signals["confidence"] += 25
            
            # CRYPTO-SPECIFIC: Whale activity detection
            # Sudden volume spikes with specific price patterns
            max_recent_volume = recent_volume.max()
            volume_spike_ratio = max_recent_volume / avg_volume_20
            
            if volume_spike_ratio > 3.0:  # 3x normal volume
                # Check if it's accumulation or distribution
                spike_day_index = recent_volume.idxmax()
                spike_day_price_change = (df.loc[spike_day_index, 'close'] - df.loc[spike_day_index, 'open']) / df.loc[spike_day_index, 'open']
                
                if abs(spike_day_price_change) > 0.03:  # Significant price move with volume
                    signals["whale_activity"] = True
                    signals["strength"] += 5
                    signals["confidence"] += 30
                    
                    if spike_day_price_change > 0:
                        signals["accumulation"] = True
                    else:
                        signals["distribution"] = True
            
            # Enhanced accumulation detection
            volume_above_avg = np.sum(recent_volume > avg_volume_20) / len(recent_volume)
            if (volume_above_avg > 0.7 and  # 70% of recent days above average volume
                mf_trend > 0 and            # Improving money flow
                0 < price_change_5d < 0.08 and  # Moderate positive price action
                price_volatility < 0.05):   # Controlled volatility
                signals["accumulation"] = True
                signals["strength"] += 3
                signals["confidence"] += 20
            
            # Enhanced distribution detection
            if (volume_above_avg > 0.6 and  # High volume
                mf_trend < 0 and            # Deteriorating money flow
                -0.08 < price_change_5d < 0 and  # Moderate negative price action
                price_volatility > 0.03):   # Increasing volatility
                signals["distribution"] = True
                signals["strength"] += 3
                signals["confidence"] += 20
            
            # CRYPTO-SPECIFIC: Unusual activity patterns
            # High volume with minimal price impact (absorption)
            avg_volume_impact = abs(price_change_5d) / (np.mean(recent_volume) / avg_volume_20)
            if (np.mean(recent_volume) > avg_volume_20 * 1.5 and
                avg_volume_impact < 0.01 and  # Very low price impact per volume unit
                abs(price_change_5d) < 0.03):  # Overall small price move
                signals["unusual_activity"] = True
                signals["strength"] += 2
                signals["confidence"] += 15
            
            # CRYPTO-SPECIFIC: Order flow imbalance detection
            if 'high' in df.columns and 'low' in df.columns:
                # Analyze wick patterns for absorption
                recent_wicks_up = (df['high'].iloc[-5:] - df['close'].iloc[-5:]).mean()
                recent_wicks_down = (df['close'].iloc[-5:] - df['low'].iloc[-5:]).mean()
                body_sizes = abs(df['close'].iloc[-5:] - df['open'].iloc[-5:]).mean()
                
                # Long lower wicks with volume suggest buying absorption
                if (recent_wicks_down > body_sizes * 1.5 and
                    np.mean(recent_volume) > avg_volume_20 * 1.2):
                    signals["strength"] += 2
                    signals["confidence"] += 10
                
                # Long upper wicks with volume suggest selling pressure
                elif (recent_wicks_up > body_sizes * 1.5 and
                      np.mean(recent_volume) > avg_volume_20 * 1.2):
                    signals["strength"] += 1  # Less bullish
                    signals["confidence"] += 5
            
            # Cap confidence at 100
            signals["confidence"] = min(100, signals["confidence"])
            
            return signals
            
        except Exception as e:
            print(f"Error in smart money detection: {e}")
            return {"accumulation": False, "distribution": False, "unusual_activity": False, 
                   "stealth_buying": False, "whale_activity": False, "strength": 0, "confidence": 0}

    def _dow_theory_phase(self, df: pd.DataFrame) -> str:
        """
        Enhanced Dow Theory market phase classification with comprehensive crypto-specific analysis
        
        The Four Phases of Dow Theory Market Cycles:
        1. Accumulation: Smart money buying at low prices after downtrend
        2. Markup: Public participation driving prices higher with increasing volume
        3. Distribution: Smart money selling while public still buying, volume increases without price gains
        4. Markdown: Public selling intensifies, prices decline rapidly on large volume
        """
        try:
            if len(df) < 100:
                return "Undefined"
                
            # Get comprehensive market analysis
            dow_analysis = self._comprehensive_dow_analysis(df)
            
            # Determine phase based on multiple Dow Theory factors
            phase_score = self._calculate_dow_phase_score(df, dow_analysis)
            
            return phase_score['phase']
            
        except Exception as e:
            print(f"Error in _dow_theory_phase: {e}")
            return "Undefined"
    
    def _comprehensive_dow_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Comprehensive Dow Theory analysis including trend structure, volume confirmation, and phases
        """
        try:
            analysis = {
                'primary_trend': None,
                'secondary_trend': None,
                'trend_structure': None,
                'volume_confirmation': None,
                'phase_indicators': {},
                'trend_strength': 0,
                'confirmation_score': 0
            }
            
            # 1. Identify trend structure using peak/trough analysis
            analysis['trend_structure'] = self._identify_dow_trend_structure(df)
            
            # 2. Analyze primary and secondary trends across multiple timeframes
            analysis['primary_trend'] = self._analyze_primary_trend(df)
            analysis['secondary_trend'] = self._analyze_secondary_trend(df)
            
            # 3. Volume confirmation analysis
            analysis['volume_confirmation'] = self._analyze_dow_volume_confirmation(df, analysis['primary_trend'])
            
            # 4. Phase-specific indicators
            analysis['phase_indicators'] = self._analyze_phase_indicators(df)
            
            # 5. Calculate overall trend strength and confirmation
            analysis['trend_strength'] = self._calculate_trend_strength(analysis)
            analysis['confirmation_score'] = self._calculate_confirmation_score(analysis)
            
            return analysis
            
        except Exception:
            return {'primary_trend': 'NEUTRAL', 'secondary_trend': 'NEUTRAL', 
                   'trend_structure': None, 'volume_confirmation': False,
                   'phase_indicators': {}, 'trend_strength': 0, 'confirmation_score': 0}
    
    def _identify_dow_trend_structure(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Identify trend structure using Dow Theory peak/trough analysis
        """
        try:
            structure = {
                'peaks': [],
                'troughs': [],
                'higher_highs': 0,
                'higher_lows': 0,
                'lower_highs': 0,
                'lower_lows': 0,
                'trend_direction': 'NEUTRAL'
            }
            
            # Use recent 60 periods for trend structure analysis
            recent_df = df.tail(60)
            if len(recent_df) < 20:
                return structure
            
            # Find significant peaks and troughs
            peaks = self._find_significant_peaks(recent_df['high'], min_periods=5)
            troughs = self._find_significant_troughs(recent_df['low'], min_periods=5)
            
            structure['peaks'] = peaks
            structure['troughs'] = troughs
            
            # Analyze trend structure patterns
            if len(peaks) >= 2 and len(troughs) >= 2:
                # Count higher highs and higher lows (bullish structure)
                for i in range(1, len(peaks)):
                    if peaks[i][1] > peaks[i-1][1]:
                        structure['higher_highs'] += 1
                    else:
                        structure['lower_highs'] += 1
                
                for i in range(1, len(troughs)):
                    if troughs[i][1] > troughs[i-1][1]:
                        structure['higher_lows'] += 1
                    else:
                        structure['lower_lows'] += 1
                
                # Determine trend direction based on structure
                bullish_signals = structure['higher_highs'] + structure['higher_lows']
                bearish_signals = structure['lower_highs'] + structure['lower_lows']
                
                if bullish_signals > bearish_signals * 1.5:
                    structure['trend_direction'] = 'BULLISH'
                elif bearish_signals > bullish_signals * 1.5:
                    structure['trend_direction'] = 'BEARISH'
                else:
                    structure['trend_direction'] = 'NEUTRAL'
            
            return structure
            
        except Exception:
            return {'peaks': [], 'troughs': [], 'higher_highs': 0, 'higher_lows': 0,
                   'lower_highs': 0, 'lower_lows': 0, 'trend_direction': 'NEUTRAL'}
    
    def _find_significant_peaks(self, price_series: pd.Series, min_periods: int = 5) -> List[Tuple[int, float]]:
        """
        Find significant peaks in price data for Dow Theory analysis
        """
        try:
            peaks = []
            if len(price_series) < min_periods * 2 + 1:
                return peaks
            
            for i in range(min_periods, len(price_series) - min_periods):
                is_peak = True
                current_price = price_series.iloc[i]
                
                # Check if current point is higher than surrounding points
                for j in range(i - min_periods, i + min_periods + 1):
                    if j != i and price_series.iloc[j] >= current_price:
                        is_peak = False
                        break
                
                if is_peak:
                    peaks.append((i, current_price))
            
            return peaks
            
        except Exception:
            return []
    
    def _find_significant_troughs(self, price_series: pd.Series, min_periods: int = 5) -> List[Tuple[int, float]]:
        """
        Find significant troughs in price data for Dow Theory analysis
        """
        try:
            troughs = []
            if len(price_series) < min_periods * 2 + 1:
                return troughs
            
            for i in range(min_periods, len(price_series) - min_periods):
                is_trough = True
                current_price = price_series.iloc[i]
                
                # Check if current point is lower than surrounding points
                for j in range(i - min_periods, i + min_periods + 1):
                    if j != i and price_series.iloc[j] <= current_price:
                        is_trough = False
                        break
                
                if is_trough:
                    troughs.append((i, current_price))
            
            return troughs
            
        except Exception:
            return []
    
    def _analyze_primary_trend(self, df: pd.DataFrame) -> str:
        """
        Analyze primary trend using Dow Theory principles (months to years timeframe)
        """
        try:
            # Use longer period for primary trend (100+ periods)
            long_df = df.tail(100)
            if len(long_df) < 50:
                return 'NEUTRAL'
            
            # Calculate multiple trend indicators
            price_trend = self.analyze_trend(long_df, '1d')
            
            # Moving average trend confirmation
            if 'sma_50' in long_df.columns and 'sma_200' in long_df.columns:
                ma_trend = 'BULLISH' if long_df['sma_50'].iloc[-1] > long_df['sma_200'].iloc[-1] else 'BEARISH'
            else:
                ma_trend = price_trend['trend']
            
            # Price position relative to long-term range
            price_range = long_df['high'].max() - long_df['low'].min()
            current_position = (long_df['close'].iloc[-1] - long_df['low'].min()) / price_range
            
            position_trend = 'BULLISH' if current_position > 0.6 else 'BEARISH' if current_position < 0.4 else 'NEUTRAL'
            
            # Combine indicators for primary trend
            trends = [price_trend['trend'], ma_trend, position_trend]
            bullish_count = trends.count('BULLISH')
            bearish_count = trends.count('BEARISH')
            
            if bullish_count >= 2:
                return 'BULLISH'
            elif bearish_count >= 2:
                return 'BEARISH'
            else:
                return 'NEUTRAL'
                
        except Exception:
            return 'NEUTRAL'
    
    def _analyze_secondary_trend(self, df: pd.DataFrame) -> str:
        """
        Analyze secondary trend (weeks to months corrections within primary trend)
        """
        try:
            # Use medium period for secondary trend (20-40 periods)
            medium_df = df.tail(40)
            if len(medium_df) < 20:
                return 'NEUTRAL'
            
            # Calculate shorter-term trend
            secondary_trend = self.analyze_trend(medium_df, '4h')
            
            # Recent momentum analysis
            recent_change = (medium_df['close'].iloc[-1] - medium_df['close'].iloc[-10]) / medium_df['close'].iloc[-10]
            momentum_trend = 'BULLISH' if recent_change > 0.02 else 'BEARISH' if recent_change < -0.02 else 'NEUTRAL'
            
            # RSI trend confirmation
            if 'rsi14' in medium_df.columns:
                rsi = medium_df['rsi14'].iloc[-1]
                rsi_trend = 'BULLISH' if rsi > 50 else 'BEARISH' if rsi < 50 else 'NEUTRAL'
            else:
                rsi_trend = secondary_trend['trend']
            
            # Combine for secondary trend
            trends = [secondary_trend['trend'], momentum_trend, rsi_trend]
            bullish_count = trends.count('BULLISH')
            bearish_count = trends.count('BEARISH')
            
            if bullish_count >= 2:
                return 'BULLISH'
            elif bearish_count >= 2:
                return 'BEARISH'
            else:
                return 'NEUTRAL'
                
        except Exception:
            return 'NEUTRAL'
    
    def _analyze_dow_volume_confirmation(self, df: pd.DataFrame, primary_trend: str) -> Dict[str, Any]:
        """
        Analyze volume confirmation according to Dow Theory principles
        """
        try:
            confirmation = {
                'is_confirmed': False,
                'volume_trend': 'NEUTRAL',
                'volume_quality': 0,
                'trend_days_volume': 0,
                'counter_days_volume': 0
            }
            
            if 'volume' not in df.columns or len(df) < 20:
                return confirmation
            
            recent_df = df.tail(20)
            
            # Analyze volume on trend days vs counter-trend days
            trend_vol_sum = 0
            counter_vol_sum = 0
            trend_days = 0
            counter_days = 0
            
            for i in range(1, len(recent_df)):
                price_change = recent_df['close'].iloc[i] - recent_df['close'].iloc[i-1]
                volume = recent_df['volume'].iloc[i]
                
                if primary_trend == 'BULLISH':
                    if price_change > 0:  # Up day in bull trend
                        trend_vol_sum += volume
                        trend_days += 1
                    elif price_change < 0:  # Down day in bull trend
                        counter_vol_sum += volume
                        counter_days += 1
                elif primary_trend == 'BEARISH':
                    if price_change < 0:  # Down day in bear trend
                        trend_vol_sum += volume
                        trend_days += 1
                    elif price_change > 0:  # Up day in bear trend
                        counter_vol_sum += volume
                        counter_days += 1
            
            # Calculate average volumes
            if trend_days > 0:
                confirmation['trend_days_volume'] = trend_vol_sum / trend_days
            if counter_days > 0:
                confirmation['counter_days_volume'] = counter_vol_sum / counter_days
            
            # Volume confirmation check
            if (confirmation['trend_days_volume'] > confirmation['counter_days_volume'] * 1.2 and 
                trend_days >= counter_days):
                confirmation['is_confirmed'] = True
                confirmation['volume_quality'] = 70 + min(30, (confirmation['trend_days_volume'] / 
                                                           max(confirmation['counter_days_volume'], 1) - 1) * 100)
            
            # Overall volume trend
            volume_slope = recent_df['volume'].rolling(10).mean().pct_change().mean()
            if volume_slope > 0.02:
                confirmation['volume_trend'] = 'INCREASING'
            elif volume_slope < -0.02:
                confirmation['volume_trend'] = 'DECREASING'
            else:
                confirmation['volume_trend'] = 'STABLE'
            
            return confirmation
            
        except Exception:
            return {'is_confirmed': False, 'volume_trend': 'NEUTRAL', 'volume_quality': 0,
                   'trend_days_volume': 0, 'counter_days_volume': 0}
    
    def _analyze_phase_indicators(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Analyze indicators specific to each Dow Theory phase
        """
        try:
            indicators = {
                'accumulation_score': 0,
                'markup_score': 0,
                'distribution_score': 0,
                'markdown_score': 0,
                'smart_money_activity': 0,
                'public_participation': 0
            }
            
            if len(df) < 50:
                return indicators
            
            recent_df = df.tail(50)
            current_price = recent_df['close'].iloc[-1]
            
            # Price position analysis
            price_range = recent_df['high'].max() - recent_df['low'].min()
            price_position = (current_price - recent_df['low'].min()) / price_range if price_range > 0 else 0.5
            
            # Volume analysis for phases
            recent_volume = recent_df['volume'].iloc[-10:].mean()
            longer_volume = recent_df['volume'].iloc[-30:].mean()
            volume_ratio = recent_volume / longer_volume if longer_volume > 0 else 1
            
            # Price momentum
            price_momentum = (current_price - recent_df['close'].iloc[-20]) / recent_df['close'].iloc[-20]
            
            # Smart money indicators (using our enhanced detection)
            smart_money_signals = self._detect_smart_money_activity(recent_df)
            indicators['smart_money_activity'] = smart_money_signals.get('strength', 0)
            
            # Phase-specific scoring
            
            # 1. Accumulation: Low prices + smart money buying + decreasing volume
            if price_position < 0.3:  # Near lows
                indicators['accumulation_score'] += 30
            if smart_money_signals.get('accumulation', False):
                indicators['accumulation_score'] += 25
            if volume_ratio < 0.8:  # Decreasing volume
                indicators['accumulation_score'] += 20
            if -0.1 < price_momentum < 0.05:  # Stable/slightly up prices
                indicators['accumulation_score'] += 25
            
            # 2. Markup: Rising prices + increasing volume + public participation
            if price_momentum > 0.05:  # Rising prices
                indicators['markup_score'] += 35
            if volume_ratio > 1.2:  # Increasing volume
                indicators['markup_score'] += 30
            if 0.3 < price_position < 0.8:  # Mid-range prices
                indicators['markup_score'] += 20
            if 'rsi14' in recent_df.columns and 50 < recent_df['rsi14'].iloc[-1] < 75:
                indicators['markup_score'] += 15
            
            # 3. Distribution: High prices + volume without price gains + smart money selling
            if price_position > 0.7:  # Near highs
                indicators['distribution_score'] += 30
            if volume_ratio > 1.1 and abs(price_momentum) < 0.03:  # Volume up, price flat
                indicators['distribution_score'] += 35
            if smart_money_signals.get('distribution', False):
                indicators['distribution_score'] += 25
            if 'rsi14' in recent_df.columns and recent_df['rsi14'].iloc[-1] > 70:
                indicators['distribution_score'] += 10
            
            # 4. Markdown: Declining prices + high volume + public selling
            if price_momentum < -0.05:  # Declining prices
                indicators['markdown_score'] += 35
            if volume_ratio > 1.3:  # High volume
                indicators['markdown_score'] += 30
            if price_position < 0.6:  # Below mid-range
                indicators['markdown_score'] += 20
            if 'rsi14' in recent_df.columns and recent_df['rsi14'].iloc[-1] < 30:
                indicators['markdown_score'] += 15
            
            return indicators
            
        except Exception:
            return {'accumulation_score': 0, 'markup_score': 0, 'distribution_score': 0,
                   'markdown_score': 0, 'smart_money_activity': 0, 'public_participation': 0}
    
    def _calculate_trend_strength(self, analysis: Dict[str, Any]) -> int:
        """
        Calculate overall trend strength based on Dow Theory analysis
        """
        try:
            strength = 0
            
            # Primary trend strength
            if analysis['primary_trend'] in ['BULLISH', 'BEARISH']:
                strength += 30
            
            # Trend structure confirmation
            if analysis['trend_structure'] and analysis['trend_structure']['trend_direction'] != 'NEUTRAL':
                structure = analysis['trend_structure']
                if analysis['primary_trend'] == structure['trend_direction']:
                    strength += 25  # Structure confirms primary trend
            
            # Volume confirmation
            if analysis['volume_confirmation']['is_confirmed']:
                strength += 25
                strength += min(20, analysis['volume_confirmation']['volume_quality'] // 5)
            
            # Secondary trend alignment
            if (analysis['secondary_trend'] == analysis['primary_trend'] and 
                analysis['primary_trend'] != 'NEUTRAL'):
                strength += 20
            
            return min(100, strength)
            
        except Exception:
            return 0
    
    def _calculate_confirmation_score(self, analysis: Dict[str, Any]) -> int:
        """
        Calculate Dow Theory confirmation score across multiple factors
        """
        try:
            score = 0
            
            # Multi-timeframe alignment
            if (analysis['primary_trend'] == analysis['secondary_trend'] and 
                analysis['primary_trend'] != 'NEUTRAL'):
                score += 40
            
            # Volume confirmation
            if analysis['volume_confirmation']['is_confirmed']:
                score += 30
            
            # Trend structure confirmation
            if (analysis['trend_structure'] and 
                analysis['trend_structure']['trend_direction'] == analysis['primary_trend']):
                score += 20
            
            # Phase consistency
            phase_indicators = analysis['phase_indicators']
            max_phase_score = max(
                phase_indicators.get('accumulation_score', 0),
                phase_indicators.get('markup_score', 0),
                phase_indicators.get('distribution_score', 0),
                phase_indicators.get('markdown_score', 0)
            )
            if max_phase_score > 60:
                score += 10
            
            return min(100, score)
            
        except Exception:
            return 0
    
    def _calculate_dow_phase_score(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> Dict[str, str]:
        """
        Calculate and determine the current Dow Theory phase
        """
        try:
            phase_indicators = analysis['phase_indicators']
            
            # Get scores for each phase
            accumulation = phase_indicators.get('accumulation_score', 0)
            markup = phase_indicators.get('markup_score', 0)
            distribution = phase_indicators.get('distribution_score', 0)
            markdown = phase_indicators.get('markdown_score', 0)
            
            # Determine dominant phase
            phases = {
                'Accumulation': accumulation,
                'Markup': markup,
                'Distribution': distribution,
                'Markdown': markdown
            }
            
            # Find phase with highest score
            max_phase = max(phases, key=phases.get)
            max_score = phases[max_phase]
            
            # Require minimum threshold for phase identification
            if max_score < 40:
                return {'phase': 'Undefined', 'confidence': max_score}
            
            # Additional validation based on trend analysis
            primary_trend = analysis['primary_trend']
            volume_confirmed = analysis['volume_confirmation']['is_confirmed']
            
            # Refine phase based on trend confirmation
            if max_phase == 'Accumulation':
                if primary_trend == 'BEARISH' and not volume_confirmed:
                    return {'phase': 'Accumulation', 'confidence': max_score}
                elif primary_trend == 'BULLISH':
                    max_score *= 0.7  # Reduce confidence if already trending up
            
            elif max_phase == 'Markup':
                if primary_trend == 'BULLISH' and volume_confirmed:
                    max_score *= 1.2  # Increase confidence
                elif primary_trend != 'BULLISH':
                    max_score *= 0.6
            
            elif max_phase == 'Distribution':
                if primary_trend == 'BULLISH' and not volume_confirmed:
                    return {'phase': 'Distribution', 'confidence': max_score}
                elif primary_trend == 'BEARISH':
                    max_score *= 0.7
            
            elif max_phase == 'Markdown':
                if primary_trend == 'BEARISH' and volume_confirmed:
                    max_score *= 1.2
                elif primary_trend != 'BEARISH':
                    max_score *= 0.6
            
            return {'phase': max_phase, 'confidence': min(100, int(max_score))}
            
        except Exception:
            return {'phase': 'Undefined', 'confidence': 0}
    
    def get_dow_theory_signals(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Generate comprehensive Dow Theory trading signals
        """
        try:
            # Get comprehensive Dow analysis
            analysis = self._comprehensive_dow_analysis(df)
            phase_info = self._calculate_dow_phase_score(df, analysis)
            
            signals = {
                'primary_trend': analysis['primary_trend'],
                'secondary_trend': analysis['secondary_trend'],
                'current_phase': phase_info['phase'],
                'phase_confidence': phase_info['confidence'],
                'trend_strength': analysis['trend_strength'],
                'confirmation_score': analysis['confirmation_score'],
                'volume_confirmed': analysis['volume_confirmation']['is_confirmed'],
                'trend_structure': analysis['trend_structure']['trend_direction'],
                'signal': 'HOLD',
                'signal_strength': 0
            }
            
            # Generate trading signals based on Dow Theory
            if (analysis['confirmation_score'] >= 70 and 
                analysis['trend_strength'] >= 60):
                
                if (analysis['primary_trend'] == 'BULLISH' and 
                    phase_info['phase'] in ['Accumulation', 'Markup']):
                    signals['signal'] = 'BUY'
                    signals['signal_strength'] = min(100, analysis['confirmation_score'] + 
                                                   (10 if phase_info['phase'] == 'Accumulation' else 0))
                
                elif (analysis['primary_trend'] == 'BEARISH' and 
                      phase_info['phase'] in ['Distribution', 'Markdown']):
                    signals['signal'] = 'SELL'
                    signals['signal_strength'] = min(100, analysis['confirmation_score'] + 
                                                   (10 if phase_info['phase'] == 'Distribution' else 0))
            
            return signals
            
        except Exception:
            return {'primary_trend': 'NEUTRAL', 'secondary_trend': 'NEUTRAL',
                   'current_phase': 'Undefined', 'phase_confidence': 0,
                   'trend_strength': 0, 'confirmation_score': 0,
                   'volume_confirmed': False, 'trend_structure': 'NEUTRAL',
                   'signal': 'HOLD', 'signal_strength': 0}
    
    def _enhanced_dow_volume_confirmation(self, df: pd.DataFrame, trend: str, timeframe: str) -> Dict[str, Any]:
        """
        Enhanced Dow Theory volume confirmation analysis integrating with professional volume systems
        """
        try:
            analysis = {
                'confirmation_strength': 0,  # -50 to +50
                'volume_quality': 'Neutral',
                'volume_issue': None,
                'trend_structure_confirmed': False,
                'dow_signals': []
            }
            
            if 'volume' not in df.columns or len(df) < 20 or trend == 'NEUTRAL':
                return analysis
            
            # 1. Use our enhanced Dow Theory volume confirmation
            dow_volume_data = self._analyze_dow_volume_confirmation(df, trend)
            
            # 2. Integrate with volume profile analysis
            volume_profile = self.get_professional_volume_profile(df)
            
            # 3. Calculate comprehensive confirmation strength
            confirmation_score = 0
            
            # Dow Theory volume confirmation (primary factor)
            if dow_volume_data['is_confirmed']:
                confirmation_score += 25
                analysis['dow_signals'].append('Volume confirms trend direction')
                
                # Quality bonus based on volume confirmation strength
                quality_bonus = min(15, dow_volume_data['volume_quality'] // 5)
                confirmation_score += quality_bonus
            else:
                confirmation_score -= 20
                analysis['volume_issue'] = 'not confirming trend'
            
            # Volume trend alignment
            volume_trend = dow_volume_data['volume_trend']
            if ((trend == 'BULLISH' and volume_trend == 'INCREASING') or 
                (trend == 'BEARISH' and volume_trend == 'INCREASING')):
                confirmation_score += 10
                analysis['dow_signals'].append(f'Volume trend {volume_trend.lower()} supports movement')
            elif volume_trend == 'DECREASING':
                confirmation_score -= 10
                if not analysis['volume_issue']:
                    analysis['volume_issue'] = 'decreasing on trend moves'
            
            # Volume profile Point of Control analysis
            if volume_profile and volume_profile.get('poc_price'):
                current_price = df['close'].iloc[-1]
                poc_price = volume_profile['poc_price']
                
                # Check if price action respects volume profile levels
                poc_distance = abs(current_price - poc_price) / current_price
                if poc_distance < 0.02:  # Within 2% of POC
                    if trend == 'BULLISH' and current_price > poc_price:
                        confirmation_score += 8
                        analysis['dow_signals'].append('Price above high-volume POC level')
                    elif trend == 'BEARISH' and current_price < poc_price:
                        confirmation_score += 8
                        analysis['dow_signals'].append('Price below high-volume POC level')
            
            # Timeframe-specific adjustments
            timeframe_multiplier = {
                '1m': 0.5,   # Less reliable on very short timeframes
                '5m': 0.7,
                '15m': 0.8,
                '30m': 0.9,
                '1h': 1.0,
                '4h': 1.2,   # More reliable on higher timeframes
                '1d': 1.3,
                '1w': 1.4
            }.get(timeframe, 1.0)
            
            confirmation_score = int(confirmation_score * timeframe_multiplier)
            
            # Trend structure confirmation using peak/trough analysis
            trend_structure = self._identify_dow_trend_structure(df)
            if trend_structure['trend_direction'] == trend:
                confirmation_score += 10
                analysis['trend_structure_confirmed'] = True
                analysis['dow_signals'].append('Trend structure confirms direction')
            
            # Final scoring and quality assessment
            analysis['confirmation_strength'] = max(-50, min(50, confirmation_score))
            
            if confirmation_score >= 30:
                analysis['volume_quality'] = 'Excellent'
            elif confirmation_score >= 15:
                analysis['volume_quality'] = 'Good'
            elif confirmation_score >= 0:
                analysis['volume_quality'] = 'Fair'
            elif confirmation_score >= -15:
                analysis['volume_quality'] = 'Weak'
            else:
                analysis['volume_quality'] = 'Poor'
            
            return analysis
            
        except Exception as e:
            print(f"Error in enhanced Dow volume confirmation: {e}")
            return {'confirmation_strength': 0, 'volume_quality': 'Neutral', 
                   'volume_issue': 'analysis error', 'trend_structure_confirmed': False, 
                   'dow_signals': []}
    
    def get_enhanced_dow_signals_integration(self, df: pd.DataFrame, direction: str) -> Dict[str, Any]:
        """
        Get comprehensive Dow Theory signals that integrate with our crypto signal analysis
        """
        try:
            # Get our comprehensive Dow Theory analysis
            dow_signals = self.get_dow_theory_signals(df)
            
            # Calculate signal alignment bonus for main signal confidence
            alignment_bonus = 0
            signal_details = []
            
            # Check trend alignment
            if dow_signals['primary_trend'] == direction.replace('LONG', 'BULLISH').replace('SHORT', 'BEARISH'):
                alignment_bonus += 15
                signal_details.append(f"Primary trend {dow_signals['primary_trend']} aligns")
            
            # Check phase alignment
            current_phase = dow_signals['current_phase']
            if ((direction == 'LONG' and current_phase in ['Accumulation', 'Markup']) or
                (direction == 'SHORT' and current_phase in ['Distribution', 'Markdown'])):
                alignment_bonus += 12
                signal_details.append(f"Market phase '{current_phase}' supports {direction}")
            
            # Volume confirmation bonus
            if dow_signals['volume_confirmed']:
                alignment_bonus += 10
                signal_details.append("Volume confirms trend direction")
            
            # Trend strength bonus
            if dow_signals['trend_strength'] >= 70:
                alignment_bonus += 8
                signal_details.append(f"Strong trend strength ({dow_signals['trend_strength']}%)")
            elif dow_signals['trend_strength'] >= 50:
                alignment_bonus += 5
            
            # Confirmation score bonus
            if dow_signals['confirmation_score'] >= 70:
                alignment_bonus += 8
                signal_details.append(f"High confirmation score ({dow_signals['confirmation_score']}%)")
            
            # Signal strength assessment
            if dow_signals['signal'] == 'BUY' and direction == 'LONG':
                alignment_bonus += dow_signals['signal_strength'] // 10
                signal_details.append(f"Dow Theory BUY signal (strength: {dow_signals['signal_strength']})")
            elif dow_signals['signal'] == 'SELL' and direction == 'SHORT':
                alignment_bonus += dow_signals['signal_strength'] // 10
                signal_details.append(f"Dow Theory SELL signal (strength: {dow_signals['signal_strength']})")
            elif dow_signals['signal'] != 'HOLD':
                alignment_bonus -= 10  # Conflicting signal penalty
                signal_details.append(f"Conflicting Dow signal: {dow_signals['signal']}")
            
            return {
                'alignment_bonus': min(50, alignment_bonus),  # Cap at +50
                'dow_signals': dow_signals,
                'signal_details': signal_details,
                'overall_dow_strength': (dow_signals['trend_strength'] + dow_signals['confirmation_score']) // 2
            }
            
        except Exception:
            return {'alignment_bonus': 0, 'dow_signals': {}, 'signal_details': [], 'overall_dow_strength': 0}
    
    def _fit_channel(self, df: pd.DataFrame, min_touches: int = 2, r2_threshold: float = 0.7) -> Optional[Dict[str, Any]]:
        if len(df) < 20:
            return None

        x_indices = np.arange(len(df)).reshape(-1, 1)
        mid_price = (df['high'] + df['low']) / 2

        # Fit central regression line
        reg = LinearRegression().fit(x_indices, mid_price)
        base_line = reg.predict(x_indices)

        # Check R-squared for goodness of fit
        r2 = r2_score(mid_price, base_line)
        if r2 < r2_threshold:
            return None

        # Calculate residuals and find quantile offsets for bands
        res_high = df['high'] - base_line
        res_low = df['low'] - base_line
        upper_offset = np.percentile(res_high, 95)
        lower_offset = np.percentile(res_low, 5)

        upper_band = base_line + upper_offset
        lower_band = base_line + lower_offset

        # Validate touches using an ATR-based tolerance
        if 'atr' not in df.columns or df['atr'].isnull().all(): return None # ATR must exist
        atr_mean = df['atr'].mean()
        if atr_mean == 0: return None

        touch_tolerance = atr_mean * 0.5 # A touch is within 0.5 ATR of the band

        upper_touches = np.sum(np.abs(df['high'] - upper_band) < touch_tolerance)
        lower_touches = np.sum(np.abs(df['low'] - lower_band) < touch_tolerance)

        if upper_touches < min_touches or lower_touches < min_touches:
            return None

        # All checks passed, return channel data
        return {
            'slope': reg.coef_[0],
            'intercept': reg.intercept_,
            'upper_offset': upper_offset,
            'lower_offset': lower_offset,
            'r2': r2,
            'upper_touches': upper_touches,
            'lower_touches': lower_touches,
            'avg_atr': atr_mean
        }

    def _detect_chart_patterns(self, df: pd.DataFrame, 
                            support_levels: List[Tuple[float, int]],
                            resistance_levels: List[Tuple[float, int]],
                            all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect chart patterns in price data
        
        Args:
            df: DataFrame with price and indicator data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with pattern information
        """
        if df is None or len(df) < 30:
            return {"pattern": "unknown", "confidence": 0, "direction": "NEUTRAL"}
        
        try:
            patterns = []

            # --- 1. Run Consolidated, Multi-Pattern Detectors ---
            triangle_wedge_results = self._detect_triangles_and_wedges(df, all_timeframes, support_levels, resistance_levels)
            if triangle_wedge_results:
                patterns.extend(triangle_wedge_results)

            # --- 2. Run Single, High-Probability Pattern Detectors ---
            single_pattern_detectors = [
                self._detect_head_and_shoulders_pattern,
                self._detect_inverse_head_and_shoulders_pattern,
                self._detect_double_top,
                self._detect_double_bottom,
                self._detect_channel_up,
                self._detect_channel_down,
                self._detect_sideways_channel_strategy
            ]

            for detector in single_pattern_detectors:
                result = detector(df, support_levels, resistance_levels, all_timeframes)
                if result and result.get("detected"):
                    patterns.append(result)

            # --- 3. Contextual and Fallback Signals ---
            if not patterns:
                divergence_result = self._detect_divergence_signal(df, "Divergence Signal", all_timeframes)
                if divergence_result and divergence_result.get("detected"):
                    patterns.append(divergence_result)
                else:
                    sr_test_results = self._detect_support_resistance_tests(df, support_levels, resistance_levels, all_timeframes)
                    if sr_test_results:
                        patterns.extend(sr_test_results)

            # --- 4. Final Evaluation ---
            if not patterns:
                indicator_signals = self._detect_indicator_signals(df, support_levels, resistance_levels, all_timeframes)
                if indicator_signals:
                    patterns.extend(indicator_signals)
                else:
                    return {"pattern": "unknown", "confidence": 0, "direction": "NEUTRAL"}

            patterns.sort(key=lambda x: x.get("confidence", 0), reverse=True)
            return patterns[0]

        except Exception as e:
            print(f"Error detecting chart patterns: {str(e)}")
            traceback.print_exc()
            return {"pattern": "error", "confidence": 0, "direction": "NEUTRAL"}

    def detect_chart_patterns(self, df: pd.DataFrame, 
                        support_levels: List[Tuple[float, int]],
                        resistance_levels: List[Tuple[float, int]],
                        all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect chart patterns in price data
        
        Args:
            df: DataFrame with price and indicator data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with pattern information
        """
        return self._detect_chart_patterns(df, support_levels, resistance_levels, all_timeframes)

    def _detect_double_patterns(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        Detect double top and double bottom patterns

        Args:
            df: DataFrame with price data

        Returns:
            List of detected patterns
        """
        patterns = []

        # Need at least 30 candles for reliable pattern detection
        if len(df) < 30:
            return patterns

        try:
            # Use only recent data for better relevance
            recent_df = df.tail(60).copy()

            # Find local peaks and troughs
            peaks = []
            troughs = []

            for i in range(5, len(recent_df) - 5):
                # Local peak (5 periods before and after)
                if all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 6)) and \
                   all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 6)):
                    peaks.append((i, recent_df['high'].iloc[i]))

                # Local trough (5 periods before and after)
                if all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 6)) and \
                   all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 6)):
                    troughs.append((i, recent_df['low'].iloc[i]))

            # Double Top Detection
            if len(peaks) >= 2:
                for i in range(len(peaks) - 1):
                    for j in range(i + 1, min(i + 4, len(peaks))):
                        # Check if two peaks are at similar levels (within 1.5%)
                        if abs(peaks[j][1] - peaks[i][1]) / peaks[i][1] <= 0.015:
                            # Check if there's a significant valley between them
                            valley = False
                            for k in range(i + 1, j):
                                if k < len(recent_df):
                                    mid_idx = peaks[i][0] + (peaks[j][0] - peaks[i][0]) // 2
                                    if mid_idx < len(recent_df) and mid_idx > 0:
                                        if recent_df['low'].iloc[mid_idx] < min(peaks[i][1], peaks[j][1]) * 0.97:
                                            valley = True

                            if valley and (peaks[j][0] - peaks[i][0]) >= 5:  # Minimum 5 periods apart
                                # Confirmed double top if current price is below the valley
                                current_price = recent_df['close'].iloc[-1]
                                mid_idx = peaks[i][0] + (peaks[j][0] - peaks[i][0]) // 2
                                if mid_idx < len(recent_df) and mid_idx > 0:
                                    valley_price = recent_df['low'].iloc[mid_idx]

                                    # Complete pattern only if price broke below valley
                                    if current_price < valley_price:
                                        # Calculate confidence based on:
                                        # - How similar the two peaks are
                                        # - Distance between peaks (ideally 10-20 periods)
                                        # - How far price has broken below valley
                                        similarity = 100 - (abs(peaks[j][1] - peaks[i][1]) / peaks[i][1] * 100)
                                        distance_factor = 100 - abs((peaks[j][0] - peaks[i][0] - 15) * 3)
                                        breakdown_factor = (valley_price - current_price) / valley_price * 100 * 2

                                        confidence = min(95, (similarity * 0.4 + distance_factor * 0.3 + breakdown_factor * 0.3))

                                        # Add volume confirmation
                                        if 'volume' in recent_df.columns:
                                            volume_at_first_peak = recent_df['volume'].iloc[peaks[i][0]]
                                            volume_at_second_peak = recent_df['volume'].iloc[peaks[j][0]]
                                            if volume_at_second_peak < volume_at_first_peak * 0.8:
                                                confidence += 10

                                        patterns.append({
                                            "pattern": "double_top",
                                            "confidence": min(95, confidence),
                                            "direction": "SHORT"
                                        })

            # Double Bottom Detection (similar logic, inverted)
            if len(troughs) >= 2:
                for i in range(len(troughs) - 1):
                    for j in range(i + 1, min(i + 4, len(troughs))):
                        # Check if two troughs are at similar levels (within 1.5%)
                        if abs(troughs[j][1] - troughs[i][1]) / troughs[i][1] <= 0.015:
                            # Check if there's a significant peak between them
                            peak = False
                            for k in range(i + 1, j):
                                if k < len(recent_df):
                                    mid_idx = troughs[i][0] + (troughs[j][0] - troughs[i][0]) // 2
                                    if mid_idx < len(recent_df) and mid_idx > 0:
                                        if recent_df['high'].iloc[mid_idx] > max(troughs[i][1], troughs[j][1]) * 1.03:
                                            peak = True

                            if peak and (troughs[j][0] - troughs[i][0]) >= 5:  # Minimum 5 periods apart
                                # Confirmed double bottom if current price is above the peak
                                current_price = recent_df['close'].iloc[-1]
                                mid_idx = troughs[i][0] + (troughs[j][0] - troughs[i][0]) // 2
                                if mid_idx < len(recent_df) and mid_idx > 0:
                                    peak_price = recent_df['high'].iloc[mid_idx]

                                    # Complete pattern only if price broke above peak
                                    if current_price > peak_price:
                                        # Calculate confidence based on multiple factors
                                        similarity = 100 - (abs(troughs[j][1] - troughs[i][1]) / troughs[i][1] * 100)
                                        distance_factor = 100 - abs((troughs[j][0] - troughs[i][0] - 15) * 3)
                                        breakout_factor = (current_price - peak_price) / peak_price * 100 * 2

                                        confidence = min(95, (similarity * 0.4 + distance_factor * 0.3 + breakout_factor * 0.3))

                                        # Add volume confirmation
                                        if 'volume' in recent_df.columns:
                                            volume_at_first_trough = recent_df['volume'].iloc[troughs[i][0]]
                                            volume_at_second_trough = recent_df['volume'].iloc[troughs[j][0]]
                                            if volume_at_second_trough > volume_at_first_trough * 1.2:
                                                confidence += 10

                                        patterns.append({
                                            "pattern": "double_bottom",
                                            "confidence": min(95, confidence),
                                            "direction": "LONG"
                                        })

            return patterns

        except Exception as e:
            print(f"Error in double pattern detection: {str(e)}")
            return patterns

    def _detect_head_and_shoulders(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        Detect head and shoulders and inverse head and shoulders patterns
        
        Args:
            df: DataFrame with price data
            
        Returns:
            List of detected patterns
        """
        patterns = []
        
        # Need at least 50 candles for reliable pattern detection
        if len(df) < 50:
            return patterns
        
        try:
            # Use recent data
            recent_df = df.tail(60).copy()
            
            # Find local peaks and troughs
            peaks = []
            troughs = []
            
            for i in range(5, len(recent_df) - 5):
                # Local peak (5 periods before and after)
                if all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 6)) and \
                   all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 6)):
                    peaks.append((i, recent_df['high'].iloc[i]))
                
                # Local trough (5 periods before and after)
                if all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 6)) and \
                   all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 6)):
                    troughs.append((i, recent_df['low'].iloc[i]))
            
            # Head and Shoulders Pattern
            if len(peaks) >= 3:
                for i in range(len(peaks) - 2):
                    # Look for three consecutive peaks where the middle one is higher
                    peak1_idx, peak1_val = peaks[i]
                    peak2_idx, peak2_val = peaks[i+1]
                    peak3_idx, peak3_val = peaks[i+2]
                    
                    # Check if middle peak (head) is higher than the shoulders
                    if peak2_val > peak1_val * 1.02 and peak2_val > peak3_val * 1.02:
                        # Check if shoulders are at similar levels (within 5%)
                        if abs(peak3_val - peak1_val) / peak1_val <= 0.05:
                            # Check for neckline (connecting the troughs between peaks)
                            trough_indices = []
                            
                            # Find troughs between peaks
                            for trough_idx, trough_val in troughs:
                                if peak1_idx < trough_idx < peak2_idx:
                                    trough_indices.append((trough_idx, trough_val))
                                if peak2_idx < trough_idx < peak3_idx:
                                    trough_indices.append((trough_idx, trough_val))
                            
                            if len(trough_indices) >= 2:
                                # Calculate neckline
                                trough1_idx, trough1_val = trough_indices[0]
                                trough2_idx, trough2_val = trough_indices[-1]
                                
                                # Check if neckline is relatively flat (slope less than 5%)
                                neckline_slope = (trough2_val - trough1_val) / max(trough1_val, trough2_val)
                                if abs(neckline_slope) <= 0.05:
                                    # Pattern is valid, check if it's completed (price broke below neckline)
                                    current_price = recent_df['close'].iloc[-1]
                                    
                                    # Extrapolate neckline to current position
                                    periods_since_trough2 = len(recent_df) - trough2_idx - 1
                                    neckline_current = trough2_val + (neckline_slope * trough2_val * periods_since_trough2 / (trough2_idx - trough1_idx))
                                    
                                    if current_price < neckline_current:
                                        # Calculate confidence based on pattern quality
                                        shoulder_symmetry = 100 - (abs(peak3_val - peak1_val) / peak1_val * 100)
                                        head_prominence = ((peak2_val / max(peak1_val, peak3_val)) - 1) * 100
                                        neckline_flatness = (1 - abs(neckline_slope)) * 100
                                        breakdown = (neckline_current - current_price) / neckline_current * 100 * 2
                                        
                                        confidence = min(95, (shoulder_symmetry * 0.3 + head_prominence * 0.2 + 
                                                        neckline_flatness * 0.3 + breakdown * 0.2))
                                        
                                        patterns.append({
                                            "pattern": "head_and_shoulders",
                                            "confidence": confidence,
                                            "direction": "SHORT"
                                        })
            
            # Inverse Head and Shoulders (similar logic, inverted)
            if len(troughs) >= 3:
                for i in range(len(troughs) - 2):
                    # Look for three consecutive troughs where the middle one is lower
                    trough1_idx, trough1_val = troughs[i]
                    trough2_idx, trough2_val = troughs[i+1]
                    trough3_idx, trough3_val = troughs[i+2]
                    
                    # Check if middle trough (head) is lower than the shoulders
                    if trough2_val < trough1_val * 0.98 and trough2_val < trough3_val * 0.98:
                        # Check if shoulders are at similar levels (within 5%)
                        if abs(trough3_val - trough1_val) / trough1_val <= 0.05:
                            # Check for neckline (connecting the peaks between troughs)
                            peak_indices = []
                            
                            # Find peaks between troughs
                            for peak_idx, peak_val in peaks:
                                if trough1_idx < peak_idx < trough2_idx:
                                    peak_indices.append((peak_idx, peak_val))
                                if trough2_idx < peak_idx < trough3_idx:
                                    peak_indices.append((peak_idx, peak_val))
                            
                            if len(peak_indices) >= 2:
                                # Calculate neckline
                                peak1_idx, peak1_val = peak_indices[0]
                                peak2_idx, peak2_val = peak_indices[-1]
                                
                                # Check if neckline is relatively flat (slope less than 5%)
                                neckline_slope = (peak2_val - peak1_val) / max(peak1_val, peak2_val)
                                if abs(neckline_slope) <= 0.05:
                                    # Pattern is valid, check if it's completed (price broke above neckline)
                                    current_price = recent_df['close'].iloc[-1]
                                    
                                    # Extrapolate neckline to current position
                                    periods_since_peak2 = len(recent_df) - peak2_idx - 1
                                    neckline_current = peak2_val + (neckline_slope * peak2_val * periods_since_peak2 / (peak2_idx - peak1_idx))
                                    
                                    if current_price > neckline_current:
                                        # Calculate confidence
                                        shoulder_symmetry = 100 - (abs(trough3_val - trough1_val) / trough1_val * 100)
                                        head_prominence = ((min(trough1_val, trough3_val) / trough2_val) - 1) * 100
                                        neckline_flatness = (1 - abs(neckline_slope)) * 100
                                        breakout = (current_price - neckline_current) / neckline_current * 100 * 2
                                        
                                        confidence = min(95, (shoulder_symmetry * 0.3 + head_prominence * 0.2 + 
                                                        neckline_flatness * 0.3 + breakout * 0.2))
                                        
                                        patterns.append({
                                            "pattern": "inverse_head_and_shoulders",
                                            "confidence": confidence,
                                            "direction": "LONG"
                                        })
            
            return patterns
            
        except Exception as e:
            print(f"Error in head and shoulders detection: {str(e)}")
            return patterns
    
    def _detect_triangles_and_wedges(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None, support_levels: List[Tuple[float, int]] = None, resistance_levels: List[Tuple[float, int]] = None) -> List[Dict[str, Any]]:
        """
        ENHANCED Triangle and Wedge detection with professional-grade validation
        Key improvements:
        - Prior trend context validation
        - Volume contraction analysis
        - Breakout volume spike confirmation
        """
        patterns = []
        if len(df) < 50:
            return patterns

        try:
            from itertools import combinations
            
            recent_df = df.tail(100).copy()
            # Ensure ATR is calculated
            if 'atr' not in recent_df.columns or recent_df['atr'].isnull().all():
                # Temporarily calculate indicators for this recent_df slice
                temp_df_full = self.calculate_technical_indicators(df.copy())
                recent_df = temp_df_full.tail(100).copy()

            if 'atr' not in recent_df.columns or recent_df['atr'].isnull().all():
                print("ATR could not be calculated for triangle/wedge detection.")
                return patterns

            # 1. ENHANCED PIVOT DETECTION
            atr_prominence = recent_df['atr'].mean() * 0.5
            if atr_prominence == 0: atr_prominence = recent_df['close'].mean() * 0.01 # Fallback
            min_distance = 5

            peak_indices, _ = find_peaks(recent_df['high'], prominence=atr_prominence, distance=min_distance)
            significant_peaks = [(i, recent_df['high'].iloc[i]) for i in peak_indices]

            trough_indices, _ = find_peaks(-recent_df['low'], prominence=atr_prominence, distance=min_distance)
            significant_troughs = [(i, recent_df['low'].iloc[i]) for i in trough_indices]

            if len(significant_peaks) < 2 or len(significant_troughs) < 2:
                return patterns

            # Iterate through combinations of pivots
            for peak_points in combinations(significant_peaks, 2):
                for trough_points in combinations(significant_troughs, 2):

                    # Ensure points are ordered correctly in time
                    if peak_points[0][0] >= peak_points[1][0] or trough_points[0][0] >= trough_points[1][0]:
                        continue

                    # Fit trendlines using Linear Regression
                    upper_x = np.array([p[0] for p in peak_points]).reshape(-1, 1)
                    upper_y = np.array([p[1] for p in peak_points])
                    lr_upper = LinearRegression().fit(upper_x, upper_y)
                    upper_slope = lr_upper.coef_[0]

                    lower_x = np.array([p[0] for p in trough_points]).reshape(-1, 1)
                    lower_y = np.array([p[1] for p in trough_points])
                    lr_lower = LinearRegression().fit(lower_x, lower_y)
                    lower_slope = lr_lower.coef_[0]

                    # 2. VALIDATE TRENDLINE TOUCHES
                    atr_mean_for_touch = recent_df['atr'].mean()

                    has_upper_touch = False
                    for p_idx, p_price in significant_peaks:
                        if peak_points[0][0] < p_idx < peak_points[1][0]:
                            line_val_at_p = lr_upper.predict([[p_idx]])[0]
                            if abs(p_price - line_val_at_p) < (atr_mean_for_touch * 0.5):
                                has_upper_touch = True
                                break

                    has_lower_touch = False
                    for p_idx, p_price in significant_troughs:
                        if trough_points[0][0] < p_idx < trough_points[1][0]:
                            line_val_at_p = lr_lower.predict([[p_idx]])[0]
                            if abs(p_price - line_val_at_p) < (atr_mean_for_touch * 0.5):
                                has_lower_touch = True
                                break

                    if not (has_upper_touch and has_lower_touch):
                        continue

                    # Geometric Pattern Identification
                    pattern_name = None
                    is_bullish = None # None for symmetrical, True for bullish breakout, False for bearish

                    # Check for Ascending Triangle (flat top, rising bottom)
                    if abs(upper_slope / recent_df['close'].mean()) < 0.0005 and lower_slope > 0:
                        pattern_name = "Ascending Triangle"
                        is_bullish = True
                    # Check for Descending Triangle (falling top, flat bottom)
                    elif upper_slope < 0 and abs(lower_slope / recent_df['close'].mean()) < 0.0005:
                        pattern_name = "Descending Triangle"
                        is_bullish = False
                    # Check for Rising Wedge (both lines rising, bottom is steeper)
                    elif upper_slope > 0 and lower_slope > 0 and lower_slope > upper_slope:
                        pattern_name = "Rising Wedge"
                        is_bullish = False # Bearish pattern
                    # Check for Falling Wedge (both lines falling, top is steeper)
                    elif upper_slope < 0 and lower_slope < 0 and abs(upper_slope) > abs(lower_slope):
                        pattern_name = "Falling Wedge"
                        is_bullish = True # Bullish pattern

                    if not pattern_name:
                        continue

                    # --- Start New Professional Validation ---

                    # 1. Prior Trend Context (MANDATORY FILTER)
                    # Ensure a strong, impulsive trend exists before the pattern starts forming

                    # FIX: Calculate indicators on the main DataFrame first if they don't exist.
                    if 'ema20' not in df.columns or 'ema50' not in df.columns:
                        df = self.calculate_technical_indicators(df)
                        # If calculation still fails (e.g., df is too short), we can't proceed.
                        if 'ema20' not in df.columns:
                            continue

                    # Now, safely check for the trend context
                    pattern_start_idx = min(peak_points[0][0], trough_points[0][0])
                    if pattern_start_idx >= 20:
                        # Get trend context before pattern formation
                        trend_context_df = df.iloc[pattern_start_idx-20:pattern_start_idx]
                        
                        # Check for NaN values, which can occur at the start of the series
                        if trend_context_df['ema20'].isnull().any() or trend_context_df['ema50'].isnull().any():
                            continue # Skip if indicators are NaN for this specific slice

                        ema20_before = trend_context_df['ema20'].iloc[-1]
                        ema50_before = trend_context_df['ema50'].iloc[-1]
                        
                        # For bull flag, 20-period EMA should be clearly above the 50-period EMA
                        # For bear flag, 20-period EMA should be clearly below the 50-period EMA
                        if is_bullish and not (ema20_before > ema50_before):
                            continue  # No strong bullish trend before pattern
                        elif is_bullish is False and not (ema20_before < ema50_before):
                            continue  # No strong bearish trend before pattern

                    # 2. Volume Contraction (MANDATORY SCORING)
                    # The average volume during the consolidation phase must be lower than the preceding trend
                    consolidation_start_idx = pattern_start_idx
                    consolidation_end_idx = len(recent_df) - 1
                    
                    # Get volume during consolidation phase
                    consolidation_volume = recent_df['volume'].iloc[consolidation_start_idx:consolidation_end_idx].mean()
                    
                    # Get volume during preceding trend (flagpole)
                    if consolidation_start_idx >= 20:
                        trend_volume = recent_df['volume'].iloc[consolidation_start_idx-20:consolidation_start_idx].mean()
                    else:
                        trend_volume = recent_df['volume'].iloc[:consolidation_start_idx].mean()
                    
                    # If volume increases during consolidation, it's a red flag
                    if consolidation_volume > trend_volume:
                        continue  # Pattern invalidated due to volume increase during consolidation

                    # 3. Breakout Volume Spike (MANDATORY FILTER)
                    # The candle that breaks out must have volume at least 1.75x the average volume of consolidation
                    last_candle = recent_df.iloc[-1]
                    breakout_volume_ok = last_candle['volume'] > consolidation_volume * 1.75
                    
                    if not breakout_volume_ok:
                        continue  # Breakout not confirmed due to insufficient volume

                    # 3. ENHANCED BREAKOUT CONFIRMATION
                    breakout_x = len(recent_df) - 1
                    upper_trend_val = lr_upper.predict([[breakout_x]])[0]
                    lower_trend_val = lr_lower.predict([[breakout_x]])[0]

                    is_breakout = False
                    # Bullish breakout check
                    if is_bullish and last_candle['close'] > upper_trend_val:
                        is_breakout = True
                    # Bearish breakout check
                    elif is_bullish is False and last_candle['close'] < lower_trend_val:
                        is_breakout = True

                    if not is_breakout:
                        continue

                    # --- Scoring and Signal Generation ---
                    quality_score = 60 # Base score for a confirmed pattern
                    
                    # Add Volume Contraction Score
                    if consolidation_volume < trend_volume * 0.8:
                        quality_score += 15
                    elif consolidation_volume < trend_volume:
                        quality_score += 10

                    # Add Breakout Volume Spike Score
                    if breakout_volume_ok: 
                        quality_score += 15

                    # Apex Proximity Score
                    c_upper = lr_upper.intercept_
                    c_lower = lr_lower.intercept_
                    if upper_slope != lower_slope:
                        apex_x = (c_lower - c_upper) / (upper_slope - lower_slope)
                        start_x = min(peak_points[0][0], trough_points[0][0])
                        if apex_x > start_x:
                            apex_proximity_ratio = (breakout_x - start_x) / (apex_x - start_x) if apex_x > start_x else 0
                            if 0.60 <= apex_proximity_ratio <= 0.85:
                                quality_score += 10

                    # CRITICAL: Confidence Filter
                    if quality_score < 70: # Set a high bar for quality
                        continue

                    direction = "LONG" if is_bullish else "SHORT"

                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    final_confidence = self.calculate_enhanced_confidence(
                        pattern_name, direction, df, [], [], trends,
                        base_confidence_override=quality_score
                    )

                    patterns.append({
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": final_confidence,
                        "direction": direction,
                        "quality_score": quality_score,
                        "details": {
                            "consolidation_volume": consolidation_volume,
                            "trend_volume": trend_volume,
                            "breakout_volume": last_candle['volume'],
                            "volume_contraction_ratio": consolidation_volume / trend_volume if trend_volume > 0 else 0,
                            "breakout_volume_ratio": last_candle['volume'] / consolidation_volume if consolidation_volume > 0 else 0
                        }
                    })

            return patterns

        except Exception as e:
            print(f"Error in enhanced triangle/wedge detection: {e}")
            traceback.print_exc()
            return patterns
    
    def _detect_support_resistance_tests(self, df: pd.DataFrame, support_levels: List[Tuple[float, int]],
                                    resistance_levels: List[Tuple[float, int]], all_timeframes: Dict[str, pd.DataFrame] = None) -> List[Dict[str, Any]]:
        """
        Detect tests of support and resistance levels
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            List of detected patterns
        """
        patterns = []
        
        if df is None or len(df) < 10:
            return patterns
        
        try:
            # Get recent data
            recent_df = df.tail(10).copy()
            current_price = recent_df['close'].iloc[-1]
            recent_low = recent_df['low'].iloc[-1]
            recent_high = recent_df['high'].iloc[-1]
            
            # Check for resistance tests
            for level, strength in resistance_levels:
                # Price approached resistance (within 1%)
                price_to_level = (level - recent_high) / level
                
                if 0 <= price_to_level <= 0.01:
                    # Check if price was rejected (closed below high)
                    rejection_strength = (recent_high - current_price) / recent_high
                    
                    if rejection_strength > 0.002:  # At least 0.2% rejection
                        # Pattern detected, use new confidence calculation
                        pattern_name = "Resistance Rejection"
                        direction = "SHORT"
                        
                        # Get trend information
                        trends = {}
                        if all_timeframes:
                            for tf in ["1h", "4h", "1d", "1w"]:
                                if tf in all_timeframes:
                                    trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                        
                        # Use new confidence method
                        confidence = self.calculate_signal_confidence(
                            pattern_name, direction, df, support_levels, resistance_levels, trends
                        )
                        
                        patterns.append({
                            "detected": True,
                            "pattern": pattern_name,
                            "confidence": confidence,
                            "direction": direction
                        })
                        break  # Only consider the closest resistance
                
                # Check for resistance breakout
                elif -0.005 <= price_to_level <= 0:  # Price just broke through resistance
                    breakout_strength = (current_price - level) / level
                    
                    if breakout_strength > 0.002:  # At least 0.2% breakout
                        # Check for confirmation (closed above resistance)
                        if current_price > level:
                            # Pattern detected, use new confidence calculation
                            pattern_name = "Resistance Breakout"
                            direction = "LONG"
                            
                            # Get trend information
                            trends = {}
                            if all_timeframes:
                                for tf in ["1h", "4h", "1d", "1w"]:
                                    if tf in all_timeframes:
                                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                            
                            # Use new confidence method
                            confidence = self.calculate_signal_confidence(
                                pattern_name, direction, df, support_levels, resistance_levels, trends
                            )
                            
                            patterns.append({
                                "detected": True,
                                "pattern": pattern_name,
                                "confidence": confidence,
                                "direction": direction
                            })
                            break  # Only consider the closest resistance
            
            # Check for support tests
            for level, strength in support_levels:
                # Price approached support (within 1%)
                price_to_level = (recent_low - level) / level
                
                if 0 <= price_to_level <= 0.01:
                    # Check if price bounced (closed above low)
                    bounce_strength = (current_price - recent_low) / recent_low
                    
                    if bounce_strength > 0.002:  # At least 0.2% bounce
                        # Pattern detected, use new confidence calculation
                        pattern_name = "Support Bounce"
                        direction = "LONG"
                        
                        # Get trend information
                        trends = {}
                        if all_timeframes:
                            for tf in ["1h", "4h", "1d", "1w"]:
                                if tf in all_timeframes:
                                    trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                        
                        # Use new confidence method
                        confidence = self.calculate_signal_confidence(
                            pattern_name, direction, df, support_levels, resistance_levels, trends
                        )
                        
                        patterns.append({
                            "detected": True,
                            "pattern": pattern_name,
                            "confidence": confidence,
                            "direction": direction
                        })
                        break  # Only consider the closest support
                
                # Check for support breakdown
                elif -0.005 <= price_to_level <= 0:  # Price just broke through support
                    breakdown_strength = (level - current_price) / level
                    
                    if breakdown_strength > 0.002:  # At least 0.2% breakdown
                        # Check for confirmation (closed below support)
                        if current_price < level:
                            # Pattern detected, use new confidence calculation
                            pattern_name = "Support Breakdown"
                            direction = "SHORT"
                            
                            # Get trend information
                            trends = {}
                            if all_timeframes:
                                for tf in ["1h", "4h", "1d", "1w"]:
                                    if tf in all_timeframes:
                                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                            
                            # Use new confidence method
                            confidence = self.calculate_signal_confidence(
                                pattern_name, direction, df, support_levels, resistance_levels, trends
                            )
                            
                            patterns.append({
                                "detected": True,
                                "pattern": pattern_name,
                                "confidence": confidence,
                                "direction": direction
                            })
                            break  # Only consider the closest support
            
            return patterns
            
        except Exception as e:
            print(f"Error detecting support/resistance tests: {str(e)}")
            return patterns
    
    def _detect_indicator_signals(self, df: pd.DataFrame, support_levels: List[Tuple[float, float]] = None,
                            resistance_levels: List[Tuple[float, float]] = None, all_timeframes: Dict[str, pd.DataFrame] = None) -> List[Dict[str, Any]]:
        """
        Detect trading signals based on technical indicators when no chart patterns are found
        
        Args:
            df: DataFrame with price and indicator data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            List of detected signals
        """
        signals = []
        
        if df is None or len(df) < 30:
            return signals
        
        try:
            # Make sure we have the minimum required indicators
            required_columns = ['rsi14', 'macd', 'macd_signal', 'ema50']
            
            if not all(col in df.columns for col in required_columns):
                return signals
            
            # Get recent data
            current = df.iloc[-1]
            previous = df.iloc[-2]
            
            # RSI oversold/overbought signals
            rsi = current['rsi14']
            
            if rsi < 30:  # Oversold
                # Check if RSI is turning up
                rsi_prev = previous['rsi14']
                if rsi > rsi_prev:
                    pattern_name = "RSI Oversold Bullish"
                    direction = "LONG"
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use new confidence method
                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    
                    signals.append({
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    })
            
            elif rsi > 70:  # Overbought
                # Check if RSI is turning down
                rsi_prev = previous['rsi14']
                if rsi < rsi_prev:
                    pattern_name = "RSI Overbought Bearish"
                    direction = "SHORT"
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use new confidence method
                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    
                    signals.append({
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    })
            
            # MACD signals
            macd = current['macd']
            macd_signal = current['macd_signal']
            macd_prev = previous['macd']
            macd_signal_prev = previous['macd_signal']
            
            # Bullish MACD crossover
            if macd_prev <= macd_signal_prev and macd > macd_signal:
                # Stronger signal if MACD is below zero and crossing up
                if macd < 0:
                    pattern_name = "MACD Bullish Crossover"
                    direction = "LONG"
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use new confidence method
                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    
                    signals.append({
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    })
            
            # Bearish MACD crossover
            elif macd_prev >= macd_signal_prev and macd < macd_signal:
                # Stronger signal if MACD is above zero and crossing down
                if macd > 0:
                    pattern_name = "MACD Bearish Crossover"
                    direction = "SHORT"
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use new confidence method
                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    
                    signals.append({
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    })
            
            # Moving average signals
            if 'sma50' in df.columns and 'sma200' in df.columns:
                sma50 = current['sma50']
                sma200 = current['sma200']
                sma50_prev = previous['sma50']
                sma200_prev = previous['sma200']
                
                # Golden cross (SMA50 crosses above SMA200)
                if sma50_prev <= sma200_prev and sma50 > sma200:
                    pattern_name = "Golden Cross"
                    direction = "LONG"
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use new confidence method
                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    
                    signals.append({
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    })
                
                # Death cross (SMA50 crosses below SMA200)
                elif sma50_prev >= sma200_prev and sma50 < sma200:
                    pattern_name = "Death Cross"
                    direction = "SHORT"
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use new confidence method
                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    
                    signals.append({
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    })
            
            # Sort by confidence
            signals.sort(key=lambda x: x["confidence"], reverse=True)
            
            # Return the top 3 signals
            return signals[:3]
            
        except Exception as e:
            print(f"Error detecting indicator signals: {str(e)}")
            return signals
    
    # def detect_divergence(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Detect RSI and MACD divergences
        
        Args:
            df: DataFrame with price and indicator data
            
        Returns:
            Dictionary with divergence information
        """
        if df is None or len(df) < 30:
            return {"pattern": "no_divergence", "confidence": 0, "direction": "NEUTRAL"}
        
        try:
            # Check for required columns
            required = ['close', 'rsi14']
            if not all(col in df.columns for col in required):
                return {"pattern": "no_divergence", "confidence": 0, "direction": "NEUTRAL"}
            
            # Get recent data for analysis (focus on last 30 bars)
            recent_df = df.tail(30).copy()
            
            # Find price swing highs and lows
            price_highs = []
            price_lows = []
            
            for i in range(2, len(recent_df) - 2):
                # Price high (2 bars before and after)
                if all(recent_df['close'].iloc[i] >= recent_df['close'].iloc[i-j] for j in range(1, 3)) and \
                   all(recent_df['close'].iloc[i] >= recent_df['close'].iloc[i+j] for j in range(1, 3)):
                    price_highs.append((i, recent_df['close'].iloc[i], recent_df['rsi14'].iloc[i]))
                
                # Price low (2 bars before and after)
                if all(recent_df['close'].iloc[i] <= recent_df['close'].iloc[i-j] for j in range(1, 3)) and \
                   all(recent_df['close'].iloc[i] <= recent_df['close'].iloc[i+j] for j in range(1, 3)):
                    price_lows.append((i, recent_df['close'].iloc[i], recent_df['rsi14'].iloc[i]))
            
            # Need at least 2 highs or lows to detect divergence
            if len(price_highs) >= 2:
                # Check for bearish divergence (price higher but RSI lower)
                last_high = price_highs[-1]
                prev_high = price_highs[-2]
                
                if last_high[1] > prev_high[1] and last_high[2] < prev_high[2]:
                    # Calculate strength of divergence
                    price_change = (last_high[1] / prev_high[1]) - 1
                    rsi_change = (prev_high[2] - last_high[2]) / prev_high[2]
                    
                    # Stronger divergence when price change and RSI change are larger
                    divergence_strength = (price_change + rsi_change) * 100
                    confidence = min(90, 60 + divergence_strength)
                    
                    # Check for bearish confirmation (price starting to turn down)
                    current_price = recent_df['close'].iloc[-1]
                    if current_price < last_high[1]:
                        return {
                            "pattern": "bearish_rsi_divergence",
                            "confidence": confidence,
                            "direction": "SHORT"
                        }
            
            if len(price_lows) >= 2:
                # Check for bullish divergence (price lower but RSI higher)
                last_low = price_lows[-1]
                prev_low = price_lows[-2]
                
                if last_low[1] < prev_low[1] and last_low[2] > prev_low[2]:
                    # Calculate strength of divergence
                    price_change = 1 - (last_low[1] / prev_low[1])
                    rsi_change = (last_low[2] - prev_low[2]) / prev_low[2]
                    
                    # Stronger divergence when price change and RSI change are larger
                    divergence_strength = (price_change + rsi_change) * 100
                    confidence = min(90, 60 + divergence_strength)
                    
                    # Check for bullish confirmation (price starting to turn up)
                    current_price = recent_df['close'].iloc[-1]
                    if current_price > last_low[1]:
                        return {
                            "pattern": "bullish_rsi_divergence",
                            "confidence": confidence,
                            "direction": "LONG"
                        }
            
            # Check for MACD divergence if RSI divergence not found
            if 'macd' in recent_df.columns:
                # MACD bearish divergence
                if len(price_highs) >= 2:
                    last_high = price_highs[-1]
                    prev_high = price_highs[-2]
                    
                    last_macd = recent_df['macd'].iloc[last_high[0]]
                    prev_macd = recent_df['macd'].iloc[prev_high[0]]
                    
                    if last_high[1] > prev_high[1] and last_macd < prev_macd:
                        price_change = (last_high[1] / prev_high[1]) - 1
                        macd_change = (prev_macd - last_macd) / abs(prev_macd) if prev_macd != 0 else 0.1
                        
                        divergence_strength = (price_change + macd_change) * 100
                        confidence = min(85, 55 + divergence_strength)
                        
                        current_price = recent_df['close'].iloc[-1]
                        if current_price < last_high[1]:
                            return {
                                "pattern": "bearish_macd_divergence",
                                "confidence": confidence,
                                "direction": "SHORT"
                            }
                
                # MACD bullish divergence
                if len(price_lows) >= 2:
                    last_low = price_lows[-1]
                    prev_low = price_lows[-2]
                    
                    last_macd = recent_df['macd'].iloc[last_low[0]]
                    prev_macd = recent_df['macd'].iloc[prev_low[0]]
                    
                    if last_low[1] < prev_low[1] and last_macd > prev_macd:
                        price_change = 1 - (last_low[1] / prev_low[1])
                        macd_change = (last_macd - prev_macd) / abs(prev_macd) if prev_macd != 0 else 0.1
                        
                        divergence_strength = (price_change + macd_change) * 100
                        confidence = min(85, 55 + divergence_strength)
                        
                        current_price = recent_df['close'].iloc[-1]
                        if current_price > last_low[1]:
                            return {
                                "pattern": "bullish_macd_divergence",
                                "confidence": confidence,
                                "direction": "LONG"
                            }
            
            # No divergence found
            return {"pattern": "no_divergence", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error detecting divergence: {str(e)}")
            return {"pattern": "error", "confidence": 0, "direction": "NEUTRAL"}
    
    def detect_trading_strategy(self, df: pd.DataFrame, strategy: str) -> Dict[str, Any]:
        """
        Detect if conditions match a specific trading strategy
        
        Args:
            df: DataFrame with price and indicator data
            strategy: Name of the trading strategy to check
            
        Returns:
            Dictionary with strategy detection information
        """
        if df is None or len(df) < 50:
            return {"pattern": "insufficient_data", "confidence": 0, "direction": "NEUTRAL"}
        
        try:
            # Each strategy detection function
            strategy_functions = {
                "pullback_in_uptrend": self._detect_pullback_in_uptrend,
                "oversold_in_uptrend": self._detect_oversold_in_uptrend,
                "momentum_in_uptrend": self._detect_momentum_in_uptrend,
                "ma_crossover": self._detect_ma_crossover_strategy,
                "sideways_channel": self._detect_sideways_channel_strategy,
                "breakout": self._detect_breakout_strategy
            }
            
            # Check if strategy exists in the dictionary
            if strategy.lower() in strategy_functions:
                return strategy_functions[strategy.lower()](df)
            else:
                return {"pattern": "unknown_strategy", "confidence": 0, "direction": "NEUTRAL"}
                
        except Exception as e:
            print(f"Error detecting trading strategy {strategy}: {str(e)}")
            return {"pattern": "error", "confidence": 0, "direction": "NEUTRAL"}
    
    def _detect_pullback_in_uptrend(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                               support_levels: List[Tuple[float, int]] = None,
                               resistance_levels: List[Tuple[float, int]] = None) -> Dict[str, Any]:
        """
        Detect pullback in uptrend conditions using a four-stage validation process.
        """
        try:
            # --- 1. Strong Uptrend Pre-conditions ---
            if not all(col in df.columns for col in ['ema20', 'ema50', 'ema200', 'atr']):
                return {"detected": False, "reason": "Missing indicators"}

            ema20, ema50, ema200 = df['ema20'].iloc[-1], df['ema50'].iloc[-1], df['ema200'].iloc[-1]
            price = df['close'].iloc[-1]

            # Condition 1: EMA Alignment
            ema_alignment_ok = ema20 > ema50 > ema200

            # Condition 2: Positive EMA50 Slope
            if len(df) > 10:
                ema50_slope = df['ema50'].iloc[-1] - df['ema50'].iloc[-10]
                slope_ok = ema50_slope > 0
            else:
                slope_ok = False

            # Condition 3: Price is above the core trend EMA
            price_above_ema50_ok = price > ema50

            if not (ema_alignment_ok and slope_ok and price_above_ema50_ok):
                return {"detected": False, "reason": "Uptrend conditions not met"}

            # --- 2. Dynamic Pullback Measurement ---
            recent_df = df.tail(60)
            high_peaks_indices, _ = find_peaks(recent_df['high'], distance=5)
            low_troughs_indices, _ = find_peaks(-recent_df['low'], distance=5)

            if len(high_peaks_indices) < 1 or len(low_troughs_indices) < 1:
                return {"detected": False, "reason": "Not enough swing points found"}

            # Find the last swing high and the preceding swing low
            last_swing_high_idx = high_peaks_indices[-1]
            last_swing_high_price = recent_df['high'].iloc[last_swing_high_idx]

            preceding_lows = low_troughs_indices[low_troughs_indices < last_swing_high_idx]
            if len(preceding_lows) < 1:
                return {"detected": False, "reason": "Could not find preceding swing low"}

            last_swing_low_idx = preceding_lows[-1]
            last_swing_low_price = recent_df['low'].iloc[last_swing_low_idx]

            # Validate the impulse swing
            impulse_height = last_swing_high_price - last_swing_low_price
            if impulse_height <= 0:
                return {"detected": False, "reason": "Invalid impulse swing"}

            # Check Fibonacci retracement
            fib_382 = last_swing_high_price - (impulse_height * 0.382)
            fib_618 = last_swing_high_price - (impulse_height * 0.618)

            if not (fib_618 <= price <= fib_382):
                return {"detected": False, "reason": "Price not in Fibonacci retracement zone"}

            # Check proximity to dynamic support (EMA20 or EMA50)
            atr = df['atr'].iloc[-1]
            distance_to_ema20 = abs(price - ema20)
            distance_to_ema50 = abs(price - ema50)

            if not (distance_to_ema20 < (0.75 * atr) or distance_to_ema50 < (0.75 * atr)):
                return {"detected": False, "reason": "Price not near dynamic EMA support"}

            # --- 3. Reversal Trigger Confirmation ---
            # Trigger 1: Minor Structure Break
            minor_structure_high = df['high'].iloc[-6:-1].max() # High of last 5 candles (excluding current)
            structure_break_trigger = price > minor_structure_high

            # Trigger 2: Bullish Engulfing Candle
            prev_candle = df.iloc[-2]
            current_candle = df.iloc[-1]
            engulfing_trigger = (current_candle['close'] > current_candle['open'] and
                                 prev_candle['close'] < prev_candle['open'] and
                                 current_candle['close'] > prev_candle['open'] and
                                 current_candle['open'] < prev_candle['close'])

            # Trigger 3: RSI turning up
            rsi_trigger = False
            if 'rsi14' in df.columns and len(df) > 2:
                if df['rsi14'].iloc[-1] > df['rsi14'].iloc[-2] and df['rsi14'].iloc[-1] < 60:
                    rsi_trigger = True

            # Trigger 4: MACD Histogram turning up
            macd_trigger = False
            if 'macd_hist' in df.columns and len(df) > 2:
                if df['macd_hist'].iloc[-1] > df['macd_hist'].iloc[-2]:
                    macd_trigger = True

            if not (structure_break_trigger or engulfing_trigger or rsi_trigger or macd_trigger):
                return {"detected": False, "reason": "No reversal trigger found"}

            # Volume Confirmation (optional but boosts score)
            volume_confirmation = False
            if 'volume' in df.columns:
                avg_volume = df['volume'].iloc[-11:-1].mean()
                if avg_volume > 0 and df['volume'].iloc[-1] > avg_volume * 1.5:
                    volume_confirmation = True

            # --- 4. Final Signal Generation ---
            base_confidence = 70 # Start with a strong base since many filters passed

            if engulfing_trigger and volume_confirmation:
                base_confidence += 15
            elif structure_break_trigger and volume_confirmation:
                base_confidence += 10
            elif rsi_trigger and macd_trigger:
                base_confidence += 5

            # Check for confluence with provided static support levels
            if support_levels:
                for level, strength in support_levels:
                    if abs(price - level) / price < 0.01 and strength > 4: # Within 1% of a strong support
                        base_confidence += 10
                        break

            # Get trend information
            trends = {}
            if all_timeframes:
                for tf in ["1h", "4h", "1d", "1w"]:
                    if tf in all_timeframes:
                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

            final_confidence = self.calculate_signal_confidence(
                "Pullback in Uptrend", "LONG", df, support_levels, resistance_levels, trends,
                base_confidence_override=base_confidence
            )

            return {
                "detected": True,
                "pattern": "Pullback in Uptrend",
                "confidence": final_confidence,
                "direction": "LONG"
            }
            
        except Exception as e:
            print(f"Error in _detect_pullback_in_uptrend: {e}")
            traceback.print_exc()
            return {"detected": False, "reason": f"Exception: {e}"}
    
    def _detect_oversold_in_uptrend(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                                  support_levels: List[Tuple[float, int]] = None, 
                                  resistance_levels: List[Tuple[float, int]] = None) -> Dict[str, Any]:
        """
        Enhanced 'Oversold in Uptrend' detector for crypto markets.
        1.  Validates a strong, multi-factor uptrend using EMA alignment, slope, and ADX strength.
        2.  Uses a dynamic, ATR-adjusted RSI threshold for detecting the oversold condition.
        3.  Requires multi-factor confirmation: RSI turning up, optional Stochastic crossover, and volume expansion.
        4.  Checks for confluence with high-strength, pre-calculated support levels.
        5.  Returns a detection result with a calculated base confidence score.
        """
        try:
            # --- 0. Initial Data and Indicator Checks ---
            required = ['ema20', 'ema50', 'ema200', 'rsi14', 'stoch_k', 'stoch_d', 'adx', 'atr', 'close', 'volume']
            if not all(col in df.columns and not df[col].isnull().all() for col in required):
                return {"detected": False, "reason": "Missing required indicators"}

            # --- 1. Strong Uptrend Preconditions ---
            current_price = df['close'].iloc[-1]
            ema20 = df['ema20'].iloc[-1]
            ema50 = df['ema50'].iloc[-1]
            ema200 = df['ema200'].iloc[-1]
            adx = df['adx'].iloc[-1]
            
            # Condition 1: EMA alignment and price location
            is_ema_aligned = (ema20 > ema50) and (ema50 > ema200)
            is_price_above_trend = current_price > ema50

            # Condition 2: Positive EMA slope (momentum)
            ema50_slope = (df['ema50'].iloc[-1] - df['ema50'].iloc[-10]) / df['ema50'].iloc[-10] if len(df) > 10 else 0
            is_slope_positive = ema50_slope > 0.005 # Require at least 0.5% growth over 10 periods

            # Condition 3: ADX confirms trend strength
            is_trend_strong = adx > 20

            if not (is_ema_aligned and is_price_above_trend and is_slope_positive and is_trend_strong):
                return {"detected": False, "reason": "Uptrend preconditions not met"}

            # --- 2. Dynamic Oversold Condition ---
            rsi = df['rsi14'].iloc[-1]
            atr_percent = (df['atr'].iloc[-1] / current_price) * 100

            # Dynamic threshold: Higher vol allows for higher RSI floor
            oversold_threshold = 40 + min(5, atr_percent * 0.5) # Base 40, cap at 45
            
            is_oversold = rsi < oversold_threshold

            if not is_oversold:
                return {"detected": False, "reason": "Not in dynamic oversold zone"}

            # --- 3. Multi-Factor Confirmation ---
            # Confirmation 3a: RSI is turning up
            rsi_prev = df['rsi14'].iloc[-2]
            is_rsi_turning_up = rsi > rsi_prev

            # Confirmation 3b: Stochastic crossover in oversold territory
            stoch_k = df['stoch_k'].iloc[-1]
            stoch_d = df['stoch_d'].iloc[-1]
            stoch_k_prev = df['stoch_k'].iloc[-2]
            stoch_d_prev = df['stoch_d'].iloc[-2]
            stoch_confirm = (stoch_k_prev <= stoch_d_prev and stoch_k > stoch_d and stoch_k < 30)

            # Confirmation 3c: Volume supports the move
            current_volume = df['volume'].iloc[-1]
            avg_volume = df['volume'].iloc[-11:-1].mean()
            volume_confirm = current_volume > avg_volume * 1.3 # 30% volume increase

            # A valid signal requires RSI turning up AND (Stochastic confirmation OR Volume confirmation)
            if not (is_rsi_turning_up and (stoch_confirm or volume_confirm)):
                return {"detected": False, "reason": "Lack of confirmation (RSI turn, Stoch cross, or Volume)"}

            # --- 4. Confluence Scoring ---
            base_confidence = 65  # Start with a solid base for passing all checks

            # Bonus for strong support confluence
            if support_levels:
                nearest_support_price, nearest_support_strength = support_levels[0]
                # Check if price is within 1 ATR of a strong support level
                if abs(current_price - nearest_support_price) < df['atr'].iloc[-1] and nearest_support_strength >= 4:
                    base_confidence += 10
            
            # Bonus for very deep RSI reading
            if rsi < 30:
                base_confidence += 5

            # Bonus if BOTH stochastic and volume confirmed
            if stoch_confirm and volume_confirm:
                base_confidence += 5
                
            return {
                "detected": True,
                "direction": "LONG",
                "base_confidence": min(85, base_confidence) # Cap base confidence at 85
            }

        except Exception as e:
            print(f"Error in _detect_oversold_in_uptrend: {e}")
            traceback.print_exc()
            return {"detected": False, "reason": f"Exception: {e}"}
          
    def _detect_momentum_in_uptrend(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                              support_levels: List[float] = None, resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect momentum in uptrend conditions based on MACD
        
        Args:
            df: DataFrame with price and indicator data
            all_timeframes: Dictionary with data for different timeframes
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            
        Returns:
            Dictionary with strategy detection information
        """
        try:
            # Check for required indicators
            required = ['ema50', 'ema200', 'macd', 'macd_signal', 'macd_hist', 'close']
            if not all(col in df.columns for col in required):
                return {"pattern": "missing_indicators", "confidence": 0, "direction": "NEUTRAL"}
            
            # Check for uptrend conditions
            ema50 = df['ema50'].iloc[-1]
            ema200 = df['ema200'].iloc[-1]
            
            # Strong uptrend if EMA50 is above EMA200
            uptrend = ema50 > ema200 * 1.01
            
            if not uptrend:
                return {"pattern": "no_uptrend", "confidence": 0, "direction": "NEUTRAL"}
            
            # Check price position relative to EMAs
            current_price = df['close'].iloc[-1]
            price_above_ema50 = current_price > ema50
            
            # Check MACD for momentum signals
            macd = df['macd'].iloc[-1]
            macd_signal = df['macd_signal'].iloc[-1]
            macd_hist = df['macd_hist'].iloc[-1]
            
            # Previous values for detecting crossovers
            macd_prev = df['macd'].iloc[-2]
            macd_signal_prev = df['macd_signal'].iloc[-2]
            macd_hist_prev = df['macd_hist'].iloc[-2]
            
            # Bullish MACD crossover (MACD crossing above signal line)
            macd_crossover = macd_prev <= macd_signal_prev and macd > macd_signal
            
            # MACD histogram turning positive (early momentum signal)
            hist_turning_positive = macd_hist_prev < 0 and macd_hist > 0
            
            # MACD histogram increasing (momentum building)
            hist_increasing = macd_hist > macd_hist_prev
            
            # Determine signal type
            if macd_crossover:
                signal_type = "MACD Crossover in Uptrend"
            elif hist_turning_positive:
                signal_type = "MACD Histogram Turn Positive in Uptrend"
            elif hist_increasing:
                signal_type = "MACD Momentum in Uptrend"
            else:
                return {"pattern": "no_momentum_signal", "confidence": 0, "direction": "NEUTRAL"}
            
            # Pattern detected, use new confidence calculation
            pattern_name = signal_type
            direction = "LONG"
            
            # Get trend information
            trends = {}
            if all_timeframes:
                for tf in ["1h", "4h", "1d", "1w"]:
                    if tf in all_timeframes:
                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
            
            # If support_levels not provided, calculate them
            if support_levels is None or resistance_levels is None:
                s_levels, r_levels = self.identify_support_resistance(all_timeframes)
                if support_levels is None:
                    support_levels = s_levels
                if resistance_levels is None:
                    resistance_levels = r_levels
            
            # Use new confidence method
            confidence = self.calculate_signal_confidence(
                pattern_name, direction, df, support_levels, resistance_levels, trends
            )
            
            # Adjust confidence based on price position
            if price_above_ema50:
                confidence += 5
            
            return {
                "detected": True,
                "pattern": pattern_name,
                "confidence": min(100, confidence),
                "direction": direction
            }
            
        except Exception as e:
            print(f"Error in momentum detection: {str(e)}")
            return {"pattern": "error", "confidence": 0, "direction": "NEUTRAL"}
    
    def _detect_ma_crossover_strategy(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Detect moving average crossover strategy conditions
        
        Args:
            df: DataFrame with price and indicator data
            
        Returns:
            Dictionary with strategy detection information
        """
        try:
            # Check for various MA crossover combinations
            ma_pairs = [
                ('sma5', 'sma10'),
                ('sma10', 'sma20'),
                ('sma20', 'sma50'),
                ('sma50', 'sma200'),
                ('ema9', 'ema12'),
                ('ema12', 'ema26'),
                ('ema26', 'ema50'),
                ('ema50', 'ema200')
            ]
            
            # Find available MA pairs
            available_pairs = []
            for fast, slow in ma_pairs:
                if fast in df.columns and slow in df.columns:
                    available_pairs.append((fast, slow))
            
            if not available_pairs:
                return {"pattern": "missing_indicators", "confidence": 0, "direction": "NEUTRAL"}
            
            # Check each MA pair for crossover
            crossovers = []
            
            for fast, slow in available_pairs:
                fast_ma_current = df[fast].iloc[-1]
                fast_ma_prev = df[fast].iloc[-2]
                slow_ma_current = df[slow].iloc[-1]
                slow_ma_prev = df[slow].iloc[-2]
                
                # Bullish crossover
                if fast_ma_prev <= slow_ma_prev and fast_ma_current > slow_ma_current:
                    crossover_type = f"{fast}_{slow}_bullish_cross"
                    
                    # Calculate confidence based on MA pair and crossover strength
                    # Golden cross (50/200) gets highest confidence
                    if (fast == 'sma50' and slow == 'sma200') or (fast == 'ema50' and slow == 'ema200'):
                        base_confidence = 80
                    # Medium timeframe crossovers
                    elif (fast == 'sma20' and slow == 'sma50') or (fast == 'ema26' and slow == 'ema50'):
                        base_confidence = 70
                    # Shorter timeframe crossovers
                    else:
                        base_confidence = 60
                    
                    # Crossover strength
                    strength = (fast_ma_current - slow_ma_current) / slow_ma_current * 100
                    
                    crossovers.append({
                        "pattern": crossover_type,
                        "confidence": min(95, base_confidence + min(15, strength * 5)),
                        "direction": "LONG"
                    })
                
                # Bearish crossover
                elif fast_ma_prev >= slow_ma_prev and fast_ma_current < slow_ma_current:
                    crossover_type = f"{fast}_{slow}_bearish_cross"
                    
                    # Calculate confidence based on MA pair and crossover strength
                    # Death cross (50/200) gets highest confidence
                    if (fast == 'sma50' and slow == 'sma200') or (fast == 'ema50' and slow == 'ema200'):
                        base_confidence = 80
                    # Medium timeframe crossovers
                    elif (fast == 'sma20' and slow == 'sma50') or (fast == 'ema26' and slow == 'ema50'):
                        base_confidence = 70
                    # Shorter timeframe crossovers
                    else:
                        base_confidence = 60
                    
                    # Crossover strength
                    strength = (slow_ma_current - fast_ma_current) / slow_ma_current * 100
                    
                    crossovers.append({
                        "pattern": crossover_type,
                        "confidence": min(95, base_confidence + min(15, strength * 5)),
                        "direction": "SHORT"
                    })
            
            # Return highest confidence crossover
            if crossovers:
                crossovers.sort(key=lambda x: x["confidence"], reverse=True)
                return crossovers[0]
            else:
                return {"pattern": "no_ma_crossover", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in MA crossover detection: {str(e)}")
            return {"pattern": "error", "confidence": 0, "direction": "NEUTRAL"}
    
    def _detect_sideways_channel_strategy(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                                    support_levels: List[Tuple[float, int]] = None,
                                    resistance_levels: List[Tuple[float, int]] = None) -> Dict[str, Any]:
        """
        Detects sideways channel (range-bound) trading conditions using the robust fitting engine.
        """
        try:
            recent_df = df.tail(60).copy()
            channel_data = self._fit_channel(recent_df)

            # 1. Validate that a valid, relatively flat channel was found
            if not channel_data:
                return {"detected": False}

            # Normalize slope to check for sideways action
            normalized_slope = channel_data['slope'] / recent_df['close'].mean()
            if abs(normalized_slope) > 0.001:  # Allow a very slight drift (0.1% per bar)
                return {"detected": False}

            # 2. Check for bounce signals at boundaries
            current_price = recent_df['close'].iloc[-1]
            last_idx = len(recent_df) - 1
            upper_bound = channel_data['slope'] * last_idx + channel_data['intercept'] + channel_data['upper_offset']
            lower_bound = channel_data['slope'] * last_idx + channel_data['intercept'] + channel_data['lower_offset']

            direction = "NEUTRAL"
            pattern_name = "Sideways Channel"

            # Check for bounce from lower boundary (LONG signal)
            if abs(current_price - lower_bound) < (channel_data['avg_atr'] * 0.5):
                direction = "LONG"
                pattern_name = "Sideways Channel Support"

            # Check for rejection from upper boundary (SHORT signal)
            elif abs(current_price - upper_bound) < (channel_data['avg_atr'] * 0.5):
                direction = "SHORT"
                pattern_name = "Sideways Channel Resistance"

            if direction != "NEUTRAL":
                trends = {tf: self.analyze_trend(all_timeframes[tf], tf) for tf in all_timeframes}
                confidence = self.calculate_signal_confidence(pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=channel_data['r2'] * 100)
                return {"detected": True, "pattern": pattern_name, "confidence": confidence, "direction": direction, "details": channel_data}

            return {"detected": False}
        except Exception as e:
            print(f"Error in _detect_sideways_channel_strategy: {e}")
            return {"detected": False}

    def _detect_channel_up(self, df: pd.DataFrame, support_levels: List[Tuple[float, int]],
                     resistance_levels: List[Tuple[float, int]], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        try:
            recent_df = df.tail(60).copy()
            channel_data = self._fit_channel(recent_df)

            if not channel_data or channel_data['slope'] <= 0:
                return {"detected": False}

            current_price = recent_df['close'].iloc[-1]
            last_idx = len(recent_df) - 1
            upper_bound = channel_data['slope'] * last_idx + channel_data['intercept'] + channel_data['upper_offset']
            lower_bound = channel_data['slope'] * last_idx + channel_data['intercept'] + channel_data['lower_offset']

            direction = "NEUTRAL"
            pattern_suffix = ""

            if abs(current_price - lower_bound) < (channel_data['avg_atr'] * 0.5):
                direction = "LONG"
                pattern_suffix = "Bounce"

            elif current_price > upper_bound + (channel_data['avg_atr'] * 0.25):
                direction = "LONG"
                pattern_suffix = "Breakout"

            if direction == "LONG":
                pattern_name = f"Channel Up {pattern_suffix}"
                quality_score = channel_data['r2'] * 100

                # --- CONFLUENCE CHECK ---
                confluence_score_boost = 0
                touch_tolerance = channel_data['avg_atr'] * 0.5
                lower_band = channel_data['slope'] * np.arange(len(recent_df)) + channel_data['intercept'] + channel_data['lower_offset']
                lower_touch_prices = recent_df['low'][np.abs(recent_df['low'] - lower_band) < touch_tolerance]

                for price in lower_touch_prices:
                    if self._is_level_significant(price, support_levels, min_strength=3):
                        confluence_score_boost += 5
                quality_score += confluence_score_boost

                trends = {tf: self.analyze_trend(all_timeframes[tf], tf) for tf in all_timeframes}
                confidence = self.calculate_signal_confidence(pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score)
                return {"detected": True, "pattern": pattern_name, "confidence": confidence, "direction": direction, "details": channel_data}

            return {"detected": False}
        except Exception as e:
            print(f"Error in _detect_channel_up: {e}")
            return {"detected": False}

    def _detect_channel_down(self, df: pd.DataFrame, support_levels: List[Tuple[float, int]],
                       resistance_levels: List[Tuple[float, int]], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        try:
            recent_df = df.tail(60).copy()
            channel_data = self._fit_channel(recent_df)

            if not channel_data or channel_data['slope'] >= 0:
                return {"detected": False}

            current_price = recent_df['close'].iloc[-1]
            last_idx = len(recent_df) - 1
            upper_bound = channel_data['slope'] * last_idx + channel_data['intercept'] + channel_data['upper_offset']
            lower_bound = channel_data['slope'] * last_idx + channel_data['intercept'] + channel_data['lower_offset']

            direction = "NEUTRAL"
            pattern_suffix = ""

            if abs(current_price - upper_bound) < (channel_data['avg_atr'] * 0.5):
                direction = "SHORT"
                pattern_suffix = "Bounce"

            elif current_price < lower_bound - (channel_data['avg_atr'] * 0.25):
                direction = "SHORT"
                pattern_suffix = "Breakdown"

            if direction == "SHORT":
                pattern_name = f"Channel Down {pattern_suffix}"
                quality_score = channel_data['r2'] * 100

                # --- CONFLUENCE CHECK ---
                confluence_score_boost = 0
                touch_tolerance = channel_data['avg_atr'] * 0.5
                upper_band = channel_data['slope'] * np.arange(len(recent_df)) + channel_data['intercept'] + channel_data['upper_offset']
                upper_touch_prices = recent_df['high'][np.abs(recent_df['high'] - upper_band) < touch_tolerance]

                for price in upper_touch_prices:
                    if self._is_level_significant(price, resistance_levels, min_strength=3):
                        confluence_score_boost += 5
                quality_score += confluence_score_boost

                trends = {tf: self.analyze_trend(all_timeframes[tf], tf) for tf in all_timeframes}
                confidence = self.calculate_signal_confidence(pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score)
                return {"detected": True, "pattern": pattern_name, "confidence": confidence, "direction": direction, "details": channel_data}

            return {"detected": False}
        except Exception as e:
            print(f"Error in _detect_channel_down: {e}")
            return {"detected": False}

    def _detect_breakout_strategy(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Detect breakout trading strategy conditions
        
        Args:
            df: DataFrame with price and indicator data
            
        Returns:
            Dictionary with strategy detection information
        """
        try:
            # Need at least 50 bars of data
            if len(df) < 50:
                return {"pattern": "insufficient_data", "confidence": 0, "direction": "NEUTRAL"}
            
            # Calculate support/resistance levels
            look_back = min(100, len(df))
            historical_df = df.tail(look_back).copy()
            
            # Find significant price levels
            highs = historical_df['high'].values
            lows = historical_df['low'].values
            current_price = historical_df['close'].iloc[-1]
            
            # Use recent price action to define resistance
            recent_resistance = np.percentile(highs[:-1], 90)  # 90th percentile of highs
            
            # Check for resistance breakout
            if current_price > recent_resistance * 1.01:  # Price broke above resistance by 1%
                # Check for confirmation signals
                
                # Volume confirmation
                if 'volume' in historical_df.columns:
                    recent_volume = historical_df['volume'].iloc[-1]
                    avg_volume = historical_df['volume'].iloc[-10:-1].mean()
                    volume_surge = recent_volume > avg_volume * 1.5
                else:
                    volume_surge = False
                
                # Check if this is a new high
                new_high_periods = 20
                if len(historical_df) > new_high_periods:
                    prev_high = historical_df['high'].iloc[-(new_high_periods+1):-1].max()
                    is_new_high = current_price > prev_high
                else:
                    is_new_high = False
                
                # Calculate breakout strength
                breakout_percent = (current_price / recent_resistance - 1) * 100
                
                # Base confidence on breakout strength and confirmation factors
                base_confidence = min(80, 60 + breakout_percent * 5)
                if volume_surge:
                    base_confidence += 10
                if is_new_high:
                    base_confidence += 5
                
                return {
                    "pattern": "resistance_breakout",
                    "confidence": min(95, base_confidence),
                    "direction": "LONG"
                }
            
            # Use recent price action to define support
            recent_support = np.percentile(lows[:-1], 10)  # 10th percentile of lows
            
            # Check for support breakdown
            if current_price < recent_support * 0.99:  # Price broke below support by 1%
                # Check for confirmation signals
                
                # Volume confirmation
                if 'volume' in historical_df.columns:
                    recent_volume = historical_df['volume'].iloc[-1]
                    avg_volume = historical_df['volume'].iloc[-10:-1].mean()
                    volume_surge = recent_volume > avg_volume * 1.5
                else:
                    volume_surge = False
                
                # Check if this is a new low
                new_low_periods = 20
                if len(historical_df) > new_low_periods:
                    prev_low = historical_df['low'].iloc[-(new_low_periods+1):-1].min()
                    is_new_low = current_price < prev_low
                else:
                    is_new_low = False
                
                # Calculate breakdown strength
                breakdown_percent = (1 - current_price / recent_support) * 100
                
                # Base confidence on breakdown strength and confirmation factors
                base_confidence = min(80, 60 + breakdown_percent * 5)
                if volume_surge:
                    base_confidence += 10
                if is_new_low:
                    base_confidence += 5
                
                return {
                    "pattern": "support_breakdown",
                    "confidence": min(95, base_confidence),
                    "direction": "SHORT"
                }
            
            # No breakout detected
            return {"pattern": "no_breakout", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in breakout detection: {str(e)}")
            return {"pattern": "error", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_trend_following_pullback(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                                        support_levels: List[float] = None, resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect a trend-following with pullback entry strategy.
        """
        try:
            if not all(col in df.columns for col in ['ema20', 'ema50', 'rsi14']):
                return {"detected": False}

            # Check for a confirmed uptrend
            uptrend = df['ema20'].iloc[-1] > df['ema50'].iloc[-1]
            if not uptrend:
                return {"detected": False}

            # Check for a pullback to the 20-period EMA
            current_price = df['close'].iloc[-1]
            ema20 = df['ema20'].iloc[-1]
            is_pullback = abs(current_price - ema20) / ema20 < 0.02 # Within 2% of EMA20

            # RSI should be above 40 (not oversold, but in a pullback)
            rsi = df['rsi14'].iloc[-1]
            rsi_ok = rsi > 40

            if is_pullback and rsi_ok:
                pattern_name = "Trend-Following with Pullback Entry"
                direction = "LONG"
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

                confidence = self.calculate_signal_confidence(
                    pattern_name, direction, df, support_levels, resistance_levels, trends
                )
                return {
                    "detected": True,
                    "pattern": pattern_name,
                    "confidence": confidence,
                    "direction": direction
                }

            return {"detected": False}

        except Exception as e:
            print(f"Error in _detect_trend_following_pullback: {e}")
            return {"detected": False}

    def _detect_volume_breakout(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                                support_levels: List[float] = None, resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect a volume breakout strategy.
        """
        try:
            if not all(col in df.columns for col in ['rvol']):
                return {"detected": False}

            # Check for high relative volume
            is_high_volume = df['rvol'].iloc[-1] > 2.0
            if not is_high_volume:
                return {"detected": False}

            # Check for a breakout of a recent high or low
            current_price = df['close'].iloc[-1]
            recent_high = df['high'].iloc[-10:-1].max()
            recent_low = df['low'].iloc[-10:-1].min()

            if current_price > recent_high:
                direction = "LONG"
                pattern_name = "Volume Breakout (Bullish)"
            elif current_price < recent_low:
                direction = "SHORT"
                pattern_name = "Volume Breakout (Bearish)"
            else:
                return {"detected": False}

            trends = {}
            if all_timeframes:
                for tf in ["1h", "4h", "1d", "1w"]:
                    if tf in all_timeframes:
                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

            confidence = self.calculate_signal_confidence(
                pattern_name, direction, df, support_levels, resistance_levels, trends
            )
            return {
                "detected": True,
                "pattern": pattern_name,
                "confidence": confidence,
                "direction": direction
            }

        except Exception as e:
            print(f"Error in _detect_volume_breakout: {e}")
            return {"detected": False}
        
    def _detect_chart_pattern(self, df: pd.DataFrame, pattern_name: str,
                        all_timeframes: Dict[str, pd.DataFrame] = None,
                        support_levels: List[float] = None,
                        resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect if a specific chart pattern is present
        
        Args:
            df: DataFrame with price and indicator data
            pattern_name: Name of the pattern to detect
            all_timeframes: Dictionary with data for different timeframes
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            
        Returns:
            Dictionary with detection results
        """
        result = {
            "detected": False,
            "pattern": pattern_name,
            "confidence": 0,
            "direction": "NEUTRAL"
        }
        
        try:
            # If support/resistance levels not provided, calculate them
            if support_levels is None or resistance_levels is None:
                s_levels, r_levels = self.identify_support_resistance(all_timeframes)
                if support_levels is None:
                    support_levels = s_levels
                if resistance_levels is None:
                    resistance_levels = r_levels
            

            
            # Pattern-specific detection
            pattern_matchers = {
                "Horizontal Resistance": self._detect_horizontal_resistance,
                "Horizontal Support": self._detect_horizontal_support,
                "Channel Up": self._detect_channel_up,
                "Channel Down": self._detect_channel_down,
                "Inverse Head And Shoulders": self._detect_inverse_head_and_shoulders_pattern,
                "Head And Shoulders": self._detect_head_and_shoulders_pattern,
                "Double Top": self._detect_double_top,
                "Double Bottom": self._detect_double_bottom,
                "Triple Top": self._detect_triple_top,
                "Triple Bottom": self._detect_triple_bottom,
                "Rectangle": self._detect_rectangle,
                "Flag": self._detect_flag_pattern,
                "Pennant": self._detect_pennant,
                "Point Retracement": self._detect_point_retracement,
                "Point Extension": self._detect_point_extension,
                "ABCD": self._detect_abcd_pattern,
                "Gartley": self._detect_gartley,
                "BUTTERFLY": self._detect_butterfly,
                "Drive": self._detect_drive,
                "Consecutive Candles": self._detect_consecutive_candles
            }
            
            # Check each pattern matcher for exact matches
            for pattern_key, detector in pattern_matchers.items():
                if pattern_name == pattern_key:
                    return detector(df, all_timeframes, support_levels, resistance_levels)
            
            # Now check for partial matches (in case of misspellings or alternative names)
            for pattern_key, detector in pattern_matchers.items():
                pattern_key_lower = pattern_key.lower()
                pattern_name_lower = pattern_name.lower()
                
                if pattern_key_lower in pattern_name_lower or pattern_name_lower in pattern_key_lower:
                    return detector(df, all_timeframes, support_levels, resistance_levels)
            
            # If pattern not recognized, make best guess based on name
            direction = "LONG"
            if any(term in pattern_name.lower() for term in ["down", "top", "bear", "head and shoulders", "resistance"]):
                direction = "SHORT"
            elif any(term in pattern_name.lower() for term in ["up", "bottom", "bull", "inverse", "support"]):
                direction = "LONG"
            
            # Get trend information
            trends = {}
            for tf in ["1h", "4h", "1d", "1w"]:
                if tf in all_timeframes:
                    trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
            
            # Calculate confidence score
            confidence = self.calculate_signal_confidence(
                pattern_name, direction, df, support_levels, resistance_levels, trends
            )
            
            # Return generic result
            return {
                "detected": True,
                "pattern": pattern_name,
                "confidence": confidence * 0.8,  # Reduce confidence for generic detection
                "direction": direction
            }
            
        except Exception as e:
            print(f"Error detecting chart pattern {pattern_name}: {str(e)}")
            traceback.print_exc()
            return result
    
    def _detect_macd_histogram_inflection(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Detect MACD histogram inflections (early momentum shift indicator)
        
        Args:
            df: DataFrame with price and indicator data
            
        Returns:
            Dictionary with inflection information
        """
        result = {
            "detected": False,
            "pattern": "MACD Histogram Inflection",
            "confidence": 0,
            "direction": "NEUTRAL",
            "strength": 0
        }
        
        try:
            # Need MACD histogram
            if 'macd_hist' not in df.columns or len(df) < 4:
                return result
            
            # Get recent histogram values
            hist = df['macd_hist'].iloc[-1]
            hist_prev = df['macd_hist'].iloc[-2]
            hist_prev2 = df['macd_hist'].iloc[-3]
            hist_prev3 = df['macd_hist'].iloc[-4]
            
            # Check for bullish inflection (histogram turning up)
            if hist_prev < hist and hist_prev <= hist_prev2:
                # Calculate inflection strength
                hist_change = (hist - hist_prev) / abs(hist_prev) if hist_prev != 0 else 0
                
                # Stronger inflection if histogram was previously declining for multiple bars
                multi_bar_decline = hist_prev2 > hist_prev and hist_prev3 > hist_prev2
                
                # Extra confirmation if histogram is negative but improving (often most reliable signal)
                negative_but_improving = hist_prev < 0 and hist < 0
                
                # Calculate confidence
                confidence = 60  # Base confidence
                
                if hist_change > 0.2:  # Strong change
                    confidence += 15
                elif hist_change > 0.1:  # Moderate change
                    confidence += 10
                elif hist_change > 0.05:  # Small change
                    confidence += 5
                    
                if multi_bar_decline:
                    confidence += 10  # More reliable if there was a sustained decline
                    
                if negative_but_improving:
                    confidence += 10  # Most reliable when turning up from negative
                
                return {
                    "detected": True,
                    "pattern": "Bullish MACD Histogram Inflection",
                    "confidence": min(95, confidence),
                    "direction": "LONG",
                    "strength": hist_change * 100  # Convert to percentage
                }
                
            # Check for bearish inflection (histogram turning down)
            elif hist_prev > hist and hist_prev >= hist_prev2:
                # Calculate inflection strength
                hist_change = (hist_prev - hist) / abs(hist_prev) if hist_prev != 0 else 0
                
                # Stronger inflection if histogram was previously rising for multiple bars
                multi_bar_rise = hist_prev2 < hist_prev and hist_prev3 < hist_prev2
                
                # Extra confirmation if histogram is positive but worsening (often most reliable signal)
                positive_but_worsening = hist_prev > 0 and hist > 0
                
                # Calculate confidence
                confidence = 60  # Base confidence
                
                if hist_change > 0.2:  # Strong change
                    confidence += 15
                elif hist_change > 0.1:  # Moderate change
                    confidence += 10
                elif hist_change > 0.05:  # Small change
                    confidence += 5
                    
                if multi_bar_rise:
                    confidence += 10  # More reliable if there was a sustained rise
                    
                if positive_but_worsening:
                    confidence += 10  # Most reliable when turning down from positive
                
                return {
                    "detected": True,
                    "pattern": "Bearish MACD Histogram Inflection",
                    "confidence": min(95, confidence),
                    "direction": "SHORT",
                    "strength": hist_change * 100  # Convert to percentage
                }
            
            return result
            
        except Exception as e:
            print(f"Error detecting MACD histogram inflection: {str(e)}")
            return result
    
    def _detect_macd_signal_crossover(self, df: pd.DataFrame, 
                                all_timeframes: Dict[str, pd.DataFrame] = None,
                                support_levels: List[float] = None, 
                                resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect MACD signal line crossover
        
        Args:
            df: DataFrame with price and indicator data
            all_timeframes: Dictionary with data for different timeframes
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Check for required indicators
            if not all(col in df.columns for col in ['macd', 'macd_signal']):
                return {"detected": False, "pattern": "MACD Signal Line Crossover", 
                        "confidence": 0, "direction": "NEUTRAL"}
            
            # Get recent data
            if len(df) < 3:
                return {"detected": False, "pattern": "MACD Signal Line Crossover", 
                        "confidence": 0, "direction": "NEUTRAL"}
                
            # Check for bullish crossover (MACD crosses above signal line)
            macd = df['macd'].iloc[-1]
            macd_signal = df['macd_signal'].iloc[-1]
            macd_prev = df['macd'].iloc[-2]
            macd_signal_prev = df['macd_signal'].iloc[-2]
            
            # Define pattern based on crossover direction
            pattern_name = "Fresh Momentum (MACD Signal Line) Crossover"
            direction = "NEUTRAL"
            
            # Bullish crossover (MACD crosses above signal)
            if macd_prev <= macd_signal_prev and macd > macd_signal:
                direction = "LONG"
                
            # Bearish crossover (MACD crosses below signal)
            elif macd_prev >= macd_signal_prev and macd < macd_signal:
                direction = "SHORT"
            else:
                # No crossover
                return {"detected": False, "pattern": pattern_name, 
                        "confidence": 0, "direction": "NEUTRAL"}
            
            # Get trend information from all timeframes
            trends = {}
            for tf in ["1h", "4h", "1d", "1w"]:
                if tf in all_timeframes:
                    trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
            
            # Calculate confidence using our enhanced method
            confidence = self.calculate_signal_confidence(
                pattern_name, direction, df, support_levels, resistance_levels, trends
            )
            
            # Return result with detection details
            return {
                "detected": True,
                "pattern": pattern_name,
                "confidence": confidence,
                "direction": direction
            }
                
        except Exception as e:
            print(f"Error detecting MACD signal crossover: {str(e)}")
            return {"detected": False, "pattern": "MACD Signal Line Crossover", 
                    "confidence": 0, "direction": "NEUTRAL"}

    def _detect_horizontal_resistance(self, df: pd.DataFrame, support_levels: List[float], 
                                resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect horizontal resistance pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        current_price = df['close'].iloc[-1]
        
        # Check if price is near resistance
        for level, strength in resistance_levels:
            # Price should be close to resistance (within 2%)
            if 0 <= (level - current_price) / current_price <= 0.02:
                # Calculate confidence based on how many times resistance was tested
                test_count = 0
                
                # Look back up to 30 days to count resistance tests
                for i in range(1, min(30, len(df))):
                    high = df['high'].iloc[-i]
                    if 0 <= (level - high) / high <= 0.01:
                        test_count += 1
                
                # Pattern detected, use new confidence calculation
                pattern_name = "Horizontal Resistance"
                direction = "SHORT"
                
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use new confidence method
                quality_score = 50 + (strength * 5) # Base score + strength bonus
                confidence = self.calculate_signal_confidence(
                    pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                )

                # Multi-timeframe validation
                if all_timeframes and '1d' in all_timeframes:
                    daily_trend = self.analyze_trend(all_timeframes['1d'], '1d')
                    if daily_trend.get("trend") == "BEARISH":
                        confidence = min(95, confidence + 10)
                
                return {
                    "detected": True,
                    "pattern": pattern_name,
                    "confidence": confidence,
                    "direction": direction
                }
        
        return {
            "detected": False,
            "pattern": "Horizontal Resistance",
            "confidence": 0,
            "direction": "NEUTRAL"
        }
    
    def _detect_horizontal_support(self, df: pd.DataFrame, support_levels: List[float], 
                             resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect horizontal support pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        current_price = df['close'].iloc[-1]
        
        # Check if price is near support
        for level, strength in support_levels:
            # Price should be close to support (within 2%)
            if 0 <= (current_price - level) / current_price <= 0.02:
                # Calculate confidence based on how many times support was tested
                test_count = 0
                
                # Look back up to 30 days to count support tests
                for i in range(1, min(30, len(df))):
                    low = df['low'].iloc[-i]
                    if 0 <= (low - level) / low <= 0.01:
                        test_count += 1
                
                # Pattern detected, use new confidence calculation
                pattern_name = "Horizontal Support"
                direction = "LONG"
                
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use new confidence method
                quality_score = 50 + (strength * 5) # Base score + strength bonus
                confidence = self.calculate_signal_confidence(
                    pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                )

                # Multi-timeframe validation
                if all_timeframes and '1d' in all_timeframes:
                    daily_trend = self.analyze_trend(all_timeframes['1d'], '1d')
                    if daily_trend.get("trend") == "BULLISH":
                        confidence = min(95, confidence + 10)
                
                return {
                    "detected": True,
                    "pattern": pattern_name,
                    "confidence": confidence,
                    "direction": direction
                }
        
        return {
            "detected": False,
            "pattern": "Horizontal Support",
            "confidence": 0,
            "direction": "NEUTRAL"
        }
    
    def _detect_double_top(self, df: pd.DataFrame, support_levels: List[Tuple[float, int]],
                            resistance_levels: List[Tuple[float, int]], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        ENHANCED Double Top detection with professional-grade validation
        Key improvements:
        - Stricter geometric requirements
        - Volume divergence analysis
        - RSI momentum divergence
        - Support/resistance confluence
        - Minimum pattern duration requirements
        """
        try:
            recent_df = df.tail(60).copy()
            if len(recent_df) < 35:
                return {"detected": False}

            # ENHANCED: More sophisticated peak detection
            significant_peaks = []
            for i in range(8, len(recent_df) - 8):  # Increased window for more significance
                if all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 9)) and \
                   all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 9)):
                    peak_prominence = recent_df['high'].iloc[i] - max(
                        recent_df['low'].iloc[i-8:i].min(),
                        recent_df['low'].iloc[i:i+8].min()
                    )
                    if peak_prominence > recent_df['close'].iloc[i] * 0.02:
                        significant_peaks.append((i, recent_df['high'].iloc[i], recent_df['volume'].iloc[i]))

            if len(significant_peaks) < 2:
                return {"detected": False}

            for i in range(len(significant_peaks) - 1):
                for j in range(i + 1, len(significant_peaks)):
                    peak1_idx, peak1_val, peak1_vol = significant_peaks[i]
                    peak2_idx, peak2_val, peak2_vol = significant_peaks[j]

                    if not abs(peak2_val - peak1_val) / peak1_val <= 0.015: continue
                    if not 15 <= (peak2_idx - peak1_idx) <= 40: continue

                    trough_section = recent_df.iloc[peak1_idx:peak2_idx]
                    if trough_section.empty: continue
                    trough_val = trough_section['low'].min()
                    valley_depth = (min(peak1_val, peak2_val) / trough_val) - 1
                    if not valley_depth >= 0.04: continue

                    if not recent_df['close'].iloc[-1] < trough_val * 0.995: continue

                    is_at_resistance = self._is_level_significant(peak1_val, resistance_levels, min_strength=4)
                    if not is_at_resistance:
                        # Changed to a continue to check other potential peaks
                        continue 

                    if not recent_df['close'].iloc[-1] < trough_val:
                        # This check is slightly redundant with the one above, but we keep it for clarity
                        continue
                    
                    quality_score = 55
                    quality_score += 15
                    if self._is_level_significant(trough_val, support_levels, min_strength=3):
                        quality_score += 10

                    if peak2_vol < peak1_vol * 0.8:
                        quality_score += 15

                    # Momentum Divergence Checks...
                    if 'rsi14' in df.columns:
                        try:
                            main_df_idx1 = df.index.get_loc(recent_df.index[peak1_idx])
                            main_df_idx2 = df.index.get_loc(recent_df.index[peak2_idx])
                            rsi1 = df.loc[df.index[main_df_idx1], 'rsi14']
                            rsi2 = df.loc[df.index[main_df_idx2], 'rsi14']
                            if peak2_val >= peak1_val and rsi2 < rsi1:
                                quality_score += 20
                        except KeyError:
                            pass # Index might not be found if df is sliced
                    
                    if 'macd' in df.columns:
                        try:
                            main_df_idx1 = df.index.get_loc(recent_df.index[peak1_idx])
                            main_df_idx2 = df.index.get_loc(recent_df.index[peak2_idx])
                            macd1 = df.loc[df.index[main_df_idx1], 'macd']
                            macd2 = df.loc[df.index[main_df_idx2], 'macd']
                            if peak2_val >= peak1_val and macd2 < macd1:
                                quality_score += 20
                        except KeyError:
                            pass

                    bars_since_completion = len(recent_df) - peak2_idx
                    if 3 <= bars_since_completion <= 10:
                        quality_score += 8
                    elif bars_since_completion > 20:
                        quality_score -= 10

                    if quality_score < 75:
                        continue # Continue to check other peak combinations

                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    final_confidence = self.calculate_enhanced_confidence(
                        "Double Top", "SHORT", df, support_levels, resistance_levels, 
                        trends, base_confidence_override=quality_score
                    )
                    
                    return {
                        "detected": True, "pattern": "Double Top", "confidence": final_confidence, "direction": "SHORT",
                        "details": {
                            "quality_score": quality_score, "peak1_price": peak1_val, "peak2_price": peak2_val,
                            "neckline_price": trough_val, "breakdown_price": recent_df['close'].iloc[-1],
                            "volume_divergence": peak2_vol < peak1_vol, "valley_depth_pct": valley_depth * 100
                        }
                    }
            # If loops complete without returning a valid pattern
            return {"detected": False}
            
        except Exception as e:
            print(f"Error in enhanced double top detection: {e}")
            return {"detected": False}
    
    def _detect_double_bottom(self, df: pd.DataFrame, support_levels: List[Tuple[float, int]],
                                resistance_levels: List[Tuple[float, int]], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        ENHANCED Double Bottom detection with professional-grade validation
        Key improvements:
        - Stricter geometric requirements
        - Volume divergence analysis
        - RSI momentum divergence
        - Support/resistance confluence
        - Minimum pattern duration requirements
        """
        try:
            recent_df = df.tail(60).copy()
            if len(recent_df) < 35:
                return {"detected": False}

            # ENHANCED: More sophisticated trough detection
            significant_lows = []
            for i in range(8, len(recent_df) - 8):  # Increased window for more significance
                # Require stronger trough confirmation
                if all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 9)) and \
                   all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 9)):
                    # PROFESSIONAL: Add minimum trough prominence requirement
                    trough_prominence = max(
                        recent_df['high'].iloc[i-8:i].max(),
                        recent_df['high'].iloc[i:i+8].max()
                    ) - recent_df['low'].iloc[i]
                    if trough_prominence > recent_df['close'].iloc[i] * 0.02:  # At least 2% prominence
                        significant_lows.append((i, recent_df['low'].iloc[i], recent_df['volume'].iloc[i]))

            if len(significant_lows) < 2:
                return {"detected": False}

            for i in range(len(significant_lows) - 1):
                for j in range(i + 1, len(significant_lows)):
                    low1_idx, low1_val, low1_vol = significant_lows[i]
                    low2_idx, low2_val, low2_vol = significant_lows[j]

                    # ENHANCED: Stricter price symmetry (crypto needs tighter tolerance)
                    if not abs(low2_val - low1_val) / low1_val <= 0.015: continue  # Reduced from 0.02 to 0.015
                    
                    # ENHANCED: Optimal time separation (15-40 periods for crypto)
                    if not 15 <= (low2_idx - low1_idx) <= 40: continue

                    # PROFESSIONAL: Peak height requirement (crypto volatility consideration)
                    peak_section = recent_df.iloc[low1_idx:low2_idx]
                    if peak_section.empty: continue
                    peak_val = peak_section['high'].max()
                    peak_depth = (peak_val / max(low1_val, low2_val)) - 1
                    if not peak_depth >= 0.04: continue  # At least 4% peak height

                    # CRITICAL: Confirmed breakout above neckline
                    if not recent_df['close'].iloc[-1] > peak_val * 1.005: continue  # 0.5% buffer

                    # --- Start New Professional Validation ---

                    # 1. S/R Confluence Check (MANDATORY FILTER)
                    is_at_support = self._is_level_significant(low1_val, support_levels, min_strength=4)
                    if not is_at_support:
                        return {"detected": False, "reason": "Not formed at significant support"}

                    # 2. Confirmed Breakout Check (MANDATORY FILTER)
                    if not recent_df['close'].iloc[-1] > peak_val:
                        return {"detected": False, "reason": "Neckline not yet broken"}
                    
                    # --- Start Quality Scoring ---
                    quality_score = 55  # Base score for a confirmed, located pattern

                    # Add S/R Confluence Score
                    quality_score += 15 # From the check above
                    if self._is_level_significant(peak_val, resistance_levels, min_strength=3):
                        quality_score += 10 # Neckline confluence bonus

                    # Add Volume Divergence Score
                    if low2_vol < low1_vol * 0.8:
                        quality_score += 15

                    # Add Momentum Divergence Score
                    if 'rsi14' in df.columns and len(df) > recent_df.index[0]:
                        try:
                            # Map recent_df indices to main df indices
                            main_df_idx1 = recent_df.index[low1_idx]
                            main_df_idx2 = recent_df.index[low2_idx]
                            rsi1 = df.loc[main_df_idx1, 'rsi14']
                            rsi2 = df.loc[main_df_idx2, 'rsi14']
                            
                            # Bullish RSI divergence (price lower, RSI higher)
                            if low2_val <= low1_val and rsi2 > rsi1:
                                quality_score += 20
                        except:
                            pass

                    # Add MACD Divergence Score
                    if 'macd' in df.columns and len(df) > recent_df.index[0]:
                        try:
                            # Map recent_df indices to main df indices
                            main_df_idx1 = recent_df.index[low1_idx]
                            main_df_idx2 = recent_df.index[low2_idx]
                            macd1 = df.loc[main_df_idx1, 'macd']
                            macd2 = df.loc[main_df_idx2, 'macd']
                            
                            # Bullish MACD divergence (price lower, MACD higher)
                            if low2_val <= low1_val and macd2 > macd1:
                                quality_score += 20
                        except:
                            pass

                    # ENHANCED: Pattern maturity bonus (not too fresh, not too old)
                    bars_since_completion = len(recent_df) - low2_idx
                    if 3 <= bars_since_completion <= 10:  # Optimal confirmation period
                        quality_score += 8
                    elif bars_since_completion > 20:  # Too old
                        quality_score -= 10

                    # CRITICAL: Confidence Filter
                    if quality_score < 75: # Set a high bar for quality
                        return {"detected": False, "reason": f"Low quality score: {quality_score}"}

                    # Calculate multi-timeframe trends for final confidence
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Pass the calculated score to the main confidence function
                    final_confidence = self.calculate_enhanced_confidence(
                        "Double Bottom", "LONG", df, support_levels, resistance_levels, 
                        trends, base_confidence_override=quality_score
                    )
                    
                    return {
                        "detected": True,
                        "pattern": "Double Bottom",
                        "confidence": final_confidence,
                        "direction": "LONG",
                        "details": {
                            "quality_score": quality_score,
                            "low1_price": low1_val,
                            "low2_price": low2_val,
                            "neckline_price": peak_val,
                            "breakout_price": recent_df['close'].iloc[-1],
                            "volume_divergence": low2_vol < low1_vol,
                            "peak_height_pct": peak_depth * 100
                        }
                    }

            return {"detected": False}
            
        except Exception as e:
            print(f"Error in enhanced double bottom detection: {e}")
            return {"detected": False}

    def _detect_head_and_shoulders_pattern(self, df: pd.DataFrame, support_levels: List[Tuple[float, int]],
                                         resistance_levels: List[Tuple[float, int]], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        ENHANCED Head and Shoulders detection with professional-grade validation
        Key improvements:
        - Stricter geometric requirements
        - Volume divergence analysis
        - RSI momentum divergence
        - Support/resistance confluence
        - Minimum pattern duration requirements
        """
        try:
            if len(df) < 60: return {"detected": False}
            recent_df = df.tail(100).copy()
            if len(recent_df) < 50: return {"detected": False}

            # 1. Generate a clean, alternating list of swing points
            all_pivots = []
            for i in range(5, len(recent_df) - 5):
                is_peak = all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 6)) and \
                          all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 6))
                if is_peak:
                    all_pivots.append({'idx': i, 'price': recent_df['high'].iloc[i], 'type': 'H', 'volume': recent_df['volume'].iloc[i]})

                is_trough = all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 6)) and \
                            all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 6))
                if is_trough:
                    all_pivots.append({'idx': i, 'price': recent_df['low'].iloc[i], 'type': 'L', 'volume': recent_df['volume'].iloc[i]})

            if not all_pivots: return {"detected": False}

            # Filter for alternating swings
            swings = [all_pivots[0]]
            for i in range(1, len(all_pivots)):
                if all_pivots[i]['type'] != swings[-1]['type']:
                    swings.append(all_pivots[i])

            if len(swings) < 5: return {"detected": False}

            # 2. Scan the alternating swings for the H&S pattern (H-L-HH-L-H)
            for i in range(len(swings) - 4):
                ls, l1, head, l2, rs = swings[i], swings[i+1], swings[i+2], swings[i+3], swings[i+4]
                
                # Check for correct H-L-H-L-H sequence (potential H&S pattern)
                if not (ls['type'] == 'H' and l1['type'] == 'L' and head['type'] == 'H' and l2['type'] == 'L' and rs['type'] == 'H'):
                    continue

                # --- Geometric Validation ---
                if not (head['price'] > ls['price'] and head['price'] > rs['price']): continue
                if not abs(ls['price'] - rs['price']) / max(ls['price'], rs['price']) <= 0.05: continue
                
                slope = (l2['price'] - l1['price']) / (l2['idx'] - l1['idx']) if l2['idx'] != l1['idx'] else 0
                neckline_at_breakdown = l2['price'] + slope * (len(recent_df) - 1 - l2['idx'])

                # CRITICAL: Confirmed breakdown below neckline
                if not recent_df['close'].iloc[-1] < neckline_at_breakdown: continue

                # --- Start New Professional Validation ---

                # 1. S/R Confluence Check (MANDATORY FILTER)
                # Validate that the head is forming at significant resistance
                is_at_resistance = self._is_level_significant(head['price'], resistance_levels, min_strength=4)
                if not is_at_resistance:
                    return {"detected": False, "reason": "Head not formed at significant resistance"}

                # 2. Confirmed Breakout Check (MANDATORY FILTER)
                if not recent_df['close'].iloc[-1] < neckline_at_breakdown:
                    return {"detected": False, "reason": "Neckline not yet broken"}
                
                # --- Start Quality Scoring ---
                quality_score = 55  # Base score for a confirmed, located pattern

                # Add S/R Confluence Score
                quality_score += 15 # From the check above
                if self._is_level_significant(l1['price'], support_levels, min_strength=3) or \
                   self._is_level_significant(l2['price'], support_levels, min_strength=3):
                    quality_score += 10 # Neckline confluence bonus

                # Add Volume Profile Validation Score
                # The volume accompanying the final peak (the right shoulder) should be noticeably lower
                if rs['volume'] < head['volume'] * 0.8:
                    quality_score += 15

                # Add Momentum Divergence Score
                if 'rsi14' in df.columns and len(df) > recent_df.index[0]:
                    try:
                        # Map recent_df indices to main df indices
                        main_df_idx_head = recent_df.index[head['idx']]
                        main_df_idx_rs = recent_df.index[rs['idx']]
                        rsi_head = df.loc[main_df_idx_head, 'rsi14']
                        rsi_rs = df.loc[main_df_idx_rs, 'rsi14']
                        
                        # Bearish RSI divergence (price higher or equal, RSI lower)
                        if rs['price'] <= head['price'] and rsi_rs < rsi_head:
                            quality_score += 20
                    except:
                        pass

                # Add MACD Divergence Score
                if 'macd' in df.columns and len(df) > recent_df.index[0]:
                    try:
                        # Map recent_df indices to main df indices
                        main_df_idx_head = recent_df.index[head['idx']]
                        main_df_idx_rs = recent_df.index[rs['idx']]
                        macd_head = df.loc[main_df_idx_head, 'macd']
                        macd_rs = df.loc[main_df_idx_rs, 'macd']
                        
                        # Bearish MACD divergence (price higher or equal, MACD lower)
                        if rs['price'] <= head['price'] and macd_rs < macd_head:
                            quality_score += 20
                    except:
                        pass

                # CRITICAL: Confidence Filter
                if quality_score < 75: # Set a high bar for quality
                    return {"detected": False, "reason": f"Low quality score: {quality_score}"}

                # Calculate multi-timeframe trends for final confidence
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Pass the calculated score to the main confidence function
                final_confidence = self.calculate_enhanced_confidence(
                    "Head and Shoulders", "SHORT", df, support_levels, resistance_levels, 
                    trends, base_confidence_override=quality_score
                )
                
                return {
                    "detected": True,
                    "pattern": "Head and Shoulders",
                    "confidence": final_confidence,
                    "direction": "SHORT",
                    "details": {
                        "quality_score": quality_score,
                        "head_price": head['price'],
                        "left_shoulder_price": ls['price'],
                        "right_shoulder_price": rs['price'],
                        "neckline_price": neckline_at_breakdown,
                        "breakdown_price": recent_df['close'].iloc[-1],
                        "volume_divergence": rs['volume'] < head['volume'],
                        "neckline_slope": slope
                    }
                }

            return {"detected": False}
        except Exception as e:
            print(f"Error in enhanced head and shoulders detection: {e}")
            return {"detected": False}

    def _detect_inverse_head_and_shoulders_pattern(self, df: pd.DataFrame, support_levels: List[Tuple[float, int]],
                                                     resistance_levels: List[Tuple[float, int]], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        ENHANCED Inverse Head and Shoulders detection with professional-grade validation
        Key improvements:
        - Stricter geometric requirements
        - Volume divergence analysis
        - RSI momentum divergence
        - Support/resistance confluence
        - Minimum pattern duration requirements
        """
        try:
            if len(df) < 60: return {"detected": False}
            recent_df = df.tail(100).copy()
            if len(recent_df) < 50: return {"detected": False}

            # 1. Generate a clean, alternating list of swing points
            all_pivots = []
            for i in range(5, len(recent_df) - 5):
                is_peak = all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 6)) and \
                          all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 6))
                if is_peak:
                    all_pivots.append({'idx': i, 'price': recent_df['high'].iloc[i], 'type': 'H', 'volume': recent_df['volume'].iloc[i]})

                is_trough = all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 6)) and \
                            all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 6))
                if is_trough:
                    all_pivots.append({'idx': i, 'price': recent_df['low'].iloc[i], 'type': 'L', 'volume': recent_df['volume'].iloc[i]})

            if not all_pivots: return {"detected": False}

            # Filter for alternating swings
            swings = [all_pivots[0]]
            for i in range(1, len(all_pivots)):
                if all_pivots[i]['type'] != swings[-1]['type']:
                    swings.append(all_pivots[i])

            if len(swings) < 5: return {"detected": False}

            # 2. Scan the alternating swings for the Inverse H&S pattern (L-H-LL-H-L)
            for i in range(len(swings) - 4):
                ls, p1, head, p2, rs = swings[i], swings[i+1], swings[i+2], swings[i+3], swings[i+4]

                # Check for correct L-H-L-H-L sequence (potential Inverse H&S)
                if not (ls['type'] == 'L' and p1['type'] == 'H' and head['type'] == 'L' and p2['type'] == 'H' and rs['type'] == 'L'):
                    continue
                
                # --- Geometric Validation ---
                if not (head['price'] < ls['price'] and head['price'] < rs['price']): continue
                if not abs(ls['price'] - rs['price']) / max(ls['price'], rs['price']) <= 0.05: continue

                slope = (p2['price'] - p1['price']) / (p2['idx'] - p1['idx']) if p2['idx'] != p1['idx'] else 0
                neckline_at_breakout = p2['price'] + slope * (len(recent_df) - 1 - p2['idx'])

                # CRITICAL: Confirmed breakout above neckline
                if not recent_df['close'].iloc[-1] > neckline_at_breakout: continue

                # --- Start New Professional Validation ---

                # 1. S/R Confluence Check (MANDATORY FILTER)
                # Validate that the head is forming at significant support
                is_at_support = self._is_level_significant(head['price'], support_levels, min_strength=4)
                if not is_at_support:
                    return {"detected": False, "reason": "Head not formed at significant support"}

                # 2. Confirmed Breakout Check (MANDATORY FILTER)
                if not recent_df['close'].iloc[-1] > neckline_at_breakout:
                    return {"detected": False, "reason": "Neckline not yet broken"}
                
                # --- Start Quality Scoring ---
                quality_score = 55  # Base score for a confirmed, located pattern

                # Add S/R Confluence Score
                quality_score += 15 # From the check above
                if self._is_level_significant(p1['price'], resistance_levels, min_strength=3) or \
                   self._is_level_significant(p2['price'], resistance_levels, min_strength=3):
                    quality_score += 10 # Neckline confluence bonus

                # Add Volume Profile Validation Score
                # The breakout candle that closes above the neckline must be on significantly higher volume
                if recent_df['volume'].iloc[-1] > recent_df['volume'].mean() * 1.75:
                    quality_score += 15

                # Add Momentum Divergence Score
                if 'rsi14' in df.columns and len(df) > recent_df.index[0]:
                    try:
                        # Map recent_df indices to main df indices
                        main_df_idx_head = recent_df.index[head['idx']]
                        main_df_idx_rs = recent_df.index[rs['idx']]
                        rsi_head = df.loc[main_df_idx_head, 'rsi14']
                        rsi_rs = df.loc[main_df_idx_rs, 'rsi14']
                        
                        # Bullish RSI divergence (price lower or equal, RSI higher)
                        if rs['price'] >= head['price'] and rsi_rs > rsi_head:
                            quality_score += 20
                    except:
                        pass

                # Add MACD Divergence Score
                if 'macd' in df.columns and len(df) > recent_df.index[0]:
                    try:
                        # Map recent_df indices to main df indices
                        main_df_idx_head = recent_df.index[head['idx']]
                        main_df_idx_rs = recent_df.index[rs['idx']]
                        macd_head = df.loc[main_df_idx_head, 'macd']
                        macd_rs = df.loc[main_df_idx_rs, 'macd']
                        
                        # Bullish MACD divergence (price lower or equal, MACD higher)
                        if rs['price'] >= head['price'] and macd_rs > macd_head:
                            quality_score += 20
                    except:
                        pass

                # CRITICAL: Confidence Filter
                if quality_score < 75: # Set a high bar for quality
                    return {"detected": False, "reason": f"Low quality score: {quality_score}"}

                # Calculate multi-timeframe trends for final confidence
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Pass the calculated score to the main confidence function
                final_confidence = self.calculate_enhanced_confidence(
                    "Inverse Head and Shoulders", "LONG", df, support_levels, resistance_levels, 
                    trends, base_confidence_override=quality_score
                )
                
                return {
                    "detected": True,
                    "pattern": "Inverse Head and Shoulders",
                    "confidence": final_confidence,
                    "direction": "LONG",
                    "details": {
                        "quality_score": quality_score,
                        "head_price": head['price'],
                        "left_shoulder_price": ls['price'],
                        "right_shoulder_price": rs['price'],
                        "neckline_price": neckline_at_breakout,
                        "breakout_price": recent_df['close'].iloc[-1],
                        "breakout_volume": recent_df['volume'].iloc[-1] > recent_df['volume'].mean() * 1.75,
                        "neckline_slope": slope
                    }
                }

            return {"detected": False}
        except Exception as e:
            print(f"Error in enhanced inverse head and shoulders detection: {e}")
            return {"detected": False}

    def _detect_cup_and_handle(self, df: pd.DataFrame, support_levels: List[float],
                                resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect Cup and Handle (bullish) and Inverse Cup and Handle (bearish) patterns.
        """
        try:
            # Need at least 50 bars for a reliable cup and handle pattern
            if len(df) < 50:
                return {"detected": False, "pattern": "Cup and Handle", "confidence": 0, "direction": "NEUTRAL"}

            # Use recent data for analysis
            recent_df = df.tail(100).copy()

            # Find local peaks and troughs
            lows = recent_df['low'].values
            highs = recent_df['high'].values

            # Bullish Cup and Handle
            # 1. Find a U-shaped "cup"
            # We can approximate this by looking for a significant low point (cup bottom)
            # and two high points on either side at similar levels (cup rim)
            cup_bottom_idx = np.argmin(lows[-50:-10]) + (len(lows) - 50)
            cup_bottom_val = lows[cup_bottom_idx]

            # Find left and right rim of the cup
            left_rim_idx = np.argmax(highs[:cup_bottom_idx])
            right_rim_idx = np.argmax(highs[cup_bottom_idx:]) + cup_bottom_idx

            left_rim_val = highs[left_rim_idx]
            right_rim_val = highs[right_rim_idx]

            # Rims should be at similar levels (within 5%)
            if abs(left_rim_val - right_rim_val) / right_rim_val < 0.05:
                # Cup depth should be significant (15-50% of rim height)
                cup_depth = (left_rim_val - cup_bottom_val) / left_rim_val
                if 0.15 <= cup_depth <= 0.50:
                    # 2. Find the "handle" - a smaller pullback after the cup
                    handle_df = recent_df.iloc[right_rim_idx:]
                    if len(handle_df) > 5:
                        handle_low = handle_df['low'].min()
                        handle_high = handle_df['high'].max()

                        # Handle should retrace less than 50% of the cup's height
                        handle_retracement = (right_rim_val - handle_low) / (right_rim_val - cup_bottom_val)
                        if handle_retracement < 0.5:
                            # 3. Confirmation: breakout above the rim
                            current_price = recent_df['close'].iloc[-1]
                            if current_price > right_rim_val:
                                # --- CONFLUENCE CHECK & SCORING ---
                                quality_score = 60
                                if self._is_level_significant(right_rim_val, resistance_levels, min_strength=4):
                                    quality_score += 20

                                # Pattern detected, use new confidence calculation
                                pattern_name = "Cup and Handle"
                                direction = "LONG"

                                # Get trend information
                                trends = {}
                                if all_timeframes:
                                    for tf in ["1h", "4h", "1d", "1w"]:
                                        if tf in all_timeframes:
                                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

                                # Use new confidence method
                                confidence = self.calculate_signal_confidence(
                                    pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                                )

                                return {
                                    "detected": True,
                                    "pattern": pattern_name,
                                    "confidence": confidence,
                                    "direction": direction
                                }

            # Inverse Cup and Handle (Bearish)
            # 1. Find an inverted U-shaped "cup"
            cup_top_idx = np.argmax(highs[-50:-10]) + (len(highs) - 50)
            cup_top_val = highs[cup_top_idx]

            # Find left and right rim of the inverse cup
            left_rim_idx = np.argmin(lows[:cup_top_idx])
            right_rim_idx = np.argmin(lows[cup_top_idx:]) + cup_top_idx

            left_rim_val = lows[left_rim_idx]
            right_rim_val = lows[right_rim_idx]

            # Rims should be at similar levels (within 5%)
            if abs(left_rim_val - right_rim_val) / right_rim_val < 0.05:
                # Cup depth should be significant
                cup_depth = (cup_top_val - left_rim_val) / cup_top_val
                if 0.15 <= cup_depth <= 0.50:
                    # 2. Find the "handle" - a smaller bounce
                    handle_df = recent_df.iloc[right_rim_idx:]
                    if len(handle_df) > 5:
                        handle_low = handle_df['low'].min()
                        handle_high = handle_df['high'].max()

                        handle_retracement = (handle_high - right_rim_val) / (cup_top_val - right_rim_val)
                        if handle_retracement < 0.5:
                            # 3. Confirmation: breakdown below the rim
                            current_price = recent_df['close'].iloc[-1]
                            if current_price < right_rim_val:
                                # --- CONFLUENCE CHECK & SCORING ---
                                quality_score = 60
                                if self._is_level_significant(right_rim_val, support_levels, min_strength=4):
                                    quality_score += 20

                                # Pattern detected, use new confidence calculation
                                pattern_name = "Inverse Cup and Handle"
                                direction = "SHORT"

                                # Get trend information
                                trends = {}
                                if all_timeframes:
                                    for tf in ["1h", "4h", "1d", "1w"]:
                                        if tf in all_timeframes:
                                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

                                # Use new confidence method
                                confidence = self.calculate_signal_confidence(
                                    pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                                )

                                return {
                                    "detected": True,
                                    "pattern": pattern_name,
                                    "confidence": confidence,
                                    "direction": direction
                                }

            return {"detected": False, "pattern": "Cup and Handle", "confidence": 0, "direction": "NEUTRAL"}

        except Exception as e:
            print(f"Error in Cup and Handle detection: {e}")
            return {"detected": False, "pattern": "Cup and Handle", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_broadening_formation(self, df: pd.DataFrame, support_levels: List[float],
                                     resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect Broadening Formation patterns.
        """
        try:
            if len(df) < 50:
                return {"detected": False, "pattern": "Broadening Formation", "confidence": 0, "direction": "NEUTRAL"}

            recent_df = df.tail(50).copy()
            highs = recent_df['high'].values
            lows = recent_df['low'].values

            # Find local peaks and troughs
            peaks = []
            troughs = []
            for i in range(5, len(recent_df) - 5):
                if all(highs[i] >= highs[i-j] for j in range(1, 6)) and all(highs[i] >= highs[i+j] for j in range(1, 6)):
                    peaks.append((i, highs[i]))
                if all(lows[i] <= lows[i-j] for j in range(1, 6)) and all(lows[i] <= lows[i+j] for j in range(1, 6)):
                    troughs.append((i, lows[i]))

            if len(peaks) >= 2 and len(troughs) >= 2:
                # Check for higher highs and lower lows
                if peaks[-1][1] > peaks[-2][1] and troughs[-1][1] < troughs[-2][1]:
                    pattern_name = "Broadening Formation"
                    # Direction is uncertain, but often a reversal pattern
                    # We can use the prior trend to guess the direction
                    prior_trend_df = df.iloc[:-50]
                    if not prior_trend_df.empty:
                        prior_trend = self.analyze_trend(prior_trend_df, "1d")
                        direction = "SHORT" if prior_trend.get("trend") == "BULLISH" else "LONG"
                    else:
                        direction = "NEUTRAL"

                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

                    # --- CONFLUENCE CHECK & SCORING ---
                    quality_score = 60
                    confluence_score_boost = 0
                    if self._is_level_significant(peaks[-1][1], resistance_levels, min_strength=3) or \
                       self._is_level_significant(peaks[-2][1], resistance_levels, min_strength=3):
                        confluence_score_boost += 10

                    if self._is_level_significant(troughs[-1][1], support_levels, min_strength=3) or \
                       self._is_level_significant(troughs[-2][1], support_levels, min_strength=3):
                        confluence_score_boost += 10
                    quality_score += confluence_score_boost

                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                    )

                    return {
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    }

            return {"detected": False, "pattern": "Broadening Formation", "confidence": 0, "direction": "NEUTRAL"}

        except Exception as e:
            print(f"Error in Broadening Formation detection: {e}")
            return {"detected": False, "pattern": "Broadening Formation", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_island_reversal(self, df: pd.DataFrame, support_levels: List[float],
                                resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect Island Reversal patterns.
        """
        try:
            if len(df) < 20:
                return {"detected": False, "pattern": "Island Reversal", "confidence": 0, "direction": "NEUTRAL"}

            recent_df = df.tail(20).copy()

            # Look for a gap, a period of trading, and then another gap in the opposite direction
            # For a bullish island reversal (bottom):
            # 1. A gap down
            # 2. A period of trading below the gap
            # 3. A gap up

            # Simplified check: Look for a recent low that is "isolated" by gaps
            low_point_idx = np.argmin(recent_df['low'].values)
            if 1 < low_point_idx < len(recent_df) - 1:
                pre_gap_high = recent_df['high'].iloc[low_point_idx - 1]
                island_low = recent_df['low'].iloc[low_point_idx]
                post_gap_low = recent_df['low'].iloc[low_point_idx + 1]

                if island_low < pre_gap_high and post_gap_low > pre_gap_high:
                    pattern_name = "Bullish Island Reversal"
                    direction = "LONG"
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    return {
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    }

            # For a bearish island reversal (top):
            high_point_idx = np.argmax(recent_df['high'].values)
            if 1 < high_point_idx < len(recent_df) - 1:
                pre_gap_low = recent_df['low'].iloc[high_point_idx - 1]
                island_high = recent_df['high'].iloc[high_point_idx]
                post_gap_high = recent_df['high'].iloc[high_point_idx + 1]

                if island_high > pre_gap_low and post_gap_high < pre_gap_low:
                    pattern_name = "Bearish Island Reversal"
                    direction = "SHORT"
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    return {
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    }

            return {"detected": False, "pattern": "Island Reversal", "confidence": 0, "direction": "NEUTRAL"}

        except Exception as e:
            print(f"Error in Island Reversal detection: {e}")
            return {"detected": False, "pattern": "Island Reversal", "confidence": 0, "direction": "NEUTRAL"}
    
    def _detect_flag_pattern(self, df: pd.DataFrame, support_levels: List[Tuple[float, int]],
                            resistance_levels: List[Tuple[float, int]],
                            all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        ENHANCED Flag pattern detection with professional-grade validation
        Key improvements:
        - Prior trend context validation
        - Volume contraction analysis
        - Breakout volume spike confirmation
        """
        try:
            if len(df) < 50: return {"detected": False}

            # --- 1. DYNAMIC FLAGPOLE IDENTIFICATION ---
            flagpole = None
            # Scan the last 80 bars for a potential flagpole, ending at different points
            for end_idx in range(len(df) - 20, len(df) - 10):
                # A flagpole is a strong, impulsive move over 5-15 candles
                for pole_len in range(5, 16):
                    start_idx = end_idx - pole_len
                    if start_idx < 0: continue

                    pole_df = df.iloc[start_idx:end_idx]
                    start_price = pole_df['close'].iloc[0]
                    end_price = pole_df['close'].iloc[-1]

                    move_pct = (end_price / start_price - 1) * 100
                    avg_vol = pole_df['volume'].mean()
                    avg_vol_20d_prior = df['volume'].iloc[max(0, start_idx-20):start_idx].mean()

                    # Criteria for a valid flagpole
                    if abs(move_pct) > 15 and avg_vol > avg_vol_20d_prior * 1.5:
                        flagpole = {
                            'start_idx': start_idx,
                            'end_idx': end_idx,
                            'direction': 'bull' if move_pct > 0 else 'bear',
                            'move_pct': move_pct
                        }
                        break
                if flagpole: break

            if not flagpole: return {"detected": False}

            # --- Start New Professional Validation ---

            # 1. Prior Trend Context (MANDATORY FILTER)
            # Ensure a strong, impulsive trend exists before the pattern starts forming

            # FIX: Calculate indicators on the main DataFrame first if they don't exist.
            if 'ema20' not in df.columns or 'ema50' not in df.columns:
                df = self.calculate_technical_indicators(df)
                # If calculation still fails, we can't proceed.
                if 'ema20' not in df.columns:
                    return {"detected": False, "reason": "Missing required indicators on main DF"}

            # Now, safely check for the trend context
            pattern_start_idx = flagpole['start_idx']
            if pattern_start_idx >= 20:
                # Get trend context before pattern formation
                trend_context_df = df.iloc[pattern_start_idx-20:pattern_start_idx]
                
                # Check for NaN values, which can occur at the start of the series
                if trend_context_df['ema20'].isnull().any() or trend_context_df['ema50'].isnull().any():
                    return {"detected": False, "reason": "NaN indicators in prior trend context"}

                ema20_before = trend_context_df['ema20'].iloc[-1]
                ema50_before = trend_context_df['ema50'].iloc[-1]

                # For bull flag, 20-period EMA should be clearly above the 50-period EMA
                # For bear flag, 20-period EMA should be clearly below the 50-period EMA
                if flagpole['direction'] == 'bull' and not (ema20_before > ema50_before):
                    return {"detected": False, "reason": "No strong bullish trend before bull flag"}
                elif flagpole['direction'] == 'bear' and not (ema20_before < ema50_before):
                    return {"detected": False, "reason": "No strong bearish trend before bear flag"}

            # --- 2. FLAG CHANNEL VALIDATION ---
            flag_df = df.iloc[flagpole['end_idx']:].copy()
            if len(flag_df) < 10: return {"detected": False}

            # Use pivot points to define the flag's consolidation channel
            from scipy.signal import find_peaks
            high_peaks, _ = find_peaks(flag_df['high'], distance=3)
            low_peaks, _ = find_peaks(-flag_df['low'], distance=3)

            if len(high_peaks) < 2 or len(low_peaks) < 2: return {"detected": False}

            # Fit linear regression to the pivots
            upper_x = high_peaks.reshape(-1, 1)
            upper_y = flag_df['high'].iloc[high_peaks]
            lr_upper = LinearRegression().fit(upper_x, upper_y)

            lower_x = low_peaks.reshape(-1, 1)
            lower_y = flag_df['low'].iloc[low_peaks]
            lr_lower = LinearRegression().fit(lower_x, lower_y)

            # --- Geometric and Contextual Checks ---
            # a. Slopes must be counter-trend to the flagpole
            is_counter_trend = (flagpole['direction'] == 'bull' and lr_upper.coef_[0] < 0 and lr_lower.coef_[0] < 0) or \
                               (flagpole['direction'] == 'bear' and lr_upper.coef_[0] > 0 and lr_lower.coef_[0] > 0)
            if not is_counter_trend: return {"detected": False}

            # b. Slopes should be roughly parallel
            if not abs(lr_upper.coef_[0] - lr_lower.coef_[0]) / max(abs(lr_upper.coef_[0]), abs(lr_lower.coef_[0])) < 0.4:
                return {"detected": False}

            # c. Retracement should be healthy (not too deep)
            flag_min = flag_df['low'].min()
            flag_max = flag_df['high'].max()
            pole_start_price = df['close'].iloc[flagpole['start_idx']]
            pole_end_price = df['close'].iloc[flagpole['end_idx']]

            if flagpole['direction'] == 'bull':
                retracement = (pole_end_price - flag_min) / (pole_end_price - pole_start_price)
            else: # Bear
                retracement = (flag_max - pole_end_price) / (pole_start_price - pole_end_price)
            
            if not 0.20 < retracement < 0.60: return {"detected": False}

            # 2. Volume Contraction (MANDATORY SCORING)
            # The average volume during the consolidation phase (the flag channel) must be lower than the preceding trend
            consolidation_start_idx = flagpole['end_idx']
            consolidation_end_idx = len(df)
            
            # Get volume during consolidation phase
            consolidation_volume = df['volume'].iloc[consolidation_start_idx:consolidation_end_idx].mean()
            
            # Get volume during preceding trend (flagpole)
            trend_volume = df['volume'].iloc[flagpole['start_idx']:flagpole['end_idx']].mean()
            
            # If volume increases during consolidation, it's a red flag
            if consolidation_volume > trend_volume:
                return {"detected": False, "reason": "Volume increased during consolidation"}

            # 3. Breakout Volume Spike (MANDATORY FILTER)
            # The candle that breaks out must have volume at least 1.75x the average volume of consolidation
            last_candle = flag_df.iloc[-1]
            breakout_volume_ok = last_candle['volume'] > consolidation_volume * 1.75
            
            if not breakout_volume_ok:
                return {"detected": False, "reason": "Breakout volume not confirmed"}

            # --- 3. CONFIRMED BREAKOUT DETECTION ---
            x_last = len(flag_df) - 1
            upper_trend_val = lr_upper.predict([[x_last]])[0]
            lower_trend_val = lr_lower.predict([[x_last]])[0]
            
            breakout_detected = False
            direction = "NEUTRAL"
            pattern_name = "Unknown"

            # Bull Flag Breakout
            if flagpole['direction'] == 'bull' and last_candle['close'] > upper_trend_val:
                breakout_detected = True
                direction = "LONG"
                pattern_name = "Bull Flag"

            # Bear Flag Breakout
            if flagpole['direction'] == 'bear' and last_candle['close'] < lower_trend_val:
                breakout_detected = True
                direction = "SHORT"
                pattern_name = "Bear Flag"

            if not breakout_detected: return {"detected": False}

            # --- Scoring and Signal Generation ---
            quality_score = 60 # Base score for a confirmed pattern
            
            # Add Volume Contraction Score
            if consolidation_volume < trend_volume * 0.8:
                quality_score += 15
            elif consolidation_volume < trend_volume:
                quality_score += 10

            # Add Breakout Volume Spike Score
            if breakout_volume_ok: 
                quality_score += 15

            # Add confluence score boost
            confluence_score_boost = 0
            if flagpole['direction'] == 'bull':
                if self._is_level_significant(pole_start_price, support_levels, min_strength=3):
                    confluence_score_boost += 5
                if self._is_level_significant(pole_end_price, resistance_levels, min_strength=3):
                    confluence_score_boost += 5
                if self._is_level_significant(flag_min, support_levels, min_strength=4):
                    confluence_score_boost += 10
            else: # Bear Flag
                if self._is_level_significant(pole_start_price, resistance_levels, min_strength=3):
                    confluence_score_boost += 5
                if self._is_level_significant(pole_end_price, support_levels, min_strength=3):
                    confluence_score_boost += 5
                if self._is_level_significant(flag_max, resistance_levels, min_strength=4):
                    confluence_score_boost += 10
            quality_score += confluence_score_boost

            # CRITICAL: Confidence Filter
            if quality_score < 70: # Set a high bar for quality
                return {"detected": False, "reason": f"Low quality score: {quality_score}"}

            trends = {tf: self.analyze_trend(all_timeframes[tf], tf) for tf in ["1h", "4h", "1d", "1w"] if tf in all_timeframes}
            confidence = self.calculate_enhanced_confidence(pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score)
            return {
                "detected": True, 
                "pattern": pattern_name, 
                "confidence": confidence, 
                "direction": direction,
                "details": {
                    "quality_score": quality_score,
                    "consolidation_volume": consolidation_volume,
                    "trend_volume": trend_volume,
                    "breakout_volume": last_candle['volume'],
                    "volume_contraction_ratio": consolidation_volume / trend_volume if trend_volume > 0 else 0,
                    "breakout_volume_ratio": last_candle['volume'] / consolidation_volume if consolidation_volume > 0 else 0
                }
            }

            return {"detected": False}

        except Exception as e:
            print(f"Error in enhanced flag pattern detection: {e}")
            traceback.print_exc()
            return {"detected": False}

    def _detect_pennant(self, df: pd.DataFrame, support_levels: List[float], 
                    resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect pennant pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Pennant requires at least 20 bars
            if len(df) < 20:
                return {"detected": False, "pattern": "Pennant", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 30 bars)
            recent_df = df.tail(30).copy()
            
            # Pennant pattern consists of:
            # 1. A sharp move (the flagpole)
            # 2. A consolidation phase in a small symmetrical triangle (the pennant)
            
            # Divide the data into two parts: flagpole and potential pennant
            flagpole_df = recent_df.iloc[:15]  # First half for flagpole
            pennant_df = recent_df.iloc[15:]   # Second half for pennant
            
            # Check for flagpole (a sharp price movement)
            flagpole_start = flagpole_df['close'].iloc[0]
            flagpole_end = flagpole_df['close'].iloc[-1]
            flagpole_move = (flagpole_end / flagpole_start - 1) * 100  # Percent change
            
            # Significant flagpole: at least 10% move
            is_significant_flagpole = abs(flagpole_move) >= 10
            
            if is_significant_flagpole:
                # Determine flagpole direction
                flagpole_up = flagpole_move > 0
                
                # Find local highs and lows in pennant period
                pennant_highs = []
                pennant_lows = []
                
                for i in range(2, len(pennant_df) - 2):
                    # Local high
                    if all(pennant_df['high'].iloc[i] >= pennant_df['high'].iloc[i-j] for j in range(1, 3)) and \
                    all(pennant_df['high'].iloc[i] >= pennant_df['high'].iloc[i+j] for j in range(1, 3)):
                        pennant_highs.append((i, pennant_df['high'].iloc[i]))
                    
                    # Local low
                    if all(pennant_df['low'].iloc[i] <= pennant_df['low'].iloc[i-j] for j in range(1, 3)) and \
                    all(pennant_df['low'].iloc[i] <= pennant_df['low'].iloc[i+j] for j in range(1, 3)):
                        pennant_lows.append((i, pennant_df['low'].iloc[i]))
                
                # Need at least 2 highs and 2 lows to form a pennant
                if len(pennant_highs) >= 2 and len(pennant_lows) >= 2:
                    # Calculate slopes of upper and lower trend lines
                    if len(pennant_highs) >= 2:
                        x_highs = [point[0] for point in pennant_highs]
                        y_highs = [point[1] for point in pennant_highs]
                        slope_highs, _ = np.polyfit(x_highs, y_highs, 1)
                    else:
                        slope_highs = 0
                        
                    if len(pennant_lows) >= 2:
                        x_lows = [point[0] for point in pennant_lows]
                        y_lows = [point[1] for point in pennant_lows]
                        slope_lows, _ = np.polyfit(x_lows, y_lows, 1)
                    else:
                        slope_lows = 0
                    
                    # For a valid pennant:
                    # 1. Upper trendline should slope down
                    # 2. Lower trendline should slope up
                    # 3. Lines should converge
                    is_converging = slope_highs < 0 and slope_lows > 0
                    
                    # Verify pennant is smaller than flagpole (in terms of price range)
                    pennant_range = pennant_df['high'].max() - pennant_df['low'].min()
                    flagpole_range = flagpole_df['high'].max() - flagpole_df['low'].min()
                    is_smaller = pennant_range < flagpole_range * 0.5
                    
                    if is_converging and is_smaller:
                        # Pennant pattern detected
                        pattern_name = "Bull Pennant" if flagpole_up else "Bear Pennant"
                        direction = "LONG" if flagpole_up else "SHORT"
                        
                        # Get trend information
                        trends = {}
                        if all_timeframes:
                            for tf in ["1h", "4h", "1d", "1w"]:
                                if tf in all_timeframes:
                                    trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                        
                        # --- CONFLUENCE CHECK & SCORING ---
                        quality_score = 60
                        confluence_score_boost = 0
                        if flagpole_up:
                            if self._is_level_significant(flagpole_start, support_levels, min_strength=3):
                                confluence_score_boost += 5
                            if self._is_level_significant(flagpole_end, resistance_levels, min_strength=3):
                                confluence_score_boost += 5
                        else: # Bear Pennant
                            if self._is_level_significant(flagpole_start, resistance_levels, min_strength=3):
                                confluence_score_boost += 5
                            if self._is_level_significant(flagpole_end, support_levels, min_strength=3):
                                confluence_score_boost += 5
                        quality_score += confluence_score_boost

                        # Use confidence calculation method
                        confidence = self.calculate_signal_confidence(
                            pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                        )
                        
                        return {
                            "detected": True,
                            "pattern": pattern_name,
                            "confidence": confidence,
                            "direction": direction
                        }
            
            return {"detected": False, "pattern": "Pennant", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in pennant detection: {str(e)}")
            return {"detected": False, "pattern": "Pennant", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_triple_top(self, df: pd.DataFrame, support_levels: List[float], 
                        resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect triple top pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Triple top requires at least 60 bars
            if len(df) < 60:
                return {"detected": False, "pattern": "Triple Top", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 60 bars)
            recent_df = df.tail(60).copy()
            
            # Find local peaks
            peaks = []
            for i in range(5, len(recent_df) - 5):
                # Check if this is a local high (5 periods before and after)
                if all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 6)) and \
                all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 6)):
                    peaks.append((i, recent_df['high'].iloc[i]))
            
            # Need at least 3 peaks
            if len(peaks) < 3:
                return {"detected": False, "pattern": "Triple Top", "confidence": 0, "direction": "NEUTRAL"}
            
            # Check the last three peaks
            if len(peaks) >= 3:
                peak1_idx, peak1_val = peaks[-3]
                peak2_idx, peak2_val = peaks[-2]
                peak3_idx, peak3_val = peaks[-1]
                
                # Triple top requirements:
                # 1. Three peaks at similar levels (within 2%)
                peaks_similar = (abs(peak2_val - peak1_val) / peak1_val <= 0.02) and \
                            (abs(peak3_val - peak1_val) / peak1_val <= 0.02)
                
                # 2. Peaks should be adequately spaced
                well_spaced = (peak2_idx - peak1_idx >= 8) and (peak3_idx - peak2_idx >= 8)
                
                # 3. Valleys between peaks
                valley1_idx = peak1_idx + (peak2_idx - peak1_idx) // 2
                valley2_idx = peak2_idx + (peak3_idx - peak2_idx) // 2
                
                valley1_val = recent_df['low'].iloc[valley1_idx]
                valley2_val = recent_df['low'].iloc[valley2_idx]
                
                # Calculate neckline as average of valley lows
                neckline = (valley1_val + valley2_val) / 2
                
                # 4. Confirmation: price below neckline
                current_price = recent_df['close'].iloc[-1]
                confirmed = current_price < neckline
                
                if peaks_similar and well_spaced:
                    # --- CONFLUENCE CHECK & SCORING ---
                    quality_score = 70  # Base score for a rare pattern
                    
                    # Adjust confidence based on confirmation
                    if confirmed:
                        quality_score += 15

                    confluence_score_boost = 0
                    # Check if the three peaks align with a strong resistance zone.
                    if self._is_level_significant(peak1_val, resistance_levels, min_strength=5):
                        confluence_score_boost += 10
                    if self._is_level_significant(peak2_val, resistance_levels, min_strength=5):
                        confluence_score_boost += 10
                    if self._is_level_significant(peak3_val, resistance_levels, min_strength=5):
                        confluence_score_boost += 10

                    # Check if the neckline (troughs) align with a strong support zone.
                    if self._is_level_significant(neckline, support_levels, min_strength=4):
                        confluence_score_boost += 15
                    
                    quality_score += confluence_score_boost

                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use confidence calculation method
                    final_confidence = self.calculate_signal_confidence(
                        "Triple Top", "SHORT", df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                    )
                    
                    return {
                        "detected": True,
                        "pattern": "Triple Top",
                        "confidence": final_confidence,
                        "direction": "SHORT"
                    }
            
            return {"detected": False, "pattern": "Triple Top", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in triple top detection: {str(e)}")
            return {"detected": False, "pattern": "Triple Top", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_triple_bottom(self, df: pd.DataFrame, support_levels: List[float], 
                            resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect triple bottom pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Triple bottom requires at least 60 bars
            if len(df) < 60:
                return {"detected": False, "pattern": "Triple Bottom", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 60 bars)
            recent_df = df.tail(60).copy()
            
            # Find local troughs
            troughs = []
            for i in range(5, len(recent_df) - 5):
                # Check if this is a local low (5 periods before and after)
                if all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 6)) and \
                all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 6)):
                    troughs.append((i, recent_df['low'].iloc[i]))
            
            # Need at least 3 troughs
            if len(troughs) < 3:
                return {"detected": False, "pattern": "Triple Bottom", "confidence": 0, "direction": "NEUTRAL"}
            
            # Check the last three troughs
            if len(troughs) >= 3:
                trough1_idx, trough1_val = troughs[-3]
                trough2_idx, trough2_val = troughs[-2]
                trough3_idx, trough3_val = troughs[-1]
                
                # Triple bottom requirements:
                # 1. Three troughs at similar levels (within 2%)
                troughs_similar = (abs(trough2_val - trough1_val) / trough1_val <= 0.02) and \
                                (abs(trough3_val - trough1_val) / trough1_val <= 0.02)
                
                # 2. Troughs should be adequately spaced
                well_spaced = (trough2_idx - trough1_idx >= 8) and (trough3_idx - trough2_idx >= 8)
                
                # 3. Peaks between troughs
                peak1_idx = trough1_idx + (trough2_idx - trough1_idx) // 2
                peak2_idx = trough2_idx + (trough3_idx - trough2_idx) // 2
                
                peak1_val = recent_df['high'].iloc[peak1_idx]
                peak2_val = recent_df['high'].iloc[peak2_idx]
                
                # Calculate neckline as average of peak highs
                neckline = (peak1_val + peak2_val) / 2
                
                # 4. Confirmation: price above neckline
                current_price = recent_df['close'].iloc[-1]
                confirmed = current_price > neckline
                
                if troughs_similar and well_spaced:
                    # --- CONFLUENCE CHECK & SCORING ---
                    quality_score = 70  # Base score for a rare pattern
                    
                    # Adjust confidence based on confirmation
                    if confirmed:
                        quality_score += 15

                    confluence_score_boost = 0
                    # Check if the three troughs align with a strong support zone.
                    if self._is_level_significant(trough1_val, support_levels, min_strength=5):
                        confluence_score_boost += 10
                    if self._is_level_significant(trough2_val, support_levels, min_strength=5):
                        confluence_score_boost += 10
                    if self._is_level_significant(trough3_val, support_levels, min_strength=5):
                        confluence_score_boost += 10

                    # Check if the neckline (peaks) align with a strong resistance zone.
                    if self._is_level_significant(neckline, resistance_levels, min_strength=4):
                        confluence_score_boost += 15

                    quality_score += confluence_score_boost
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use confidence calculation method
                    final_confidence = self.calculate_signal_confidence(
                        "Triple Bottom", "LONG", df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                    )
                    
                    return {
                        "detected": True,
                        "pattern": "Triple Bottom",
                        "confidence": final_confidence,
                        "direction": "LONG"
                    }
            
            return {"detected": False, "pattern": "Triple Bottom", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in triple bottom detection: {str(e)}")
            return {"detected": False, "pattern": "Triple Bottom", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_rectangle(self, df: pd.DataFrame, support_levels: List[float], 
                        resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect rectangle pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Rectangle requires at least 30 bars
            if len(df) < 30:
                return {"detected": False, "pattern": "Rectangle", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 30 bars)
            recent_df = df.tail(30).copy()
            
            # Identify potential support and resistance lines
            highs = recent_df['high'].values
            lows = recent_df['low'].values
            
            # Find potential resistance level (use upper quartile of highs)
            potential_resistance = np.percentile(highs, 75)
            
            # Find potential support level (use lower quartile of lows)
            potential_support = np.percentile(lows, 25)
            
            # Count touches of resistance and support levels
            resistance_touches = sum(1 for h in highs if abs(h - potential_resistance) / potential_resistance < 0.01)
            support_touches = sum(1 for l in lows if abs(l - potential_support) / potential_support < 0.01)
            
            # Rectangle criteria:
            # 1. At least 3 touches of either level
            has_enough_touches = resistance_touches >= 3 or support_touches >= 3
            
            # 2. Channel width is significant (at least 3% but not more than 20%)
            channel_width = (potential_resistance - potential_support) / potential_support
            is_valid_width = 0.03 <= channel_width <= 0.20
            
            # 3. Channel is relatively flat (no significant trend)
            x = np.arange(len(recent_df))
            close_prices = recent_df['close'].values
            slope, _ = np.polyfit(x, close_prices, 1)
            
            # Normalize slope as percentage of price
            avg_price = np.mean(close_prices)
            norm_slope = (slope * len(recent_df)) / avg_price
            
            is_flat = abs(norm_slope) < 0.05  # Less than 5% overall trend
            
            if has_enough_touches and is_valid_width and is_flat:
                # Rectangle pattern detected
                current_price = recent_df['close'].iloc[-1]
                
                # Determine trading direction based on position in rectangle
                position_ratio = (current_price - potential_support) / (potential_resistance - potential_support)
                
                if position_ratio < 0.3:
                    # Near support - potential buy
                    direction = "LONG"
                elif position_ratio > 0.7:
                    # Near resistance - potential sell
                    direction = "SHORT"
                else:
                    # Middle of range - neutral
                    direction = "NEUTRAL"
                
                # Only return a signal if we have a trading direction
                if direction != "NEUTRAL":
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # --- CONFLUENCE CHECK & SCORING ---
                    quality_score = 60 # Base score
                    confluence_score_boost = 0
                    if self._is_level_significant(potential_resistance, resistance_levels, min_strength=4):
                        confluence_score_boost += 10
                    if self._is_level_significant(potential_support, support_levels, min_strength=4):
                        confluence_score_boost += 10
                    quality_score += confluence_score_boost

                    # Use confidence calculation method
                    confidence = self.calculate_signal_confidence(
                        "Rectangle", direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                    )
                    
                    return {
                        "detected": True,
                        "pattern": "Rectangle",
                        "confidence": confidence,
                        "direction": direction
                    }
            
            return {"detected": False, "pattern": "Rectangle", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in rectangle detection: {str(e)}")
            return {"detected": False, "pattern": "Rectangle", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_point_retracement(self, df: pd.DataFrame, support_levels: List[float], 
                            resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect Fibonacci retracement pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Need at least 30 bars for pattern detection
            if len(df) < 30:
                return {"detected": False, "pattern": "Fibonacci Retracement", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 30 bars)
            recent_df = df.tail(30).copy()
            
            # Find significant swing high and low in recent data
            swing_high = recent_df['high'].max()
            swing_low = recent_df['low'].min()
            
            # Fibonacci retracement levels (standard)
            fib_levels = [0.236, 0.382, 0.5, 0.618, 0.786]
            
            # Calculate retracement levels
            # For uptrend: start from low to high
            # For downtrend: start from high to low
            
            # Determine if we're in an uptrend or downtrend
            uptrend = False
            downtrend = False
            
            if 'ema50' in df.columns and 'ema200' in df.columns:
                ema50 = df['ema50'].iloc[-1]
                ema200 = df['ema200'].iloc[-1]
                uptrend = ema50 > ema200 * 1.01
                downtrend = ema50 < ema200 * 0.99
            
            # Only proceed if we have a clear trend
            if not (uptrend or downtrend):
                return {"detected": False, "pattern": "Fibonacci Retracement", "confidence": 0, "direction": "NEUTRAL"}
            
            # Calculate retracement levels based on trend
            retracement_levels = []
            
            if uptrend:
                # In uptrend, calculate retracements from low to high
                range_size = swing_high - swing_low
                for fib in fib_levels:
                    retracement = swing_high - (range_size * fib)
                    retracement_levels.append(retracement)
            else:
                # In downtrend, calculate retracements from high to low
                range_size = swing_high - swing_low
                for fib in fib_levels:
                    retracement = swing_low + (range_size * fib)
                    retracement_levels.append(retracement)
            
            # Check if current price is near any retracement level
            current_price = recent_df['close'].iloc[-1]
            nearest_level = None
            nearest_distance_pct = float('inf')
            
            for level in retracement_levels:
                distance_pct = abs(current_price - level) / level
                if distance_pct < nearest_distance_pct:
                    nearest_distance_pct = distance_pct
                    nearest_level = level
            
            # If price is within 1% of a Fibonacci level, consider it a valid signal
            if nearest_distance_pct <= 0.01:
                # Determine direction based on trend and price position
                direction = "NEUTRAL"
                
                if uptrend:
                    # In uptrend, buy on pullbacks to retracement levels
                    direction = "LONG"
                elif downtrend:
                    # In downtrend, sell on bounces to retracement levels
                    direction = "SHORT"
                
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use confidence calculation method
                confidence = self.calculate_signal_confidence(
                    "Fibonacci Retracement", direction, df, support_levels, resistance_levels, trends
                )
                
                return {
                    "detected": True,
                    "pattern": "Fibonacci Retracement",
                    "confidence": confidence,
                    "direction": direction
                }
            
            return {"detected": False, "pattern": "Fibonacci Retracement", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in Fibonacci retracement detection: {str(e)}")
            return {"detected": False, "pattern": "Fibonacci Retracement", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_point_extension(self, df: pd.DataFrame, support_levels: List[float], 
                            resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect Fibonacci extension pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Need at least 30 bars for pattern detection
            if len(df) < 30:
                return {"detected": False, "pattern": "Fibonacci Extension", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 30 bars)
            recent_df = df.tail(30).copy()
            
            # Find significant swing high and low in recent data
            swing_high = recent_df['high'].max()
            swing_low = recent_df['low'].min()
            
            # Fibonacci extension levels (standard)
            fib_levels = [1.0, 1.272, 1.414, 1.618, 2.0, 2.618]
            
            # Determine if we're in an uptrend or downtrend
            uptrend = False
            downtrend = False
            
            if 'ema50' in df.columns and 'ema200' in df.columns:
                ema50 = df['ema50'].iloc[-1]
                ema200 = df['ema200'].iloc[-1]
                uptrend = ema50 > ema200 * 1.01
                downtrend = ema50 < ema200 * 0.99
            
            # Only proceed if we have a clear trend
            if not (uptrend or downtrend):
                return {"detected": False, "pattern": "Fibonacci Extension", "confidence": 0, "direction": "NEUTRAL"}
            
            # Calculate extension levels based on trend
            extension_levels = []
            
            if uptrend:
                # In uptrend, calculate extensions from low to high
                range_size = swing_high - swing_low
                for fib in fib_levels:
                    extension = swing_high + (range_size * (fib - 1.0))
                    extension_levels.append(extension)
            else:
                # In downtrend, calculate extensions from high to low
                range_size = swing_high - swing_low
                for fib in fib_levels:
                    extension = swing_low - (range_size * (fib - 1.0))
                    extension_levels.append(extension)
            
            # Check if current price is approaching an extension level
            current_price = recent_df['close'].iloc[-1]
            
            # Find the next target extension level
            next_target = None
            
            if uptrend:
                # In uptrend, find the next higher extension level
                potential_targets = [level for level in extension_levels if level > current_price]
                if potential_targets:
                    next_target = min(potential_targets)
            else:
                # In downtrend, find the next lower extension level
                potential_targets = [level for level in extension_levels if level < current_price]
                if potential_targets:
                    next_target = max(potential_targets)
            
            # If we have a target and it's reasonably close (within 10% of current price)
            if next_target is not None and abs(next_target - current_price) / current_price <= 0.1:
                # Determine direction based on trend
                direction = "LONG" if uptrend else "SHORT"
                
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use confidence calculation method
                confidence = self.calculate_signal_confidence(
                    "Fibonacci Extension", direction, df, support_levels, resistance_levels, trends
                )
                
                return {
                    "detected": True,
                    "pattern": "Fibonacci Extension",
                    "confidence": confidence,
                    "direction": direction
                }
            
            return {"detected": False, "pattern": "Fibonacci Extension", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in Fibonacci extension detection: {str(e)}")
            return {"detected": False, "pattern": "Fibonacci Extension", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_abcd_pattern(self, df: pd.DataFrame, support_levels: List[float], 
                        resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect ABCD harmonic pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # ABCD pattern requires at least 20 bars
            if len(df) < 20:
                return {"detected": False, "pattern": "ABCD Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 30 bars)
            recent_df = df.tail(30).copy()
            
            # Find potential swing points (peaks and troughs)
            swing_points = []
            
            for i in range(3, len(recent_df) - 3):
                # Local high
                if all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 4)) and \
                all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 4)):
                    swing_points.append((i, recent_df['high'].iloc[i], 'high'))
                
                # Local low
                if all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 4)) and \
                all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 4)):
                    swing_points.append((i, recent_df['low'].iloc[i], 'low'))
            
            # Sort swing points by index
            swing_points.sort(key=lambda x: x[0])
            
            # Need at least 4 swing points to form ABCD
            if len(swing_points) < 4:
                return {"detected": False, "pattern": "ABCD Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Look for ABCD patterns in the swing points
            for i in range(len(swing_points) - 3):
                # Get potential ABCD points
                a_idx, a_val, a_type = swing_points[i]
                b_idx, b_val, b_type = swing_points[i+1]
                c_idx, c_val, c_type = swing_points[i+2]
                d_idx, d_val, d_type = swing_points[i+3]
                
                # Check alternating high/low pattern
                if a_type == b_type or b_type == c_type or c_type == d_type:
                    continue
                
                # Verify proper sequence
                if not (a_idx < b_idx < c_idx < d_idx):
                    continue
                
                # Calculate leg lengths
                ab_length = abs(b_val - a_val)
                bc_length = abs(c_val - b_val)
                cd_length = abs(d_val - c_val)
                
                # ABCD criteria for bullish pattern (A and C are lows, B and D are highs)
                bullish = a_type == 'low' and b_type == 'high' and c_type == 'low' and d_type == 'high'
                
                # ABCD criteria for bearish pattern (A and C are highs, B and D are lows)
                bearish = a_type == 'high' and b_type == 'low' and c_type == 'high' and d_type == 'low'
                
                # Check Fibonacci relationships (approximate)
                # AB  CD and BC is a retracement of AB (typically around 0.618)
                ab_cd_similar = 0.8 <= (ab_length / cd_length) <= 1.2
                bc_retracement = 0.5 <= (bc_length / ab_length) <= 0.786
                
                if (bullish or bearish) and ab_cd_similar and bc_retracement:
                    # ABCD pattern found
                    direction = "LONG" if bullish else "SHORT"
                    
                    # D point should be recent (within last 5 bars)
                    is_recent = d_idx >= len(recent_df) - 5
                    
                    if is_recent:
                        # --- CONFLUENCE CHECK & SCORING ---
                        quality_score = 65
                        confluence_score_boost = 0
                        sr_levels_to_check = support_levels if direction == "LONG" else resistance_levels

                        if self._is_level_significant(d_val, sr_levels_to_check, min_strength=4):
                            confluence_score_boost += 20
                        if self._is_level_significant(a_val, sr_levels_to_check, min_strength=3):
                            confluence_score_boost += 5
                        if self._is_level_significant(c_val, sr_levels_to_check, min_strength=3):
                            confluence_score_boost += 5
                        quality_score += confluence_score_boost

                        # Get trend information
                        trends = {}
                        if all_timeframes:
                            for tf in ["1h", "4h", "1d", "1w"]:
                                if tf in all_timeframes:
                                    trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                        
                        # Use confidence calculation method
                        confidence = self.calculate_signal_confidence(
                            "ABCD Pattern", direction, df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                        )
                        
                        return {
                            "detected": True,
                            "pattern": "ABCD Pattern",
                            "confidence": confidence,
                            "direction": direction
                        }
            
            return {"detected": False, "pattern": "ABCD Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in ABCD pattern detection: {str(e)}")
            return {"detected": False, "pattern": "ABCD Pattern", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_gartley(self, df: pd.DataFrame, support_levels: List[float], 
                    resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect Gartley harmonic pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Gartley pattern requires at least 50 bars
            if len(df) < 50:
                return {"detected": False, "pattern": "Gartley Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 50 bars)
            recent_df = df.tail(50).copy()
            
            # Find potential swing points (peaks and troughs)
            swing_points = []
            
            for i in range(3, len(recent_df) - 3):
                # Local high
                if all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 4)) and \
                all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 4)):
                    swing_points.append((i, recent_df['high'].iloc[i], 'high'))
                
                # Local low
                if all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 4)) and \
                all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 4)):
                    swing_points.append((i, recent_df['low'].iloc[i], 'low'))
            
            # Sort swing points by index
            swing_points.sort(key=lambda x: x[0])
            
            # Need at least 5 swing points to form XABCD (Gartley)
            if len(swing_points) < 5:
                return {"detected": False, "pattern": "Gartley Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Look for Gartley patterns in the swing points
            for i in range(len(swing_points) - 4):
                # Get potential XABCD points
                x_idx, x_val, x_type = swing_points[i]
                a_idx, a_val, a_type = swing_points[i+1]
                b_idx, b_val, b_type = swing_points[i+2]
                c_idx, c_val, c_type = swing_points[i+3]
                d_idx, d_val, d_type = swing_points[i+4]
                
                # Check alternating high/low pattern
                if x_type == a_type or a_type == b_type or b_type == c_type or c_type == d_type:
                    continue
                
                # Verify proper sequence
                if not (x_idx < a_idx < b_idx < c_idx < d_idx):
                    continue
                
                # Calculate leg lengths and retracements
                xa_length = abs(a_val - x_val)
                ab_length = abs(b_val - a_val)
                bc_length = abs(c_val - b_val)
                cd_length = abs(d_val - c_val)
                
                # Calculate retracement ratios
                ab_xa_ratio = ab_length / xa_length if xa_length > 0 else 0
                bc_ab_ratio = bc_length / ab_length if ab_length > 0 else 0
                cd_bc_ratio = cd_length / bc_length if bc_length > 0 else 0
                
                # Gartley pattern specific ratios for bullish pattern
                if x_type == 'low' and a_type == 'high':
                    # Bullish Gartley
                    # AB = 0.618 of XA
                    # BC = 0.382 to 0.886 of AB
                    # CD = 1.272 to 1.618 of BC
                    # D completes at 0.786 of XA
                    
                    ab_xa_valid = 0.58 <= ab_xa_ratio <= 0.65
                    bc_ab_valid = 0.38 <= bc_ab_ratio <= 0.886
                    cd_bc_valid = 1.27 <= cd_bc_ratio <= 1.618
                    
                    xd_retracement = abs(d_val - x_val) / xa_length
                    d_completes_valid = 0.75 <= xd_retracement <= 0.82
                    
                    if ab_xa_valid and bc_ab_valid and cd_bc_valid and d_completes_valid:
                        # D point should be recent (within last 5 bars)
                        is_recent = d_idx >= len(recent_df) - 5
                        
                        if is_recent:
                            # --- CONFLUENCE CHECK & SCORING ---
                            quality_score = 70
                            confluence_score_boost = 0
                            if self._is_level_significant(d_val, support_levels, min_strength=4):
                                confluence_score_boost += 20
                            if self._is_level_significant(x_val, support_levels, min_strength=3):
                                confluence_score_boost += 5
                            quality_score += confluence_score_boost

                            # Get trend information
                            trends = {}
                            if all_timeframes:
                                for tf in ["1h", "4h", "1d", "1w"]:
                                    if tf in all_timeframes:
                                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                            
                            # Use confidence calculation method
                            confidence = self.calculate_signal_confidence(
                                "Bullish Gartley", "LONG", df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                            )
                            
                            return {
                                "detected": True,
                                "pattern": "Bullish Gartley",
                                "confidence": confidence,
                                "direction": "LONG"
                            }
                
                elif x_type == 'high' and a_type == 'low':
                    # Bearish Gartley
                    # AB = 0.618 of XA
                    # BC = 0.382 to 0.886 of AB
                    # CD = 1.272 to 1.618 of BC
                    # D completes at 0.786 of XA
                    
                    ab_xa_valid = 0.58 <= ab_xa_ratio <= 0.65
                    bc_ab_valid = 0.38 <= bc_ab_ratio <= 0.886
                    cd_bc_valid = 1.27 <= cd_bc_ratio <= 1.618
                    
                    xd_retracement = abs(d_val - x_val) / xa_length
                    d_completes_valid = 0.75 <= xd_retracement <= 0.82
                    
                    if ab_xa_valid and bc_ab_valid and cd_bc_valid and d_completes_valid:
                        # D point should be recent (within last 5 bars)
                        is_recent = d_idx >= len(recent_df) - 5
                        
                        if is_recent:
                            # --- CONFLUENCE CHECK & SCORING ---
                            quality_score = 70
                            confluence_score_boost = 0
                            if self._is_level_significant(d_val, resistance_levels, min_strength=4):
                                confluence_score_boost += 20
                            if self._is_level_significant(x_val, resistance_levels, min_strength=3):
                                confluence_score_boost += 5
                            quality_score += confluence_score_boost

                            # Get trend information
                            trends = {}
                            if all_timeframes:
                                for tf in ["1h", "4h", "1d", "1w"]:
                                    if tf in all_timeframes:
                                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                            
                            # Use confidence calculation method
                            confidence = self.calculate_signal_confidence(
                                "Bearish Gartley", "SHORT", df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                            )
                            
                            return {
                                "detected": True,
                                "pattern": "Bearish Gartley",
                                "confidence": confidence,
                                "direction": "SHORT"
                            }
            
            return {"detected": False, "pattern": "Gartley Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in Gartley pattern detection: {str(e)}")
            return {"detected": False, "pattern": "Gartley Pattern", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_butterfly(self, df: pd.DataFrame, support_levels: List[float], 
                        resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect Butterfly harmonic pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Butterfly pattern requires at least 50 bars
            if len(df) < 50:
                return {"detected": False, "pattern": "Butterfly Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 50 bars)
            recent_df = df.tail(50).copy()
            
            # Find potential swing points (peaks and troughs)
            swing_points = []
            
            for i in range(3, len(recent_df) - 3):
                # Local high
                if all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 4)) and \
                all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 4)):
                    swing_points.append((i, recent_df['high'].iloc[i], 'high'))
                
                # Local low
                if all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 4)) and \
                all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 4)):
                    swing_points.append((i, recent_df['low'].iloc[i], 'low'))
            
            # Sort swing points by index
            swing_points.sort(key=lambda x: x[0])
            
            # Need at least 5 swing points to form XABCD (Butterfly)
            if len(swing_points) < 5:
                return {"detected": False, "pattern": "Butterfly Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Look for Butterfly patterns in the swing points
            for i in range(len(swing_points) - 4):
                # Get potential XABCD points
                x_idx, x_val, x_type = swing_points[i]
                a_idx, a_val, a_type = swing_points[i+1]
                b_idx, b_val, b_type = swing_points[i+2]
                c_idx, c_val, c_type = swing_points[i+3]
                d_idx, d_val, d_type = swing_points[i+4]
                
                # Check alternating high/low pattern
                if x_type == a_type or a_type == b_type or b_type == c_type or c_type == d_type:
                    continue
                
                # Verify proper sequence
                if not (x_idx < a_idx < b_idx < c_idx < d_idx):
                    continue
                
                # Calculate leg lengths and retracements
                xa_length = abs(a_val - x_val)
                ab_length = abs(b_val - a_val)
                bc_length = abs(c_val - b_val)
                cd_length = abs(d_val - c_val)
                
                # Calculate retracement ratios
                ab_xa_ratio = ab_length / xa_length if xa_length > 0 else 0
                bc_ab_ratio = bc_length / ab_length if ab_length > 0 else 0
                cd_bc_ratio = cd_length / bc_length if bc_length > 0 else 0
                
                # Butterfly pattern specific ratios for bullish pattern
                if x_type == 'low' and a_type == 'high':
                    # Bullish Butterfly
                    # AB = 0.786 of XA
                    # BC = 0.382 to 0.886 of AB
                    # CD = 1.618 to 2.618 of BC
                    # D completes at 1.27 to 1.618 extension of XA
                    
                    ab_xa_valid = 0.75 <= ab_xa_ratio <= 0.82
                    bc_ab_valid = 0.38 <= bc_ab_ratio <= 0.886
                    cd_bc_valid = 1.618 <= cd_bc_ratio <= 2.618
                    
                    xd_extension = abs(d_val - x_val) / xa_length
                    d_completes_valid = 1.27 <= xd_extension <= 1.618
                    
                    # Check if D is below X (extension)
                    d_below_x = d_val < x_val
                    
                    if ab_xa_valid and bc_ab_valid and cd_bc_valid and d_completes_valid and d_below_x:
                        # D point should be recent (within last 5 bars)
                        is_recent = d_idx >= len(recent_df) - 5
                        
                        if is_recent:
                            # --- CONFLUENCE CHECK & SCORING ---
                            quality_score = 70
                            confluence_score_boost = 0
                            if self._is_level_significant(d_val, support_levels, min_strength=4):
                                confluence_score_boost += 20
                            if self._is_level_significant(x_val, support_levels, min_strength=3):
                                confluence_score_boost += 5
                            quality_score += confluence_score_boost

                            # Get trend information
                            trends = {}
                            if all_timeframes:
                                for tf in ["1h", "4h", "1d", "1w"]:
                                    if tf in all_timeframes:
                                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                            
                            # Use confidence calculation method
                            confidence = self.calculate_signal_confidence(
                                "Bullish Butterfly", "LONG", df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                            )
                            
                            return {
                                "detected": True,
                                "pattern": "Bullish Butterfly",
                                "confidence": confidence,
                                "direction": "LONG"
                            }
                
                elif x_type == 'high' and a_type == 'low':
                    # Bearish Butterfly
                    # AB = 0.786 of XA
                    # BC = 0.382 to 0.886 of AB
                    # CD = 1.618 to 2.618 of BC
                    # D completes at 1.27 to 1.618 extension of XA
                    
                    ab_xa_valid = 0.75 <= ab_xa_ratio <= 0.82
                    bc_ab_valid = 0.38 <= bc_ab_ratio <= 0.886
                    cd_bc_valid = 1.618 <= cd_bc_ratio <= 2.618
                    
                    xd_extension = abs(d_val - x_val) / xa_length
                    d_completes_valid = 1.27 <= xd_extension <= 1.618
                    
                    # Check if D is above X (extension)
                    d_above_x = d_val > x_val
                    
                    if ab_xa_valid and bc_ab_valid and cd_bc_valid and d_completes_valid and d_above_x:
                        # D point should be recent (within last 5 bars)
                        is_recent = d_idx >= len(recent_df) - 5
                        
                        if is_recent:
                            # --- CONFLUENCE CHECK & SCORING ---
                            quality_score = 70
                            confluence_score_boost = 0
                            if self._is_level_significant(d_val, resistance_levels, min_strength=4):
                                confluence_score_boost += 20
                            if self._is_level_significant(x_val, resistance_levels, min_strength=3):
                                confluence_score_boost += 5
                            quality_score += confluence_score_boost

                            # Get trend information
                            trends = {}
                            if all_timeframes:
                                for tf in ["1h", "4h", "1d", "1w"]:
                                    if tf in all_timeframes:
                                        trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                            
                            # Use confidence calculation method
                            confidence = self.calculate_signal_confidence(
                                "Bearish Butterfly", "SHORT", df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                            )
                            
                            return {
                                "detected": True,
                                "pattern": "Bearish Butterfly",
                                "confidence": confidence,
                                "direction": "SHORT"
                            }
            
            return {"detected": False, "pattern": "Butterfly Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in Butterfly pattern detection: {str(e)}")
            return {"detected": False, "pattern": "Butterfly Pattern", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_drive(self, df: pd.DataFrame, support_levels: List[float], 
                resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect Three-Drive harmonic pattern
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Three-Drive pattern requires at least 40 bars
            if len(df) < 40:
                return {"detected": False, "pattern": "Three-Drive Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 40 bars)
            recent_df = df.tail(40).copy()
            
            # Find potential swing points (peaks and troughs)
            swing_points = []
            
            for i in range(3, len(recent_df) - 3):
                # Local high
                if all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i-j] for j in range(1, 4)) and \
                all(recent_df['high'].iloc[i] >= recent_df['high'].iloc[i+j] for j in range(1, 4)):
                    swing_points.append((i, recent_df['high'].iloc[i], 'high'))
                
                # Local low
                if all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i-j] for j in range(1, 4)) and \
                all(recent_df['low'].iloc[i] <= recent_df['low'].iloc[i+j] for j in range(1, 4)):
                    swing_points.append((i, recent_df['low'].iloc[i], 'low'))
            
            # Sort swing points by index
            swing_points.sort(key=lambda x: x[0])
            
            # Need at least 6 swing points for Three-Drive (3 drives + 3 corrections)
            if len(swing_points) < 6:
                return {"detected": False, "pattern": "Three-Drive Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Look for Three-Drive patterns in the swing points
            for i in range(len(swing_points) - 5):
                # Get 6 consecutive points (3 drives and 3 corrections)
                p0_idx, p0_val, p0_type = swing_points[i]
                p1_idx, p1_val, p1_type = swing_points[i+1]
                p2_idx, p2_val, p2_type = swing_points[i+2]
                p3_idx, p3_val, p3_type = swing_points[i+3]
                p4_idx, p4_val, p4_type = swing_points[i+4]
                p5_idx, p5_val, p5_type = swing_points[i+5]
                
                # Check alternating high/low pattern
                if p0_type == p1_type or p1_type == p2_type or p2_type == p3_type or p3_type == p4_type or p4_type == p5_type:
                    continue
                
                # Verify proper sequence
                if not (p0_idx < p1_idx < p2_idx < p3_idx < p4_idx < p5_idx):
                    continue
                
                # Analyze bullish Three-Drive pattern (p0, p2, p4 are lows / p1, p3, p5 are highs)
                if p0_type == 'low' and p2_type == 'low' and p4_type == 'low' and \
                p1_type == 'high' and p3_type == 'high' and p5_type == 'high':
                    
                    # Check if drives are getting progressively lower
                    drives_getting_lower = p0_val > p2_val > p4_val
                    
                    # Check if corrections are getting progressively lower
                    corrections_getting_lower = p1_val > p3_val > p5_val
                    
                    if drives_getting_lower and corrections_getting_lower:
                        # Check Fibonacci relationships (approximate)
                        # Drive 2 should be 1.27 to 1.618 of Drive 1
                        # Drive 3 should be 1.27 to 1.618 of Drive 2
                        drive1_length = abs(p1_val - p0_val)
                        drive2_length = abs(p3_val - p2_val)
                        drive3_length = abs(p5_val - p4_val)
                        
                        drive2_to_drive1_ratio = drive2_length / drive1_length if drive1_length > 0 else 0
                        drive3_to_drive2_ratio = drive3_length / drive2_length if drive2_length > 0 else 0
                        
                        valid_drive_ratios = (1.27 <= drive2_to_drive1_ratio <= 1.618) and \
                                            (1.27 <= drive3_to_drive2_ratio <= 1.618)
                        
                        if valid_drive_ratios:
                            # Pattern is valid and recent (last point is within 3 bars)
                            is_recent = p5_idx >= len(recent_df) - 3
                            
                            if is_recent:
                                # --- CONFLUENCE CHECK & SCORING ---
                                quality_score = 70
                                confluence_score_boost = 0
                                if self._is_level_significant(p5_val, resistance_levels, min_strength=4):
                                    confluence_score_boost += 20
                                quality_score += confluence_score_boost

                                # Get trend information
                                trends = {}
                                if all_timeframes:
                                    for tf in ["1h", "4h", "1d", "1w"]:
                                        if tf in all_timeframes:
                                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                                
                                # Use confidence calculation method
                                confidence = self.calculate_signal_confidence(
                                    "Bearish Three-Drive", "SHORT", df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                                )
                                
                                return {
                                    "detected": True,
                                    "pattern": "Bearish Three-Drive",
                                    "confidence": confidence,
                                    "direction": "SHORT"  # Reversal expected after three downward drives
                                }
                
                # Analyze bearish Three-Drive pattern (p0, p2, p4 are highs / p1, p3, p5 are lows)
                elif p0_type == 'high' and p2_type == 'high' and p4_type == 'high' and \
                    p1_type == 'low' and p3_type == 'low' and p5_type == 'low':
                    
                    # Check if drives are getting progressively higher
                    drives_getting_higher = p0_val < p2_val < p4_val
                    
                    # Check if corrections are getting progressively higher
                    corrections_getting_higher = p1_val < p3_val < p5_val
                    
                    if drives_getting_higher and corrections_getting_higher:
                        # Check Fibonacci relationships (approximate)
                        drive1_length = abs(p1_val - p0_val)
                        drive2_length = abs(p3_val - p2_val)
                        drive3_length = abs(p5_val - p4_val)
                        
                        drive2_to_drive1_ratio = drive2_length / drive1_length if drive1_length > 0 else 0
                        drive3_to_drive2_ratio = drive3_length / drive2_length if drive2_length > 0 else 0
                        
                        valid_drive_ratios = (1.27 <= drive2_to_drive1_ratio <= 1.618) and \
                                            (1.27 <= drive3_to_drive2_ratio <= 1.618)
                        
                        if valid_drive_ratios:
                            # Pattern is valid and recent (last point is within 3 bars)
                            is_recent = p5_idx >= len(recent_df) - 3
                            
                            if is_recent:
                                # --- CONFLUENCE CHECK & SCORING ---
                                quality_score = 70
                                confluence_score_boost = 0
                                if self._is_level_significant(p5_val, support_levels, min_strength=4):
                                    confluence_score_boost += 20
                                quality_score += confluence_score_boost

                                # Get trend information
                                trends = {}
                                if all_timeframes:
                                    for tf in ["1h", "4h", "1d", "1w"]:
                                        if tf in all_timeframes:
                                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                                
                                # Use confidence calculation method
                                confidence = self.calculate_signal_confidence(
                                    "Bullish Three-Drive", "LONG", df, support_levels, resistance_levels, trends, base_confidence_override=quality_score
                                )
                                
                                return {
                                    "detected": True,
                                    "pattern": "Bullish Three-Drive",
                                    "confidence": confidence,
                                    "direction": "LONG"  # Reversal expected after three upward drives
                                }
            
            return {"detected": False, "pattern": "Three-Drive Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in Three-Drive pattern detection: {str(e)}")
            return {"detected": False, "pattern": "Three-Drive Pattern", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_consecutive_candles(self, df: pd.DataFrame, support_levels: List[float], 
                                resistance_levels: List[float], all_timeframes: Dict[str, pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Detect consecutive candle patterns
        
        Args:
            df: DataFrame with price data
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Need at least 10 bars
            if len(df) < 10:
                return {"detected": False, "pattern": "Consecutive Candles", "confidence": 0, "direction": "NEUTRAL"}
            
            # Use recent data (last 10 bars)
            recent_df = df.tail(10).copy()
            
            # Look for three consecutive bullish or bearish candles
            consecutive_count = 3
            
            # Check for consecutive bullish candles
            bullish_count = 0
            for i in range(1, len(recent_df)):
                if recent_df['close'].iloc[-i] > recent_df['open'].iloc[-i]:
                    bullish_count += 1
                else:
                    break
                
                if bullish_count >= consecutive_count:
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use confidence calculation method
                    confidence = self.calculate_signal_confidence(
                        "Consecutive Bullish Candles", "LONG", df, support_levels, resistance_levels, trends
                    )
                    
                    return {
                        "detected": True,
                        "pattern": f"{bullish_count} Consecutive Bullish Candles",
                        "confidence": confidence,
                        "direction": "LONG"
                    }
            
            # Check for consecutive bearish candles
            bearish_count = 0
            for i in range(1, len(recent_df)):
                if recent_df['close'].iloc[-i] < recent_df['open'].iloc[-i]:
                    bearish_count += 1
                else:
                    break
                
                if bearish_count >= consecutive_count:
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use confidence calculation method
                    confidence = self.calculate_signal_confidence(
                        "Consecutive Bearish Candles", "SHORT", df, support_levels, resistance_levels, trends
                    )
                    
                    return {
                        "detected": True,
                        "pattern": f"{bearish_count} Consecutive Bearish Candles",
                        "confidence": confidence,
                        "direction": "SHORT"
                    }
            
            # Check for strong momentum candles (3 large candles in same direction)
            bullish_momentum = True
            bearish_momentum = True
            avg_body_size = sum(abs(recent_df['close'].iloc[-i] - recent_df['open'].iloc[-i]) 
                            for i in range(1, 6)) / 5  # Average of last 5 candles
            
            for i in range(1, consecutive_count + 1):
                body_size = abs(recent_df['close'].iloc[-i] - recent_df['open'].iloc[-i])
                is_bullish = recent_df['close'].iloc[-i] > recent_df['open'].iloc[-i]
                is_bearish = recent_df['close'].iloc[-i] < recent_df['open'].iloc[-i]
                is_large = body_size > 1.5 * avg_body_size
                
                if not (is_bullish and is_large):
                    bullish_momentum = False
                
                if not (is_bearish and is_large):
                    bearish_momentum = False
            
            if bullish_momentum:
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use confidence calculation method
                confidence = self.calculate_signal_confidence(
                    "Strong Bullish Momentum", "LONG", df, support_levels, resistance_levels, trends
                )
                
                return {
                    "detected": True,
                    "pattern": "Strong Bullish Momentum Candles",
                    "confidence": confidence,
                    "direction": "LONG"
                }
                
            if bearish_momentum:
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use confidence calculation method
                confidence = self.calculate_signal_confidence(
                    "Strong Bearish Momentum", "SHORT", df, support_levels, resistance_levels, trends
                )
                
                return {
                    "detected": True,
                    "pattern": "Strong Bearish Momentum Candles",
                    "confidence": confidence,
                    "direction": "SHORT"
                }
            
            return {"detected": False, "pattern": "Consecutive Candles", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in consecutive candles pattern detection: {str(e)}")
            return {"detected": False, "pattern": "Consecutive Candles", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_any_breakout(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                        support_levels: List[float] = None, resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect any breakout pattern
        
        Args:
            df: DataFrame with price data
            all_timeframes: Dictionary with data for different timeframes
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Need at least 20 bars
            if df is None or len(df) < 20:
                return {"detected": False, "pattern": "Pattern Breakout", "confidence": 0, "direction": "NEUTRAL"}
            
            # If support/resistance levels not provided, calculate them
            if support_levels is None or resistance_levels is None:
                s_levels, r_levels = self.identify_support_resistance(all_timeframes)
                if support_levels is None:
                    support_levels = s_levels
                if resistance_levels is None:
                    resistance_levels = r_levels
            
            # Get recent data
            recent_df = df.tail(10).copy()
            current_price = recent_df['close'].iloc[-1]
            previous_price = recent_df['close'].iloc[-2]
            
            # Check for resistance breakout
            for level in resistance_levels:
                # If price just broke above resistance (current price > level but previous price was below)
                if current_price > level * 1.01 and previous_price < level:
                    # Calculate breakout strength
                    breakout_strength = (current_price / level - 1) * 100
                    
                    # Pattern detected, use confidence calculation
                    pattern_name = "Resistance Breakout"
                    direction = "LONG"
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use confidence calculation method
                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    
                    # Adjust confidence based on breakout strength
                    if breakout_strength > 3:  # Strong breakout
                        confidence = min(95, confidence + 10)
                    
                    return {
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    }
            
            # Check for support breakdown
            for level in support_levels:
                # If price just broke below support (current price < level but previous price was above)
                if current_price < level * 0.99 and previous_price > level:
                    # Calculate breakdown strength
                    breakdown_strength = (1 - current_price / level) * 100
                    
                    # Pattern detected, use confidence calculation
                    pattern_name = "Support Breakdown"
                    direction = "SHORT"
                    
                    # Get trend information
                    trends = {}
                    if all_timeframes:
                        for tf in ["1h", "4h", "1d", "1w"]:
                            if tf in all_timeframes:
                                trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                    
                    # Use confidence calculation method
                    confidence = self.calculate_signal_confidence(
                        pattern_name, direction, df, support_levels, resistance_levels, trends
                    )
                    
                    # Adjust confidence based on breakdown strength
                    if breakdown_strength > 3:  # Strong breakdown
                        confidence = min(95, confidence + 10)
                    
                    return {
                        "detected": True,
                        "pattern": pattern_name,
                        "confidence": confidence,
                        "direction": direction
                    }
            
            # Check for other pattern breakouts by calling other pattern functions
            pattern_detectors = [
                self._detect_triangles_and_wedges,
                self._detect_head_and_shoulders_pattern,
                self._detect_inverse_head_and_shoulders_pattern,
                self._detect_double_patterns,
                self._detect_flag_pattern,
                self._detect_pennant
            ]
            
            for detector in pattern_detectors:
                result = detector(df, support_levels, resistance_levels, all_timeframes)
                if result.get("detected", False):
                    return result
            
            return {"detected": False, "pattern": "Pattern Breakout", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in breakout detection: {str(e)}")
            return {"detected": False, "pattern": "Pattern Breakout", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_emerging_pattern(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                            support_levels: List[float] = None, resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect emerging patterns (patterns that are still forming)
        
        Args:
            df: DataFrame with price data
            all_timeframes: Dictionary with data for different timeframes
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Need at least 20 bars
            if df is None or len(df) < 20:
                return {"detected": False, "pattern": "Emerging Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
            # Get current price
            current_price = df['close'].iloc[-1]
            
            # If support/resistance levels not provided, calculate them
            if support_levels is None or resistance_levels is None:
                s_levels, r_levels = self.identify_support_resistance(df)
                if support_levels is None:
                    support_levels = s_levels
                if resistance_levels is None:
                    resistance_levels = r_levels
            
            # Check if price is approaching a key support or resistance level
            approaching_support = False
            approaching_resistance = False
            nearest_support = None
            nearest_resistance = None
            nearest_support_distance = float('inf')
            nearest_resistance_distance = float('inf')
            
            # Find nearest support and resistance
            for level in support_levels:
                if level < current_price:
                    distance = current_price - level
                    if distance < nearest_support_distance:
                        nearest_support_distance = distance
                        nearest_support = level
            
            for level in resistance_levels:
                if level > current_price:
                    distance = level - current_price
                    if distance < nearest_resistance_distance:
                        nearest_resistance_distance = distance
                        nearest_resistance = level
            
            # Calculate distance as percentage
            if nearest_support:
                support_distance_percent = nearest_support_distance / current_price * 100
                approaching_support = support_distance_percent < 3  # Within 3% of support
            
            if nearest_resistance:
                resistance_distance_percent = nearest_resistance_distance / current_price * 100
                approaching_resistance = resistance_distance_percent < 3  # Within 3% of resistance
            
            # Pattern detection based on proximity to key levels
            if approaching_support:
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use confidence calculation method
                pattern_name = "Approaching Support"
                direction = "LONG"
                confidence = self.calculate_signal_confidence(
                    pattern_name, direction, df, support_levels, resistance_levels, trends
                )
                
                return {
                    "detected": True,
                    "pattern": pattern_name,
                    "confidence": confidence,
                    "direction": direction
                }
                
            elif approaching_resistance:
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use confidence calculation method
                pattern_name = "Approaching Resistance"
                direction = "SHORT"
                confidence = self.calculate_signal_confidence(
                    pattern_name, direction, df, support_levels, resistance_levels, trends
                )
                
                return {
                    "detected": True,
                    "pattern": pattern_name,
                    "confidence": confidence,
                    "direction": direction
                }
            
            # Check for other emerging patterns
            # Call pattern detection functions with a stricter threshold
            pattern_detectors = [
                self._detect_channel_up,
                self._detect_channel_down,
                self._detect_flag_pattern,
                self._detect_pennant,
                self._detect_rectangle
            ]
            
            for detector in pattern_detectors:
                result = detector(df, support_levels, resistance_levels, all_timeframes)
                if result.get("detected", False):
                    # It's an emerging pattern if it was detected but hasn't broken out yet
                    return result
            
            return {"detected": False, "pattern": "Emerging Pattern", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in emerging pattern detection: {str(e)}")
            return {"detected": False, "pattern": "Emerging Pattern", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_breakout_with_trend(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                                support_levels: List[float] = None, resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect breakout patterns in the direction of the overall trend
        
        Args:
            df: DataFrame with price data
            all_timeframes: Dictionary with data for different timeframes
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Need at least 50 bars
            if df is None or len(df) < 50:
                return {"detected": False, "pattern": "Trend Breakout", "confidence": 0, "direction": "NEUTRAL"}
            
            # Determine the overall trend
            uptrend = False
            downtrend = False
            
            if 'ema50' in df.columns and 'ema200' in df.columns:
                ema50 = df['ema50'].iloc[-1]
                ema200 = df['ema200'].iloc[-1]
                uptrend = ema50 > ema200 * 1.01
                downtrend = ema50 < ema200 * 0.99
            
            # Get breakout signals
            breakout_result = self._detect_any_breakout(df, all_timeframes, support_levels, resistance_levels)
            
            # Only return breakouts that align with the trend
            if breakout_result.get("detected", False):
                breakout_direction = breakout_result.get("direction", "NEUTRAL")
                
                if (uptrend and breakout_direction == "LONG") or (downtrend and breakout_direction == "SHORT"):
                    # This breakout aligns with the trend - increase confidence
                    breakout_result["pattern"] = f"Trend-Aligned {breakout_result.get('pattern', 'Breakout')}"
                    breakout_result["confidence"] = min(95, breakout_result.get("confidence", 70) + 10)
                    return breakout_result
            
            return {"detected": False, "pattern": "Trend Breakout", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in trend breakout detection: {str(e)}")
            return {"detected": False, "pattern": "Trend Breakout", "confidence": 0, "direction": "NEUTRAL"}

    def _detect_big_movement(self, df: pd.DataFrame, all_timeframes: Dict[str, pd.DataFrame] = None,
                            support_levels: List[float] = None, resistance_levels: List[float] = None) -> Dict[str, Any]:
        """
        Detect big price movements
        
        Args:
            df: DataFrame with price data
            all_timeframes: Dictionary with data for different timeframes
            support_levels: List of support levels
            resistance_levels: List of resistance levels
            
        Returns:
            Dictionary with detection results
        """
        try:
            # Need at least 20 bars
            if df is None or len(df) < 20:
                return {"detected": False, "pattern": "Big Movement", "confidence": 0, "direction": "NEUTRAL"}
            
            # Check recent price movement
            recent_df = df.tail(3).copy()
            
            # Calculate the size of the most recent candle
            latest_open = recent_df['open'].iloc[-1]
            latest_close = recent_df['close'].iloc[-1]
            latest_high = recent_df['high'].iloc[-1]
            latest_low = recent_df['low'].iloc[-1]
            
            # Calculate body and range percentages
            body_size = abs(latest_close - latest_open)
            range_size = latest_high - latest_low
            body_percent = body_size / latest_open * 100
            
            # Calculate average body size for reference
            avg_body_size = sum(abs(df['close'].iloc[-i] - df['open'].iloc[-i]) 
                            for i in range(2, 11)) / 9  # Last 9 candles excluding current
            avg_body_percent = avg_body_size / df['open'].iloc[-10:-1].mean() * 100
            
            # Determine if this is a big movement
            is_big_movement = body_size > avg_body_size * 3 and body_percent > 5
            
            if is_big_movement:
                # Direction of the movement
                direction = "LONG" if latest_close > latest_open else "SHORT"
                
                # Calculate volume increase
                if 'volume' in df.columns:
                    avg_volume = df['volume'].iloc[-11:-1].mean()
                    current_volume = df['volume'].iloc[-1]
                    volume_increase = current_volume / avg_volume if avg_volume > 0 else 1
                else:
                    volume_increase = 1
                
                # Pattern is more significant with higher volume
                has_high_volume = volume_increase > 2
                
                # Get trend information
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)
                
                # Use confidence calculation method
                pattern_name = "Big Price Movement"
                confidence_base = 60
                
                # Adjust confidence based on size and volume
                if body_percent > 10:  # Very large movement
                    confidence_base += 15
                elif body_percent > 7:  # Large movement
                    confidence_base += 10
                
                if has_high_volume:
                    confidence_base += 10
                
                # Use trend alignment to further adjust confidence
                if all_timeframes and "1d" in all_timeframes:
                    day_trend = self.analyze_trend(all_timeframes["1d"], "1d")
                    trend_direction = day_trend.get("trend", "NEUTRAL")
                    
                    if (direction == "LONG" and trend_direction == "BULLISH") or \
                    (direction == "SHORT" and trend_direction == "BEARISH"):
                        confidence_base += 10
                
                confidence = min(90, confidence_base)  # Cap at 90%
                
                return {
                    "detected": True,
                    "pattern": pattern_name,
                    "confidence": confidence,
                    "direction": direction
                }
            
            return {"detected": False, "pattern": "Big Movement", "confidence": 0, "direction": "NEUTRAL"}
            
        except Exception as e:
            print(f"Error in big movement detection: {str(e)}")
            return {"detected": False, "pattern": "Big Movement", "confidence": 0, "direction": "NEUTRAL"}
    
    def _detect_candlestick_pattern(self, df: pd.DataFrame, pattern_name: str,
                                  all_timeframes: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """
        Detect specific candlestick pattern with confirmation from the next candle.
        """
        result = {"detected": False, "pattern": pattern_name, "confidence": 0, "direction": "NEUTRAL"}
        
        try:
            # Need at least 6 candles for pattern and confirmation
            if len(df) < 6:
                return result
            
            # The pattern is identified on `prev`, confirmed by `current`
            current = df.iloc[-1]
            prev = df.iloc[-2]
            prev2 = df.iloc[-3]
            
            pattern_lower = pattern_name.lower()
            
            # Calculate recent average body size for confirmation check
            avg_body_size = (abs(df['close'].iloc[-10:-2] - df['open'].iloc[-10:-2])).mean()

            # --- Bullish Patterns ---
            if "hammer" in pattern_lower:
                body_size = abs(prev['close'] - prev['open'])
                total_size = prev['high'] - prev['low']
                lower_shadow = min(prev['open'], prev['close']) - prev['low']
                
                is_hammer = (total_size > 0 and body_size / total_size < 0.3 and lower_shadow / total_size > 0.6)
                in_downtrend = prev2['close'] < prev2['open']

                if is_hammer and in_downtrend:
                    # Confirmation Logic
                    is_bullish_candle = current['close'] > current['open']
                    closes_above_high = current['close'] > prev['high']
                    confirmation_body = current['close'] - current['open']
                    has_strong_body = confirmation_body > avg_body_size * 0.7
                    
                    if is_bullish_candle and closes_above_high and has_strong_body:
                        return {"detected": True, "pattern": "Confirmed Hammer", "confidence": 75, "direction": "LONG"}

            elif "engulfing" in pattern_lower:
                # Bullish Engulfing on `prev` and `prev2`
                is_bullish_engulfing = (prev['open'] < prev['close'] and prev2['open'] > prev2['close'] and
                                        prev['open'] <= prev2['close'] and prev['close'] > prev2['open'])
                
                if is_bullish_engulfing:
                    # Confirmation Logic
                    is_bullish_candle = current['close'] > current['open']
                    closes_above_high = current['close'] > prev['high']
                    confirmation_body = current['close'] - current['open']
                    has_strong_body = confirmation_body > avg_body_size * 0.7

                    if is_bullish_candle and closes_above_high and has_strong_body:
                        return {"detected": True, "pattern": "Confirmed Bullish Engulfing", "confidence": 80, "direction": "LONG"}

            # --- Bearish Patterns ---
            elif "shooting star" in pattern_lower:
                body_size = abs(prev['close'] - prev['open'])
                total_size = prev['high'] - prev['low']
                upper_shadow = prev['high'] - max(prev['open'], prev['close'])
                
                is_shooting_star = (total_size > 0 and body_size / total_size < 0.3 and upper_shadow / total_size > 0.6)
                in_uptrend = prev2['close'] > prev2['open']

                if is_shooting_star and in_uptrend:
                    # Confirmation Logic
                    is_bearish_candle = current['close'] < current['open']
                    closes_below_low = current['close'] < prev['low']
                    confirmation_body = current['open'] - current['close']
                    has_strong_body = confirmation_body > avg_body_size * 0.7

                    if is_bearish_candle and closes_below_low and has_strong_body:
                        return {"detected": True, "pattern": "Confirmed Shooting Star", "confidence": 75, "direction": "SHORT"}

            elif "engulfing" in pattern_lower: # Re-check for bearish
                is_bearish_engulfing = (prev['open'] > prev['close'] and prev2['open'] < prev2['close'] and
                                        prev['open'] >= prev2['close'] and prev['close'] < prev2['open'])

                if is_bearish_engulfing:
                    # Confirmation Logic
                    is_bearish_candle = current['close'] < current['open']
                    closes_below_low = current['close'] < prev['low']
                    confirmation_body = current['open'] - current['close']
                    has_strong_body = confirmation_body > avg_body_size * 0.7

                    if is_bearish_candle and closes_below_low and has_strong_body:
                        return {"detected": True, "pattern": "Confirmed Bearish Engulfing", "confidence": 80, "direction": "SHORT"}

            return result
            
        except Exception as e:
            print(f"Error in candlestick pattern detection: {str(e)}")
            return result

    def _detect_trend_momentum(self, df: pd.DataFrame, pattern_name: str,
                             all_timeframes: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """
        Detect trend and momentum indicators
        
        Args:
            df: DataFrame with price and indicator data
            pattern_name: Name of the pattern to detect
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        result = {
            "detected": False,
            "pattern": pattern_name,
            "confidence": 0,
            "direction": "NEUTRAL"
        }
        
        try:
            # Lowercase pattern name for easier comparison
            pattern_lower = pattern_name.lower()
            
            # MA Crossover detection
            if "crossover" in pattern_lower:
                # Extract which MAs to check
                ma_types = []
                if "sma" in pattern_lower:
                    ma_types.append("sma")
                elif "ema" in pattern_lower:
                    ma_types.append("ema")
                else:
                    ma_types = ["sma", "ema"]  # Check both types if not specified
                
                # Try to extract MA periods
                ma_periods = []
                for p in [5, 10, 20, 30, 50, 100, 200]:
                    if str(p) in pattern_lower:
                        ma_periods.append(p)
                
                # If periods not found, use default pairs
                if len(ma_periods) < 2:
                    if "ema" in pattern_lower:
                        ma_periods = [9, 21] if "9" in pattern_lower and "21" in pattern_lower else [12, 26]
                    else:
                        ma_periods = [50, 200] if "50" in pattern_lower and "200" in pattern_lower else [20, 50]
                
                # Check for crossovers
                for ma_type in ma_types:
                    # Need at least 2 periods to form a pair
                    if len(ma_periods) >= 2:
                        fast_period = min(ma_periods)
                        slow_period = max(ma_periods)
                        
                        fast_col = f"{ma_type}{fast_period}"
                        slow_col = f"{ma_type}{slow_period}"
                        
                        # Check if these columns exist
                        if fast_col in df.columns and slow_col in df.columns:
                            fast_current = df[fast_col].iloc[-1]
                            fast_prev = df[fast_col].iloc[-2]
                            slow_current = df[slow_col].iloc[-1]
                            slow_prev = df[slow_col].iloc[-2]
                            
                            # Bullish crossover (fast crosses above slow)
                            if fast_prev <= slow_prev and fast_current > slow_current:
                                confidence = 70
                                
                                # Golden cross gets higher confidence
                                if (ma_type == "sma" or ma_type == "ema") and fast_period == 50 and slow_period == 200:
                                    confidence = 80
                                
                                return {
                                    "detected": True,
                                    "pattern": f"{ma_type.upper()} {fast_period}/{slow_period} Bullish Crossover",
                                    "confidence": confidence,
                                    "direction": "LONG"
                                }
                            
                            # Bearish crossover (fast crosses below slow)
                            elif fast_prev >= slow_prev and fast_current < slow_current:
                                confidence = 70
                                
                                # Death cross gets higher confidence
                                if (ma_type == "sma" or ma_type == "ema") and fast_period == 50 and slow_period == 200:
                                    confidence = 80
                                
                                return {
                                    "detected": True,
                                    "pattern": f"{ma_type.upper()} {fast_period}/{slow_period} Bearish Crossover",
                                    "confidence": confidence,
                                    "direction": "SHORT"
                                }
            
            # Price breaking Bollinger Bands
            elif "bollinger" in pattern_lower:
                if 'bb_upper' in df.columns and 'bb_lower' in df.columns:
                    current_close = df['close'].iloc[-1]
                    bb_upper = df['bb_upper'].iloc[-1]
                    bb_lower = df['bb_lower'].iloc[-1]
                    
                    # Price breaks above upper band
                    if "broke upper" in pattern_lower and current_close > bb_upper * 1.01:
                        return {
                            "detected": True,
                            "pattern": "Price Broke Above Bollinger Upper Band",
                            "confidence": 70,
                            "direction": "LONG"  # Short-term momentum continuation
                        }
                    
                    # Price breaks below lower band
                    elif "broke lower" in pattern_lower and current_close < bb_lower * 0.99:
                        return {
                            "detected": True,
                            "pattern": "Price Broke Below Bollinger Lower Band",
                            "confidence": 70,
                            "direction": "SHORT"  # Short-term momentum continuation
                        }
            
            # Volume spike
            elif "volume" in pattern_lower and ("spike" in pattern_lower or "unusual" in pattern_lower):
                if 'volume' in df.columns:
                    # Calculate volume ratio (current volume vs 20-day average)
                    avg_volume = df['volume'].iloc[-21:-1].mean()
                    current_volume = df['volume'].iloc[-1]
                    
                    if avg_volume > 0 and current_volume > avg_volume * 2:  # Volume at least 2x average
                        # Determine direction based on price action
                        is_up_day = df['close'].iloc[-1] > df['open'].iloc[-1]
                        
                        return {
                            "detected": True,
                            "pattern": "Volume Spike",
                            "confidence": 65,
                            "direction": "LONG" if is_up_day else "SHORT"
                        }
            
            # User specifically requested this pattern, force detection with lower confidence
            return {
                "detected": True,
                "pattern": pattern_name,
                "confidence": 60,
                "direction": "LONG" if any(term in pattern_lower for term in ["golden", "bullish", "above"]) 
                           else "SHORT" if any(term in pattern_lower for term in ["death", "bearish", "below"]) 
                           else "NEUTRAL"
            }
            
        except Exception as e:
            print(f"Error in trend/momentum detection: {str(e)}")
            return result
    
    def _detect_oscillator_signal(self, df: pd.DataFrame, pattern_name: str,
                                all_timeframes: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """
        Detect signals from oscillator indicators
        
        Args:
            df: DataFrame with price and indicator data
            pattern_name: Name of the pattern to detect
            all_timeframes: Dictionary with data for different timeframes
            
        Returns:
            Dictionary with detection results
        """
        result = {
            "detected": False,
            "pattern": pattern_name,
            "confidence": 0,
            "direction": "NEUTRAL"
        }
        
        try:
            # Lowercase pattern name for easier comparison
            pattern_lower = pattern_name.lower()
            
            # RSI signals
            if "rsi" in pattern_lower:
                # Determine which RSI to use
                rsi_period = 14  # Default
                if "9" in pattern_lower:
                    rsi_period = 9
                elif "25" in pattern_lower:
                    rsi_period = 25
                
                rsi_col = f"rsi{rsi_period}"
                
                # Check if RSI column exists
                if rsi_col in df.columns:
                    current_rsi = df[rsi_col].iloc[-1]
                    prev_rsi = df[rsi_col].iloc[-2]
                    
                    # Oversold condition (RSI below 30)
                    if current_rsi < 30 and current_rsi > prev_rsi:  # RSI turning up from oversold
                        # Calculate confidence based on oversold level
                        confidence = 70 + (30 - current_rsi) * 2
                        
                        return {
                            "detected": True,
                            "pattern": f"RSI({rsi_period}) Oversold",
                            "confidence": min(95, confidence),
                            "direction": "LONG"
                        }
                    
                    # Overbought condition (RSI above 70)
                    elif current_rsi > 70 and current_rsi < prev_rsi:  # RSI turning down from overbought
                        # Calculate confidence based on overbought level
                        confidence = 70 + (current_rsi - 70) * 2
                        
                        return {
                            "detected": True,
                            "pattern": f"RSI({rsi_period}) Overbought",
                            "confidence": min(95, confidence),
                            "direction": "SHORT"
                        }
            
            # Stochastic signals
            elif "stoch" in pattern_lower:
                if 'stoch_k' in df.columns and 'stoch_d' in df.columns:
                    current_k = df['stoch_k'].iloc[-1]
                    current_d = df['stoch_d'].iloc[-1]
                    prev_k = df['stoch_k'].iloc[-2]
                    prev_d = df['stoch_d'].iloc[-2]
                    
                    # Bullish crossover (K crosses above D) in oversold region
                    if prev_k <= prev_d and current_k > current_d and current_k < 30:
                        confidence = 70 + (30 - current_k)
                        
                        return {
                            "detected": True,
                            "pattern": "Stochastic Oversold Bullish Crossover",
                            "confidence": min(95, confidence),
                            "direction": "LONG"
                        }
                    
                    # Bearish crossover (K crosses below D) in overbought region
                    elif prev_k >= prev_d and current_k < current_d and current_k > 70:
                        confidence = 70 + (current_k - 70)
                        
                        return {
                            "detected": True,
                            "pattern": "Stochastic Overbought Bearish Crossover",
                            "confidence": min(95, confidence),
                            "direction": "SHORT"
                        }
            
            # MACD signals
            elif "macd" in pattern_lower:
                if 'macd' in df.columns and 'macd_signal' in df.columns:
                    current_macd = df['macd'].iloc[-1]
                    current_signal = df['macd_signal'].iloc[-1]
                    prev_macd = df['macd'].iloc[-2]
                    prev_signal = df['macd_signal'].iloc[-2]
                    
                    # Bullish crossover (MACD crosses above signal)
                    if prev_macd <= prev_signal and current_macd > current_signal:
                        # Calculate confidence based on position
                        if current_macd < 0:
                            # Crossing while negative is early bullish signal
                            confidence = 75
                        else:
                            # Crossing while positive is continuation signal
                            confidence = 65
                        
                        return {
                            "detected": True,
                            "pattern": "MACD Bullish Crossover",
                            "confidence": confidence,
                            "direction": "LONG"
                        }
                    
                    # Bearish crossover (MACD crosses below signal)
                    elif prev_macd >= prev_signal and current_macd < current_signal:
                        # Calculate confidence based on position
                        if current_macd > 0:
                            # Crossing while positive is early bearish signal
                            confidence = 75
                        else:
                            # Crossing while negative is continuation signal
                            confidence = 65
                        
                        return {
                            "detected": True,
                            "pattern": "MACD Bearish Crossover",
                            "confidence": confidence,
                            "direction": "SHORT"
                        }
            
            # Williams %R signals
            elif "williams" in pattern_lower:
                if 'williams_r' in df.columns:
                    current_wr = df['williams_r'].iloc[-1]
                    prev_wr = df['williams_r'].iloc[-2]
                    
                    # Oversold condition (Williams %R below -80)
                    if current_wr < -80 and current_wr > prev_wr:  # Turning up from oversold
                        # Calculate confidence based on oversold level
                        confidence = 70 + abs(current_wr + 80)
                        
                        return {
                            "detected": True,
                            "pattern": "Williams %R Oversold",
                            "confidence": min(95, confidence),
                            "direction": "LONG"
                        }
                    
                    # Overbought condition (Williams %R above -20)
                    elif current_wr > -20 and current_wr < prev_wr:  # Turning down from overbought
                        # Calculate confidence based on overbought level
                        confidence = 70 + abs(current_wr + 20)
                        
                        return {
                            "detected": True,
                            "pattern": "Williams %R Overbought",
                            "confidence": min(95, confidence),
                            "direction": "SHORT"
                        }
            
            # User specifically requested this oscillator, force detection with lower confidence
            # Determine direction based on general pattern name
            direction = "LONG" if any(term in pattern_lower for term in ["oversold", "bullish", "positive"]) \
                      else "SHORT" if any(term in pattern_lower for term in ["overbought", "bearish", "negative"]) \
                      else "NEUTRAL"
            
            return {
                "detected": True,
                "pattern": pattern_name,
                "confidence": 60,
                "direction": direction
            }
            
        except Exception as e:
            print(f"Error in oscillator signal detection: {str(e)}")
            return result
    
    def _detect_divergence_signal(self, df: pd.DataFrame, pattern_name: str,
                                    all_timeframes: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """
        Detects both Regular (reversal) and Hidden (continuation) divergences
        for RSI and MACD. It distinguishes between an "Emerging" divergence and a
        "Confirmed" divergence, which requires a market structure break for a higher
        confidence signal.
        """
        result = {"detected": False, "pattern": "Divergence", "confidence": 0, "direction": "NEUTRAL"}
        if len(df) < 50: return result

        try:
            recent_df = df.tail(60).copy()
            if 'rsi14' not in recent_df.columns or 'macd' not in recent_df.columns:
                return result

            # Use ATR for dynamic prominence in peak detection for robustness
            prominence = recent_df['atr'].iloc[-1] * 0.5 if 'atr' in recent_df.columns and recent_df['atr'].iloc[-1] > 0 else recent_df['close'].std() * 0.5
            if prominence == 0: prominence = recent_df['close'].mean() * 0.01

            high_indices, _ = find_peaks(recent_df['high'], prominence=prominence, width=3)
            low_indices, _ = find_peaks(-recent_df['low'], prominence=prominence, width=3)

            # --- Bearish Divergence Checks ---
            if len(high_indices) >= 2:
                p1_idx, p2_idx = high_indices[-2], high_indices[-1]
                p1_price, p2_price = recent_df['high'].iloc[p1_idx], recent_df['high'].iloc[p2_idx]
                p1_rsi, p2_rsi = recent_df['rsi14'].iloc[p1_idx], recent_df['rsi14'].iloc[p2_idx]
                p1_macd, p2_macd = recent_df['macd'].iloc[p1_idx], recent_df['macd'].iloc[p2_idx]
                
                # 1. Regular Bearish Divergence (Higher High in Price, Lower High in Oscillator) -> Reversal
                if p2_price > p1_price:
                    is_divergence = False
                    pattern = "Unknown"
                    if p2_rsi < p1_rsi:
                        is_divergence, pattern = True, "Bearish RSI Divergence"
                    elif p2_macd < p1_macd:
                        is_divergence, pattern = True, "Bearish MACD Divergence"
                    
                    if is_divergence:
                        confirmation_low_section = recent_df['low'].iloc[p1_idx:p2_idx]
                        if not confirmation_low_section.empty:
                            confirmation_level = confirmation_low_section.min()
                            # If market structure is broken, it's a "Confirmed" signal with high confidence
                            if recent_df['close'].iloc[-1] < confirmation_level:
                                result = {"detected": True, "pattern": f"Confirmed {pattern}", "confidence": 78, "direction": "SHORT"}
                            # Otherwise, it's an "Emerging" signal with lower confidence
                            else:
                                result = {"detected": True, "pattern": f"Emerging {pattern}", "confidence": 62, "direction": "SHORT"}
                
                # 2. Hidden Bearish Divergence (Lower High in Price, Higher High in Oscillator) -> Continuation
                elif p2_price < p1_price:
                    is_hidden_divergence = False
                    pattern = "Unknown"
                    if p2_rsi > p1_rsi:
                        is_hidden_divergence, pattern = True, "Hidden Bearish RSI Divergence"
                    elif p2_macd > p1_macd:
                        is_hidden_divergence, pattern = True, "Hidden Bearish MACD Divergence"
                        
                    if is_hidden_divergence:
                        # Confirmation for continuation is breaking the most recent swing low
                        recent_low_after_p2 = recent_df['low'].iloc[p2_idx:].min()
                        if recent_df['close'].iloc[-1] < recent_low_after_p2:
                            result = {"detected": True, "pattern": f"Confirmed {pattern}", "confidence": 75, "direction": "SHORT"}
                        else:
                            result = {"detected": True, "pattern": f"Emerging {pattern}", "confidence": 60, "direction": "SHORT"}

            # --- Bullish Divergence Checks ---
            if len(low_indices) >= 2:
                v1_idx, v2_idx = low_indices[-2], low_indices[-1]
                v1_price, v2_price = recent_df['low'].iloc[v1_idx], recent_df['low'].iloc[v2_idx]
                v1_rsi, v2_rsi = recent_df['rsi14'].iloc[v1_idx], recent_df['rsi14'].iloc[v2_idx]
                v1_macd, v2_macd = recent_df['macd'].iloc[v1_idx], recent_df['macd'].iloc[v2_idx]
                
                # 1. Regular Bullish Divergence (Lower Low in Price, Higher Low in Oscillator) -> Reversal
                if v2_price < v1_price:
                    is_divergence = False
                    pattern = "Unknown"
                    if v2_rsi > v1_rsi:
                        is_divergence, pattern = True, "Bullish RSI Divergence"
                    elif v2_macd > v1_macd:
                        is_divergence, pattern = True, "Bullish MACD Divergence"
                        
                    if is_divergence:
                        confirmation_high_section = recent_df['high'].iloc[v1_idx:v2_idx]
                        if not confirmation_high_section.empty:
                            confirmation_level = confirmation_high_section.max()
                            if recent_df['close'].iloc[-1] > confirmation_level:
                                result = {"detected": True, "pattern": f"Confirmed {pattern}", "confidence": 78, "direction": "LONG"}
                            else:
                                result = {"detected": True, "pattern": f"Emerging {pattern}", "confidence": 62, "direction": "LONG"}
                
                # 2. Hidden Bullish Divergence (Higher Low in Price, Lower Low in Oscillator) -> Continuation
                elif v2_price > v1_price:
                    is_hidden_divergence = False
                    pattern = "Unknown"
                    if v2_rsi < v1_rsi:
                        is_hidden_divergence, pattern = True, "Hidden Bullish RSI Divergence"
                    elif v2_macd < v1_macd:
                        is_hidden_divergence, pattern = True, "Hidden Bullish MACD Divergence"
                        
                    if is_hidden_divergence:
                        recent_high_after_v2 = recent_df['high'].iloc[v2_idx:].max()
                        if recent_df['close'].iloc[-1] > recent_high_after_v2:
                            result = {"detected": True, "pattern": f"Confirmed {pattern}", "confidence": 75, "direction": "LONG"}
                        else:
                            result = {"detected": True, "pattern": f"Emerging {pattern}", "confidence": 60, "direction": "LONG"}
                            
            # Final confidence calculation using your comprehensive scoring model
            if result['detected']:
                trends = {tf: self.analyze_trend(all_timeframes[tf], tf) for tf in all_timeframes if tf in all_timeframes}
                result['confidence'] = self.calculate_signal_confidence(
                    result['pattern'], result['direction'], df, [], [], trends,
                    base_confidence_override=result['confidence']
                )

            return result

        except Exception as e:
            print(f"Error in _detect_divergence_signal: {e}")
            traceback.print_exc()
            return {"detected": False, "pattern": pattern_name, "confidence": 0, "direction": "NEUTRAL"}
        
        
    def analyze_trend(self, df: pd.DataFrame, timeframe: str) -> Dict[str, Any]:
        """
        Analyze price trend for a specific timeframe
        
        Args:
            df: DataFrame with price data
            timeframe: The timeframe being analyzed (e.g., "1h", "4h", "1d")
            
        Returns:
            Dictionary with trend information
        """
        if df is None or len(df) < 30:
            return {"trend": "NEUTRAL", "strength": 0, "description": "Insufficient data"}
        
        try:
            # Use different indicators to determine trend
            
            # Moving average trend
            ma_trend = "NEUTRAL"
            ma_strength = 0
            
            # Check if required MAs are available
            ma_checks = []
            
            if all(col in df.columns for col in ['ema9', 'ema50']):
                ema9 = df['ema9'].iloc[-1]
                ema50 = df['ema50'].iloc[-1]
                ma_checks.append(("Short-term", ema9 > ema50, ema9 / ema50 - 1))
            
            if all(col in df.columns for col in ['ema50', 'ema200']):
                ema50 = df['ema50'].iloc[-1]
                ema200 = df['ema200'].iloc[-1]
                ma_checks.append(("Long-term", ema50 > ema200, ema50 / ema200 - 1))
            
            if all(col in df.columns for col in ['sma20', 'sma50']):
                sma20 = df['sma20'].iloc[-1]
                sma50 = df['sma50'].iloc[-1]
                ma_checks.append(("Medium-term", sma20 > sma50, sma20 / sma50 - 1))
            
            # Determine trend based on MA alignments
            if ma_checks:
                bullish_count = sum(1 for _, is_bullish, _ in ma_checks if is_bullish)
                bearish_count = len(ma_checks) - bullish_count
                
                if bullish_count > bearish_count:
                    ma_trend = "BULLISH"
                    ma_strength = sum(strength for _, is_bullish, strength in ma_checks if is_bullish) * 100
                elif bearish_count > bullish_count:
                    ma_trend = "BEARISH"
                    ma_strength = sum(strength for _, is_bullish, strength in ma_checks if not is_bullish) * 100
                else:
                    ma_trend = "NEUTRAL"
                    ma_strength = 0
            
            # Check price action
            price_action_trend = "NEUTRAL"
            price_action_strength = 0
            
            # Calculate recent price changes
            if len(df) >= 10:
                current_price = df['close'].iloc[-1]
                price_5_periods_ago = df['close'].iloc[-6] if len(df) >= 6 else df['close'].iloc[0]
                price_10_periods_ago = df['close'].iloc[-11] if len(df) >= 11 else df['close'].iloc[0]
                
                change_5_periods = (current_price / price_5_periods_ago - 1) * 100
                change_10_periods = (current_price / price_10_periods_ago - 1) * 100
                
                if change_5_periods > 2 and change_10_periods > 5:
                    price_action_trend = "BULLISH"
                    price_action_strength = min(100, max(change_5_periods * 2, change_10_periods))
                elif change_5_periods < -2 and change_10_periods < -5:
                    price_action_trend = "BEARISH"
                    price_action_strength = min(100, max(-change_5_periods * 2, -change_10_periods))
                else:
                    price_action_trend = "NEUTRAL"
                    price_action_strength = min(50, abs(change_5_periods) + abs(change_10_periods))
            
            # Check indicators for additional confirmation
            indicator_trend = "NEUTRAL"
            indicator_strength = 0
            
            if 'rsi14' in df.columns:
                rsi = df['rsi14'].iloc[-1]
                if rsi > 60:
                    indicator_trend = "BULLISH"
                    indicator_strength = min(100, (rsi - 50) * 2)
                elif rsi < 40:
                    indicator_trend = "BEARISH"
                    indicator_strength = min(100, (50 - rsi) * 2)
                else:
                    indicator_trend = "NEUTRAL"
                    indicator_strength = 0
            
            # Calculate a weighted trend
            trends = []
            
            if ma_trend != "NEUTRAL":
                trends.append((ma_trend, ma_strength, 0.5))  # 50% weight to MA trend
            
            if price_action_trend != "NEUTRAL":
                trends.append((price_action_trend, price_action_strength, 0.3))  # 30% weight to price action
            
            if indicator_trend != "NEUTRAL":
                trends.append((indicator_trend, indicator_strength, 0.2))  # 20% weight to indicators
            
            if not trends:
                final_trend = "NEUTRAL"
                final_strength = 0
                description = "No clear trend indicators"
            else:
                # Calculate weighted trend
                bullish_weight = sum(strength * weight for trend, strength, weight in trends if trend == "BULLISH")
                bearish_weight = sum(strength * weight for trend, strength, weight in trends if trend == "BEARISH")
                
                if bullish_weight > bearish_weight:
                    final_trend = "BULLISH"
                    final_strength = bullish_weight * 100 / sum(weight for _, _, weight in trends)
                    
                    # Create description
                    if final_strength > 80:
                        description = "Strong bullish trend across multiple indicators"
                    elif final_strength > 60:
                        description = "Moderate bullish trend"
                    else:
                        description = "Weak bullish bias"
                        
                elif bearish_weight > bullish_weight:
                    final_trend = "BEARISH"
                    final_strength = bearish_weight * 100 / sum(weight for _, _, weight in trends)
                    
                    # Create description
                    if final_strength > 80:
                        description = "Strong bearish trend across multiple indicators"
                    elif final_strength > 60:
                        description = "Moderate bearish trend"
                    else:
                        description = "Weak bearish bias"
                        
                else:
                    final_trend = "NEUTRAL"
                    final_strength = 50
                    description = "Mixed signals, no clear trend direction"
            
            # ENHANCED Dow Theory Volume Confirmation - Professional Implementation
            dow_volume_analysis = self._enhanced_dow_volume_confirmation(df, final_trend, timeframe)
            
            # Apply sophisticated volume confirmation adjustments
            if dow_volume_analysis['confirmation_strength'] > 0:
                volume_multiplier = 1 + (dow_volume_analysis['confirmation_strength'] / 100)
                final_strength *= volume_multiplier
                if dow_volume_analysis['volume_quality'] == 'Excellent':
                    description += " (Strong volume confirmation)"
                elif dow_volume_analysis['volume_quality'] == 'Good':
                    description += " (Good volume confirmation)"
            elif dow_volume_analysis['confirmation_strength'] < 0:
                volume_penalty = 1 + (dow_volume_analysis['confirmation_strength'] / 100)
                final_strength *= volume_penalty
                description += f" (Volume {dow_volume_analysis['volume_issue']})"
            
            # Add Dow Theory trend structure information
            if dow_volume_analysis.get('trend_structure_confirmed', False):
                description += " [Trend structure confirmed]"
            
            # Cap final strength at reasonable bounds
            final_strength = max(0, min(100, final_strength))

            # Add time reference to description
            time_reference = {
                "1m": "1-minute",
                "5m": "5-minute",
                "15m": "15-minute",
                "30m": "30-minute",
                "1h": "1-hour",
                "4h": "4-hour", 
                "12h": "12-hour",
                "1d": "daily",
                "1w": "weekly"
            }
            
            timeframe_desc = time_reference.get(timeframe, timeframe)
            description = f"{timeframe_desc} timeframe: {description}"
            
            return {
                "trend": final_trend,
                "strength": final_strength,
                "description": description
            }
            
        except Exception as e:
            print(f"Error analyzing trend: {str(e)}")
            return {"trend": "NEUTRAL", "strength": 0, "description": f"Error: {str(e)}"}
    
    def _calculate_fibonacci_extensions(self, df: pd.DataFrame, direction: str) -> List[float]:
        """
        Calculate Fibonacci extension levels based on the most recent significant trend.
        """
        if len(df) < 30:
            return []

        # Find the most recent major swing high and low to define the primary trend impulse
        recent_df = df.tail(100)
        swing_high_price = recent_df['high'].max()
        swing_low_price = recent_df['low'].min()

        # Determine the end of the retracement to project from
        if direction == "LONG":
            # Primary move is up, so we're looking for extensions from a retracement low
            retracement_end_price = recent_df['low'].iloc[-30:].min()
            price_range = swing_high_price - retracement_end_price
            base_price = swing_high_price
            extension_levels = [base_price + price_range * ext for ext in [0.236, 0.382, 0.5, 0.618, 0.786, 1.0, 1.272, 1.618]]
        else: # SHORT
            # Primary move is down, looking for extensions from a retracement high
            retracement_end_price = recent_df['high'].iloc[-30:].max()
            price_range = retracement_end_price - swing_low_price
            base_price = swing_low_price
            extension_levels = [base_price - price_range * ext for ext in [0.236, 0.382, 0.5, 0.618, 0.786, 1.0, 1.272, 1.618]]

        return [level for level in extension_levels if level > 0]

    def _calculate_measured_move_targets(self, extreme_price: float, neckline_price: float, breakout_price: float, direction: str) -> List[float]:
        """
        Calculate measured move targets for Double Top/Bottom patterns.
        """
        height = abs(extreme_price - neckline_price)
        if direction == "LONG":
            return [
                breakout_price + (height * 0.5),
                breakout_price + height
            ]
        else: # SHORT
            return [
                breakout_price - (height * 0.5),
                breakout_price - height
            ]

    def calculate_targets_and_stop(self, df, direction, support_levels_with_strength, resistance_levels_with_strength, pattern, pattern_details=None):
        """
        Overhauled target and stop loss calculation with dynamic volatility-based risk management
        and prioritization of strong S/R levels.
        
        This function implements professional-grade risk management:
        1. Dynamic stop-loss based on ATR and structural anchors
        2. Multi-source target generation with clustering
        3. Risk/Reward optimization
        """
        if df is None or len(df) < 10:
            return [], 0

        try:
            current_price = df['close'].iloc[-1]
            
            # Get ATR for volatility-based calculations
            if 'atr' not in df.columns:
                # Calculate ATR manually if not present
                high_low = df['high'] - df['low']
                high_close = abs(df['high'] - df['close'].shift())
                low_close = abs(df['low'] - df['close'].shift())
                true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                atr = true_range.iloc[-1]
            else:
                atr = df['atr'].iloc[-1]
            
            stop_loss = 0
            
            # --- 1. Dynamic Stop-Loss Calculation ---
            if direction == "LONG":
                # Find structural anchor (recent swing low or strong support)
                structural_anchor = self._find_structural_swing_low(df, current_price)
                
                if structural_anchor:
                    # Calculate stop-loss as: structural_anchor - (ATR * multiplier)
                    stop_loss = structural_anchor - (atr * 1.5)
                else:
                    # Fallback to closest high-strength support level (strength_score >= 5)
                    strong_supports = [lvl for lvl, strg in support_levels_with_strength if strg >= 5 and lvl < current_price]
                    if strong_supports:
                        closest_support = max(strong_supports)
                        stop_loss = closest_support - (atr * 1.5)
                    else:
                        # Ultimate fallback
                        stop_loss = current_price - (atr * 2.5)

            elif direction == "SHORT":
                # Find structural anchor (recent swing high or strong resistance)
                structural_anchor = self._find_structural_swing_high(df, current_price)
                
                if structural_anchor:
                    # Calculate stop-loss as: structural_anchor + (ATR * multiplier)
                    stop_loss = structural_anchor + (atr * 1.5)
                else:
                    # Fallback to closest high-strength resistance level (strength_score >= 5)
                    strong_resistances = [lvl for lvl, strg in resistance_levels_with_strength if strg >= 5 and lvl > current_price]
                    if strong_resistances:
                        closest_resistance = min(strong_resistances)
                        stop_loss = closest_resistance + (atr * 1.5)
                    else:
                        # Ultimate fallback
                        stop_loss = current_price + (atr * 2.5)

            # --- 2. Risk Validation ---
            risk = abs(current_price - stop_loss)
            if risk == 0:
                return [], stop_loss  # Avoid division by zero
            
            # Ensure risk is within reasonable range (1% to 8% of entry price)
            min_risk = current_price * 0.01
            max_risk = current_price * 0.08
            
            if risk < min_risk:
                # Adjust stop to meet minimum risk
                if direction == "LONG":
                    stop_loss = current_price - min_risk
                else:
                    stop_loss = current_price + min_risk
                risk = min_risk
            elif risk > max_risk:
                # Adjust stop to meet maximum risk
                if direction == "LONG":
                    stop_loss = current_price - max_risk
                else:
                    stop_loss = current_price + max_risk
                risk = max_risk

            # --- 3. Gather Potential Targets from Hierarchy ---
            potential_targets = []
            
            # Source 1: R/R Multiples
            for r_multiple in [1.5, 2.0, 3.0, 5.0]:
                price = current_price + (risk * r_multiple) if direction == "LONG" else current_price - (risk * r_multiple)
                potential_targets.append({
                    'price': price, 
                    'source': 'risk_multiple',
                    'reason': f'{r_multiple}R Target',
                    'probability': 70 - (r_multiple * 5)  # Decreasing probability with higher multiples
                })

            # Source 2: Structural Levels
            levels_to_check = resistance_levels_with_strength if direction == "LONG" else support_levels_with_strength
            for level, strength in levels_to_check:
                if strength >= 4:  # Only high-strength levels
                    potential_targets.append({
                        'price': level, 
                        'source': 'structure',
                        'reason': f'Structural Level (Strength: {strength})',
                        'probability': min(90, 50 + strength * 5)
                    })
            
            # Source 3: Measured Moves (If Applicable)
            if pattern_details and "head" in pattern_details:
                head_price = pattern_details['head']['price']
                neckline_price = pattern_details['neckline_break']
                breakout_price = current_price  # USE CURRENT PRICE FOR PROJECTION

                measured_move_targets = self._calculate_measured_move_targets(head_price, neckline_price, breakout_price, direction)
                
                for i, price in enumerate(measured_move_targets):
                    reason = f"H&S {['50%', '100%', '161.8%'][min(i, 2)]} Measured Move"
                    potential_targets.append({
                        'price': price, 
                        'source': 'measured_move',
                        'reason': reason,
                        'probability': 80 - i*10
                    })
            elif pattern_details and "extreme_price" in pattern_details:
                measured_move_targets = self._calculate_measured_move_targets(
                    pattern_details['extreme_price'], pattern_details['neckline_price'],
                    current_price, direction
                )
                for i, price in enumerate(measured_move_targets):
                    reason = f"Double Top/Bottom {['50%', '100%'][min(i, 1)]} Measured Move"
                    potential_targets.append({
                        'price': price, 
                        'source': 'measured_move',
                        'reason': reason,
                        'probability': 75 - i * 10
                    })

            # Source 4: Fibonacci Extensions
            fib_levels = self._calculate_fibonacci_extensions(df, direction)
            fib_ratios = [1.272, 1.618, 2.0, 2.618]  # Key Fibonacci ratios
            for i, lvl in enumerate(fib_levels[:len(fib_ratios)]):  # Limit to key levels
                if i < len(fib_ratios):
                    potential_targets.append({
                        'price': lvl, 
                        'source': 'fibonacci',
                        'reason': f'{fib_ratios[i]} Fib Extension',
                        'probability': 65 - i*5
                    })

            # --- 4. Cluster and Finalize Targets ---
            if not potential_targets:
                return [], stop_loss

            # Cluster targets that are close to each other
            final_targets = self._cluster_targets(potential_targets, tolerance=0.005)  # 0.5% tolerance
            
            # Filter targets in the direction of trade
            valid_targets = [t for t in final_targets if 
                           (direction == "LONG" and t['price'] > current_price) or 
                           (direction == "SHORT" and t['price'] < current_price)]
            
            # Sort by price (ascending for shorts, descending for longs)
            valid_targets.sort(key=lambda x: x['price'], reverse=(direction == "SHORT"))
            
            # Return top 10 targets
            return valid_targets[:10], stop_loss
            
        except Exception as e:
            print(f"Error in target calculation: {e}")
            traceback.print_exc()
            # Fallback to simple targets
            return self._simple_fallback_targets(current_price, direction), current_price * (0.97 if direction == "LONG" else 1.03)


    def _calculate_comprehensive_volatility(self, df: pd.DataFrame) -> Dict[str, float]:
        """Calculate comprehensive volatility metrics for dynamic risk management"""
        try:
            metrics = {}
            
            # 1. ATR-based volatility (primary)
            if 'atr' in df.columns:
                current_atr = df['atr'].iloc[-1]
                avg_atr_14 = df['atr'].iloc[-14:].mean()
                avg_atr_30 = df['atr'].iloc[-30:].mean() if len(df) >= 30 else avg_atr_14
                
                metrics['atr_current'] = current_atr
                metrics['atr_14day'] = avg_atr_14
                metrics['atr_30day'] = avg_atr_30
                metrics['atr_trend'] = avg_atr_14 / avg_atr_30 if avg_atr_30 > 0 else 1.0
            else:
                # Calculate ATR manually
                high_low = df['high'] - df['low']
                high_close = abs(df['high'] - df['close'].shift())
                low_close = abs(df['low'] - df['close'].shift())
                true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                
                metrics['atr_current'] = true_range.iloc[-1]
                metrics['atr_14day'] = true_range.rolling(14).mean().iloc[-1]
                metrics['atr_30day'] = true_range.rolling(30).mean().iloc[-1] if len(df) >= 30 else metrics['atr_14day']
                metrics['atr_trend'] = metrics['atr_14day'] / metrics['atr_30day'] if metrics['atr_30day'] > 0 else 1.0
            
            # 2. Price volatility (standard deviation)
            returns = df['close'].pct_change().dropna()
            metrics['price_std_7d'] = returns.iloc[-7:].std() * 100 if len(returns) >= 7 else 0
            metrics['price_std_14d'] = returns.iloc[-14:].std() * 100 if len(returns) >= 14 else metrics['price_std_7d']
            metrics['price_std_30d'] = returns.iloc[-30:].std() * 100 if len(returns) >= 30 else metrics['price_std_14d']
            
            # 3. Intraday volatility (high-low range)
            daily_ranges = ((df['high'] - df['low']) / df['close']) * 100
            metrics['intraday_vol_current'] = daily_ranges.iloc[-1]
            metrics['intraday_vol_avg'] = daily_ranges.iloc[-10:].mean() if len(daily_ranges) >= 10 else daily_ranges.iloc[-1]
            
            # 4. Volume-weighted volatility
            if 'volume' in df.columns:
                volume_weighted_std = (returns.iloc[-14:] * df['volume'].iloc[-14:]).std() * 100 if len(returns) >= 14 else 0
                metrics['volume_weighted_vol'] = volume_weighted_std
            else:
                metrics['volume_weighted_vol'] = metrics['price_std_14d']
            
            # 5. Volatility percentile (where current volatility ranks historically)
            if len(returns) >= 50:
                current_vol = metrics['price_std_7d']
                historical_vols = []
                for i in range(7, min(len(returns), 100)):
                    historical_vols.append(returns.iloc[i-7:i].std() * 100)
                
                if historical_vols:
                    percentile = sum(1 for v in historical_vols if v < current_vol) / len(historical_vols)
                    metrics['volatility_percentile'] = percentile
                else:
                    metrics['volatility_percentile'] = 0.5
            else:
                metrics['volatility_percentile'] = 0.5
            
            return metrics
            
        except Exception as e:
            print(f"Error calculating volatility metrics: {e}")
            # Return safe defaults
            return {
                'atr_current': df['close'].iloc[-1] * 0.02,
                'atr_14day': df['close'].iloc[-1] * 0.02,
                'atr_30day': df['close'].iloc[-1] * 0.02,
                'atr_trend': 1.0,
                'price_std_7d': 2.0,
                'price_std_14d': 2.0,
                'price_std_30d': 2.0,
                'intraday_vol_current': 2.0,
                'intraday_vol_avg': 2.0,
                'volume_weighted_vol': 2.0,
                'volatility_percentile': 0.5
            }

    def _calculate_dynamic_minimum_risk(self, df: pd.DataFrame, volatility_metrics: Dict[str, float]) -> float:
        """Enhanced minimum risk calculation with crypto-specific adjustments"""
        try:
            current_price = df['close'].iloc[-1]
            
            # CRYPTO BASE: Start with 2.0% minimum (higher than traditional markets)
            base_min_risk_pct = 2.0
            
            # 1. ATR-based adjustment (primary factor for crypto)
            atr_pct = (volatility_metrics['atr_14day'] / current_price) * 100
            if atr_pct > 12:  # Extremely volatile crypto
                atr_adjustment = 3.0
            elif atr_pct > 8:  # Highly volatile
                atr_adjustment = 2.5
            elif atr_pct > 5:  # Moderately volatile
                atr_adjustment = 2.0
            elif atr_pct > 3:  # Normal crypto volatility
                atr_adjustment = 1.5
            else:  # Low volatility (rare in crypto)
                atr_adjustment = 1.2
            
            # 2. Price standard deviation adjustment (crypto can be extreme)
            price_std = volatility_metrics['price_std_14d']
            if price_std > 15:  # Extremely volatile
                std_adjustment = 2.2
            elif price_std > 10:  # Highly volatile
                std_adjustment = 1.8
            elif price_std > 6:  # Moderately volatile
                std_adjustment = 1.4
            elif price_std > 3:  # Normal crypto volatility
                std_adjustment = 1.0
            else:  # Low volatility
                std_adjustment = 0.9
            
            # 3. Market regime adjustment (crypto-specific)
            volatility_percentile = volatility_metrics['volatility_percentile']
            if volatility_percentile > 0.9:  # Top 10% volatility periods
                regime_adjustment = 1.6
            elif volatility_percentile > 0.7:  # High volatility periods
                regime_adjustment = 1.3
            elif volatility_percentile < 0.2:  # Rare low volatility in crypto
                regime_adjustment = 0.8
            else:  # Normal periods
                regime_adjustment = 1.0
            
            # 4. Intraday volatility spike protection
            intraday_adjustment = 1.0
            if volatility_metrics['intraday_vol_current'] > volatility_metrics['intraday_vol_avg'] * 2.0:
                intraday_adjustment = 1.4  # Much higher adjustment for crypto spikes
            
            # Weighted combination (crypto-optimized weights)
            total_adjustment = (
                atr_adjustment * 0.40 +        # ATR gets highest weight in crypto
                std_adjustment * 0.25 +       # Price volatility
                regime_adjustment * 0.20 +    # Market regime
                intraday_adjustment * 0.15    # Intraday spikes
            )
            
            # Calculate final minimum risk with crypto floor
            dynamic_min_risk_pct = base_min_risk_pct * total_adjustment
            
            # CRYPTO BOUNDS: 1.5% absolute minimum, 7% maximum for minimum risk
            dynamic_min_risk_pct = max(1.5, min(7.0, dynamic_min_risk_pct))
            
            # Convert to absolute value
            dynamic_min_risk = current_price * (dynamic_min_risk_pct / 100)
            
            return dynamic_min_risk
            
        except Exception as e:
            print(f"Error calculating dynamic minimum risk: {e}")
            return df['close'].iloc[-1] * 0.025  # Fallback to 2.5% for crypto


    def _calculate_enhanced_long_stop(self, df: pd.DataFrame, current_price: float, 
                                    support_levels: List[float], volatility_metrics: Dict[str, float],
                                    dynamic_min_risk: float) -> float:
        """Calculate enhanced stop loss for long positions with crypto volatility safeguards"""
        try:
            stop_candidates = []
            
            # 1. ATR-based stop with crypto-specific multipliers
            atr_multiplier = 2.5  # Higher base multiplier for crypto
            
            atr_pct = (volatility_metrics['atr_14day'] / current_price) * 100
            if atr_pct > 10:  # Extreme crypto volatility
                atr_multiplier = 3.5
            elif atr_pct > 7:   # High volatility
                atr_multiplier = 3.0
            elif atr_pct > 4:   # Moderate volatility
                atr_multiplier = 2.5
            else:               # Lower volatility
                atr_multiplier = 2.0
            
            atr_stop = current_price - (volatility_metrics['atr_14day'] * atr_multiplier)
            stop_candidates.append(('ATR', atr_stop))
            
            # 2. Support-based stop with crypto buffer
            if support_levels:
                valid_supports = [s for s in support_levels if s < current_price * 0.95]
                if valid_supports:
                    nearest_support = max(valid_supports)
                    # Larger buffer for crypto volatility
                    buffer = volatility_metrics['atr_14day'] * 0.8
                    support_stop = nearest_support - buffer
                    stop_candidates.append(('Support', support_stop))
            
            # 3. Recent swing low with enhanced buffer
            if len(df) >= 20:
                lookback = min(30, len(df))
                recent_low = df['low'].iloc[-lookback:].min()
                # Enhanced buffer for crypto
                swing_buffer = volatility_metrics['atr_14day'] * 0.6
                swing_stop = recent_low - swing_buffer
                stop_candidates.append(('Swing Low', swing_stop))
            
            # 4. Percentage-based stop (crypto-adjusted)
            volatility_adjusted_pct = max(3.0, volatility_metrics['price_std_14d'] * 1.0)
            pct_stop = current_price * (1 - volatility_adjusted_pct / 100)
            stop_candidates.append(('Percentage', pct_stop))
            
            # Choose the most conservative stop that meets minimum risk
            min_required_stop = current_price - dynamic_min_risk
            
            valid_stops = []
            for name, stop_price in stop_candidates:
                if stop_price <= min_required_stop:
                    valid_stops.append((name, stop_price))
            
            if valid_stops:
                chosen_stop = max(valid_stops, key=lambda x: x[1])[1]
            else:
                chosen_stop = min_required_stop
            
            # CRYPTO SAFEGUARD: Ensure stop is at least 1.5% below current price
            crypto_min_stop = current_price * 0.985
            if chosen_stop > crypto_min_stop:
                chosen_stop = crypto_min_stop
            
            # Final validation
            if (current_price - chosen_stop) < dynamic_min_risk:
                chosen_stop = current_price - dynamic_min_risk
            
            return chosen_stop
            
        except Exception as e:
            print(f"Error calculating enhanced long stop: {e}")
            return current_price - dynamic_min_risk


    def _calculate_enhanced_short_stop(self, df: pd.DataFrame, current_price: float, 
                                    resistance_levels: List[float], volatility_metrics: Dict[str, float],
                                    dynamic_min_risk: float) -> float:
        """Calculate enhanced stop loss for short positions with volatility consideration"""
        try:
            stop_candidates = []
            
            # 1. ATR-based stop with volatility adjustment
            atr_multiplier = 2.0  # Base multiplier
            
            # Adjust multiplier based on volatility regime
            atr_pct = (volatility_metrics['atr_14day'] / current_price) * 100
            if atr_pct > 6:  # High volatility - need wider stops
                atr_multiplier = 2.8
            elif atr_pct > 4:
                atr_multiplier = 2.4
            elif atr_pct > 2:
                atr_multiplier = 2.0
            else:  # Low volatility - can use tighter stops
                atr_multiplier = 1.8
            
            atr_stop = current_price + (volatility_metrics['atr_14day'] * atr_multiplier)
            stop_candidates.append(('ATR', atr_stop))
            
            # 2. Resistance-based stop (with buffer)
            if resistance_levels:
                valid_resistances = [r for r in resistance_levels if r > current_price * 1.05]  # At least 5% above
                if valid_resistances:
                    nearest_resistance = min(valid_resistances)
                    # Add volatility-based buffer above resistance
                    buffer = volatility_metrics['atr_14day'] * 0.5
                    resistance_stop = nearest_resistance + buffer
                    stop_candidates.append(('Resistance', resistance_stop))
            
            # 3. Recent swing high with volatility buffer
            if len(df) >= 20:
                lookback = min(30, len(df))  # Look back 30 periods or less
                recent_high = df['high'].iloc[-lookback:].max()
                
                # Add volatility-based buffer above swing high
                swing_buffer = volatility_metrics['atr_14day'] * 0.3
                swing_stop = recent_high + swing_buffer
                stop_candidates.append(('Swing High', swing_stop))
            
            # 4. Percentage-based stop adjusted for volatility
            volatility_adjusted_pct = max(2.5, volatility_metrics['price_std_14d'] * 0.8)  # At least 2.5%
            pct_stop = current_price * (1 + volatility_adjusted_pct / 100)
            stop_candidates.append(('Percentage', pct_stop))
            
            # Choose the most conservative stop that still meets minimum risk requirement
            valid_stops = []
            max_allowed_stop = current_price + dynamic_min_risk
            
            for name, stop_price in stop_candidates:
                if stop_price >= max_allowed_stop:  # Stop is far enough to meet minimum risk
                    valid_stops.append((name, stop_price))
            
            if valid_stops:
                # Choose the lowest (most conservative) valid stop
                chosen_stop = min(valid_stops, key=lambda x: x[1])[1]
            else:
                # If no stops meet minimum risk, use the minimum risk stop
                chosen_stop = max_allowed_stop
            
            # Final validation: ensure stop is at least dynamic minimum risk away
            if (chosen_stop - current_price) < dynamic_min_risk:
                chosen_stop = current_price + dynamic_min_risk
            
            return chosen_stop
            
        except Exception as e:
            print(f"Error calculating enhanced short stop: {e}")
            return current_price + dynamic_min_risk

    def _validate_and_enforce_risk(self, current_price: float, stop_loss: float, risk: float,
                                direction: str, dynamic_min_risk: float) -> Tuple[float, float]:
        """Enhanced risk validation with crypto-specific safeguards"""
        try:
            # CRYPTO-SPECIFIC MINIMUM RISK (never less than 1.5% for crypto)
            crypto_min_risk = max(current_price * 0.015, dynamic_min_risk)  # At least 1.5%
            
            # Maximum risk (never more than 8% to prevent excessive losses)
            max_risk = current_price * 0.08
            
            if direction == "LONG":
                # CRITICAL: Ensure stop loss is meaningfully below current price
                if stop_loss >= current_price * 0.985:  # Stop must be at least 1.5% below
                    print(f"WARNING: Stop loss too high ({stop_loss:.6f} vs price {current_price:.6f}). Enforcing minimum.")
                    stop_loss = current_price - crypto_min_risk
                    risk = crypto_min_risk
                else:
                    risk = current_price - stop_loss
                    
                    # Enforce minimum risk for crypto volatility
                    if risk < crypto_min_risk:
                        print(f"WARNING: Risk too small ({risk/current_price*100:.2f}%). Enforcing {crypto_min_risk/current_price*100:.2f}%")
                        stop_loss = current_price - crypto_min_risk
                        risk = crypto_min_risk
                    
                    # Cap maximum risk
                    elif risk > max_risk:
                        print(f"WARNING: Risk too large ({risk/current_price*100:.2f}%). Capping at 8%")
                        stop_loss = current_price - max_risk
                        risk = max_risk
                        
            else:  # SHORT
                # CRITICAL: Ensure stop loss is meaningfully above current price
                if stop_loss <= current_price * 1.015:  # Stop must be at least 1.5% above
                    print(f"WARNING: Stop loss too low ({stop_loss:.6f} vs price {current_price:.6f}). Enforcing minimum.")
                    stop_loss = current_price + crypto_min_risk
                    risk = crypto_min_risk
                else:
                    risk = stop_loss - current_price
                    
                    # Enforce minimum risk
                    if risk < crypto_min_risk:
                        print(f"WARNING: Risk too small ({risk/current_price*100:.2f}%). Enforcing {crypto_min_risk/current_price*100:.2f}%")
                        stop_loss = current_price + crypto_min_risk
                        risk = crypto_min_risk
                    
                    # Cap maximum risk
                    elif risk > max_risk:
                        print(f"WARNING: Risk too large ({risk/current_price*100:.2f}%). Capping at 8%")
                        stop_loss = current_price + max_risk
                        risk = max_risk
            
            # Final validation with absolute floor
            risk_pct = (risk / current_price) * 100
            if risk_pct < 1.5:  # Absolute floor for crypto
                if direction == "LONG":
                    stop_loss = current_price * 0.985
                    risk = current_price * 0.015
                else:
                    stop_loss = current_price * 1.015
                    risk = current_price * 0.015
                risk_pct = 1.5
            
            return stop_loss, risk
            
        except Exception as e:
            print(f"Error in risk validation: {e}")
            # Ultra-safe fallback
            if direction == "LONG":
                safe_stop = current_price * 0.975  # 2.5% risk
                safe_risk = current_price * 0.025
            else:
                safe_stop = current_price * 1.025  # 2.5% risk
                safe_risk = current_price * 0.025
            return safe_stop, safe_risk



    def _calculate_enhanced_atr_inline(self, df: pd.DataFrame) -> float:
        """Calculate enhanced ATR with adaptive period"""
        try:
            if 'atr' in df.columns:
                # Use existing ATR but enhance with volatility adjustment
                base_atr = df['atr'].iloc[-1]
                
                # Adjust for recent volatility changes
                recent_atr = df['atr'].iloc[-5:].mean()
                longer_atr = df['atr'].iloc[-20:].mean() if len(df) >= 20 else recent_atr
                
                # If recent volatility is higher, use it; otherwise use longer average
                if recent_atr > longer_atr * 1.2:
                    return recent_atr
                else:
                    return (recent_atr + longer_atr) / 2
            else:
                # Calculate ATR manually with adaptive period
                high_low = df['high'] - df['low']
                high_close = abs(df['high'] - df['close'].shift())
                low_close = abs(df['low'] - df['close'].shift())
                
                true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                
                # Use adaptive period based on market volatility
                volatility = true_range.std()
                period = 14 if volatility < true_range.mean() else 10
                
                return true_range.rolling(window=period).mean().iloc[-1]
                
        except Exception as e:
            return df['close'].iloc[-1] * 0.02  # Fallback to 2%

    def _get_volatility_regime_inline(self, df: pd.DataFrame) -> str:
        """Determine current volatility regime"""
        try:
            if 'atr_percent' in df.columns:
                current_vol = df['atr_percent'].iloc[-5:].mean()
                historical_vol = df['atr_percent'].iloc[-50:].mean() if len(df) >= 50 else current_vol
                
                if current_vol < historical_vol * 0.7:
                    return 'low_vol'
                elif current_vol > historical_vol * 1.3:
                    return 'high_vol'
                else:
                    return 'normal_vol'
            
            # Fallback using price standard deviation
            price_returns = df['close'].pct_change().dropna()
            current_vol = price_returns.iloc[-10:].std() * 100 if len(price_returns) >= 10 else price_returns.std() * 100
            historical_vol = price_returns.iloc[-50:].std() * 100 if len(price_returns) >= 50 else current_vol
            
            if current_vol < historical_vol * 0.7:
                return 'low_vol'
            elif current_vol > historical_vol * 1.3:
                return 'high_vol'
            else:
                return 'normal_vol'
                
        except Exception as e:
            return 'normal_vol'

    def _calculate_dynamic_long_stop_inline(self, df: pd.DataFrame, current_price: float, atr: float, 
                                        support_levels: List[float], risk_multiplier: float) -> float:
        """Calculate dynamic stop loss for long positions"""
        try:
            # Method 1: ATR-based with dynamic multiplier
            atr_stop = current_price - (atr * risk_multiplier)
            
            # Method 2: Nearest significant support
            valid_supports = [s for s in support_levels if s < current_price * 0.98]
            nearest_support = max(valid_supports) if valid_supports else current_price * 0.95
            
            # Method 3: Recent swing low with buffer
            recent_low = df['low'].iloc[-20:].min() if len(df) >= 20 else df['low'].min()
            swing_stop = recent_low * 0.995  # 0.5% below recent low
            
            # Method 4: Trailing stop based on recent price action
            if len(df) >= 10:
                recent_high = df['high'].iloc[-10:].max()
                trailing_stop = recent_high * (1 - (atr / current_price) * 2)
            else:
                trailing_stop = current_price * 0.96
            
            # Choose the most conservative (highest) stop that still provides reasonable risk
            potential_stops = [atr_stop, nearest_support, swing_stop, trailing_stop]
            valid_stops = [stop for stop in potential_stops if stop < current_price]
            
            if valid_stops:
                chosen_stop = max(valid_stops)  # Most conservative
                
                # Ensure stop loss isn't too close (min 0.8%) or too far (max 6%)
                min_stop = current_price * 0.992
                max_stop = current_price * 0.94
                
                chosen_stop = min(max(chosen_stop, max_stop), min_stop)
                return chosen_stop
            
            return current_price * 0.97  # Fallback 3% stop
            
        except Exception as e:
            return current_price * 0.97

    def _calculate_dynamic_short_stop_inline(self, df: pd.DataFrame, current_price: float, atr: float, 
                                        resistance_levels: List[float], risk_multiplier: float) -> float:
        """Calculate dynamic stop loss for short positions"""
        try:
            # Method 1: ATR-based with dynamic multiplier
            atr_stop = current_price + (atr * risk_multiplier)
            
            # Method 2: Nearest significant resistance
            valid_resistances = [r for r in resistance_levels if r > current_price * 1.02]
            nearest_resistance = min(valid_resistances) if valid_resistances else current_price * 1.05
            
            # Method 3: Recent swing high with buffer
            recent_high = df['high'].iloc[-20:].max() if len(df) >= 20 else df['high'].max()
            swing_stop = recent_high * 1.005  # 0.5% above recent high
            
            # Method 4: Trailing stop based on recent price action
            if len(df) >= 10:
                recent_low = df['low'].iloc[-10:].min()
                trailing_stop = recent_low * (1 + (atr / current_price) * 2)
            else:
                trailing_stop = current_price * 1.04
            
            # Choose the most conservative (lowest) stop that still provides reasonable risk
            potential_stops = [atr_stop, nearest_resistance, swing_stop, trailing_stop]
            valid_stops = [stop for stop in potential_stops if stop > current_price]
            
            if valid_stops:
                chosen_stop = min(valid_stops)  # Most conservative for short
                
                # Ensure stop loss isn't too close (min 0.8%) or too far (max 6%)
                min_stop = current_price * 1.008
                max_stop = current_price * 1.06
                
                chosen_stop = max(min(chosen_stop, max_stop), min_stop)
                return chosen_stop
            
            return current_price * 1.03  # Fallback 3% stop
            
        except Exception as e:
            return current_price * 1.03

    def _generate_dynamic_long_targets_inline(self, df: pd.DataFrame, current_price: float, stop_loss: float,
                                            resistance_levels: List[float], target_multiplier: float) -> List[Dict[str, Any]]:
        """Generate dynamic targets for long positions"""
        try:
            targets = []
            risk = current_price - stop_loss
            
            # Base targets using risk-reward ratios
            rr_ratios = [1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
            for i, rr in enumerate(rr_ratios):
                target_price = current_price + (risk * rr * target_multiplier)
                probability = max(30, 85 - (i * 8))  # Decreasing probability
                
                targets.append({
                    "price": target_price,
                    "reason": f"{rr:.1f}:1 Risk-Reward",
                    "probability": probability,
                    "percent": ((target_price / current_price) - 1) * 100
                })
            
            # Add resistance-based targets
            valid_resistances = [r for r in resistance_levels if r > current_price * 1.01]
            for i, resistance in enumerate(valid_resistances[:4]):
                if risk > 0:
                    rr_ratio = (resistance - current_price) / risk
                    if rr_ratio > 1.2:  # Only if decent risk-reward
                        probability = max(40, 80 - (i * 10))
                        targets.append({
                            "price": resistance,
                            "reason": "Key Resistance Level",
                            "probability": probability,
                            "percent": ((resistance / current_price) - 1) * 100
                        })
            
            # Add Fibonacci extension targets
            if len(df) >= 30:
                recent_high = df['high'].iloc[-30:].max()
                recent_low = df['low'].iloc[-30:].min()
                
                if recent_high > recent_low:
                    fib_range = recent_high - recent_low
                    fib_extensions = [1.272, 1.618, 2.0, 2.618]
                    
                    for fib in fib_extensions:
                        fib_target = recent_high + (fib_range * (fib - 1.0))
                        if fib_target > current_price * 1.02 and risk > 0:
                            rr_ratio = (fib_target - current_price) / risk
                            if rr_ratio > 1.5:
                                probability = max(35, 70 - int(fib * 15))
                                targets.append({
                                    "price": fib_target,
                                    "reason": f"Fibonacci {fib} Extension",
                                    "probability": probability,
                                    "percent": ((fib_target / current_price) - 1) * 100
                                })
            
            return targets
            
        except Exception as e:
            return []

    def _generate_dynamic_short_targets_inline(self, df: pd.DataFrame, current_price: float, stop_loss: float,
                                            support_levels: List[float], target_multiplier: float) -> List[Dict[str, Any]]:
        """Generate dynamic targets for short positions"""
        try:
            targets = []
            risk = stop_loss - current_price
            
            # Base targets using risk-reward ratios
            rr_ratios = [1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
            for i, rr in enumerate(rr_ratios):
                target_price = current_price - (risk * rr * target_multiplier)
                probability = max(30, 85 - (i * 8))  # Decreasing probability
                
                targets.append({
                    "price": target_price,
                    "reason": f"{rr:.1f}:1 Risk-Reward",
                    "probability": probability,
                    "percent": ((target_price / current_price) - 1) * 100
                })
            
            # Add support-based targets
            valid_supports = [s for s in support_levels if s < current_price * 0.99]
            for i, support in enumerate(valid_supports[:4]):
                if risk > 0:
                    rr_ratio = (current_price - support) / risk
                    if rr_ratio > 1.2:  # Only if decent risk-reward
                        probability = max(40, 80 - (i * 10))
                        targets.append({
                            "price": support,
                            "reason": "Key Support Level",
                            "probability": probability,
                            "percent": ((support / current_price) - 1) * 100
                        })
            
            # Add Fibonacci extension targets
            if len(df) >= 30:
                recent_high = df['high'].iloc[-30:].max()
                recent_low = df['low'].iloc[-30:].min()
                
                if recent_high > recent_low:
                    fib_range = recent_high - recent_low
                    fib_extensions = [1.272, 1.618, 2.0, 2.618]
                    
                    for fib in fib_extensions:
                        fib_target = recent_low - (fib_range * (fib - 1.0))
                        if fib_target < current_price * 0.98 and risk > 0:
                            rr_ratio = (current_price - fib_target) / risk
                            if rr_ratio > 1.5:
                                probability = max(35, 70 - int(fib * 15))
                                targets.append({
                                    "price": fib_target,
                                    "reason": f"Fibonacci {fib} Extension",
                                    "probability": probability,
                                    "percent": ((fib_target / current_price) - 1) * 100
                                })
            
            return targets
            
        except Exception as e:
            return []
        

    def _generate_enhanced_long_targets(self, df: pd.DataFrame, current_price: float, risk: float,
                                  resistance_levels: List[float], volatility_metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """Generate enhanced targets for long positions with volatility-adjusted probabilities"""
        try:
            targets = []
            
            # 1. Risk-Reward based targets (adjusted for volatility)
            base_rr_ratios = [1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
            
            # Adjust target multiplier based on volatility
            volatility_factor = self._calculate_volatility_target_factor(volatility_metrics)
            
            for i, rr in enumerate(base_rr_ratios):
                target_price = current_price + (risk * rr * volatility_factor)
                
                # Adjust probability based on volatility and risk-reward ratio
                base_probability = max(30, 85 - (i * 8))
                volatility_adjusted_probability = self._adjust_probability_for_volatility(
                    base_probability, volatility_metrics, rr
                )
                
                targets.append({
                    "price": target_price,
                    "reason": f"{rr:.1f}:1 Risk-Reward",
                    "probability": volatility_adjusted_probability,
                    "percent": ((target_price / current_price) - 1) * 100
                })
            
            # 2. Resistance-based targets (with volatility consideration)
            valid_resistances = [r for r in resistance_levels if r > current_price * 1.01]
            for i, resistance in enumerate(valid_resistances[:4]):
                if risk > 0:
                    rr_ratio = (resistance - current_price) / risk
                    if rr_ratio > 1.2:  # Only if decent risk-reward
                        # Reduce probability for resistance levels in high volatility
                        base_probability = max(40, 80 - (i * 10))
                        resistance_probability = self._adjust_resistance_probability(
                            base_probability, volatility_metrics
                        )
                        
                        targets.append({
                            "price": resistance,
                            "reason": "Key Resistance Level",
                            "probability": resistance_probability,
                            "percent": ((resistance / current_price) - 1) * 100
                        })
            
            # 3. Fibonacci extension targets (volatility-adjusted)
            if len(df) >= 30:
                fib_targets = self._calculate_fibonacci_targets_long(df, current_price, risk, volatility_metrics)
                targets.extend(fib_targets)
            
            # 4. ATR-based targets (new - based on volatility)
            atr_targets = self._calculate_atr_based_targets_long(current_price, risk, volatility_metrics)
            targets.extend(atr_targets)
            
            return targets
            
        except Exception as e:
            print(f"Error generating enhanced long targets: {e}")
            return self._generate_fallback_long_targets(current_price, risk)

    def _generate_enhanced_short_targets(self, df: pd.DataFrame, current_price: float, risk: float,
                                    support_levels: List[float], volatility_metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """Generate enhanced targets for short positions with volatility-adjusted probabilities"""
        try:
            targets = []
            
            # 1. Risk-Reward based targets (adjusted for volatility)
            base_rr_ratios = [1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
            
            # Adjust target multiplier based on volatility
            volatility_factor = self._calculate_volatility_target_factor(volatility_metrics)
            
            for i, rr in enumerate(base_rr_ratios):
                target_price = current_price - (risk * rr * volatility_factor)
                
                # Adjust probability based on volatility and risk-reward ratio
                base_probability = max(30, 85 - (i * 8))
                volatility_adjusted_probability = self._adjust_probability_for_volatility(
                    base_probability, volatility_metrics, rr
                )
                
                targets.append({
                    "price": target_price,
                    "reason": f"{rr:.1f}:1 Risk-Reward",
                    "probability": volatility_adjusted_probability,
                    "percent": ((target_price / current_price) - 1) * 100
                })
            
            # 2. Support-based targets (with volatility consideration)
            valid_supports = [s for s in support_levels if s < current_price * 0.99]
            for i, support in enumerate(valid_supports[:4]):
                if risk > 0:
                    rr_ratio = (current_price - support) / risk
                    if rr_ratio > 1.2:  # Only if decent risk-reward
                        # Reduce probability for support levels in high volatility
                        base_probability = max(40, 80 - (i * 10))
                        support_probability = self._adjust_resistance_probability(
                            base_probability, volatility_metrics
                        )
                        
                        targets.append({
                            "price": support,
                            "reason": "Key Support Level",
                            "probability": support_probability,
                            "percent": ((support / current_price) - 1) * 100
                        })
            
            # 3. Fibonacci extension targets (volatility-adjusted)
            if len(df) >= 30:
                fib_targets = self._calculate_fibonacci_targets_short(df, current_price, risk, volatility_metrics)
                targets.extend(fib_targets)
            
            # 4. ATR-based targets (new - based on volatility)
            atr_targets = self._calculate_atr_based_targets_short(current_price, risk, volatility_metrics)
            targets.extend(atr_targets)
            
            return targets
            
        except Exception as e:
            print(f"Error generating enhanced short targets: {e}")
            return self._generate_fallback_short_targets(current_price, risk)

    def _calculate_volatility_target_factor(self, volatility_metrics: Dict[str, float]) -> float:
        """Calculate target adjustment factor based on market volatility"""
        try:
            # Base factor is 1.0 (no adjustment)
            base_factor = 1.0
            
            # Adjust based on ATR percentage
            atr_14d = volatility_metrics.get('atr_14day', 0)
            if atr_14d > 0:
                # In high volatility, targets should be further (easier to reach due to big moves)
                # In low volatility, targets should be closer (harder to reach big moves)
                volatility_percentile = volatility_metrics.get('volatility_percentile', 0.5)
                
                if volatility_percentile > 0.8:  # High volatility period
                    factor = 1.2  # Targets 20% further
                elif volatility_percentile > 0.6:  # Above average volatility
                    factor = 1.1  # Targets 10% further
                elif volatility_percentile < 0.2:  # Low volatility period
                    factor = 0.9  # Targets 10% closer
                else:  # Normal volatility
                    factor = 1.0
            else:
                factor = 1.0
            
            return factor
            
        except Exception as e:
            return 1.0

    def _adjust_probability_for_volatility(self, base_probability: float, volatility_metrics: Dict[str, float], rr_ratio: float) -> float:
        """Adjust target probability based on volatility and risk-reward ratio"""
        try:
            adjusted_probability = base_probability
            
            # Get volatility percentile
            volatility_percentile = volatility_metrics.get('volatility_percentile', 0.5)
            price_std = volatility_metrics.get('price_std_14d', 2.0)
            
            # In high volatility, higher R:R targets are more likely (big moves happen)
            # In low volatility, lower R:R targets are more likely (small moves more common)
            
            if volatility_percentile > 0.7:  # High volatility
                if rr_ratio >= 3.0:  # High R:R targets get bonus in volatile markets
                    volatility_bonus = min(15, (rr_ratio - 2.0) * 5)
                    adjusted_probability += volatility_bonus
                elif rr_ratio < 2.0:  # Low R:R targets get penalty (might overshoot)
                    volatility_penalty = (2.0 - rr_ratio) * 10
                    adjusted_probability -= volatility_penalty
                    
            elif volatility_percentile < 0.3:  # Low volatility
                if rr_ratio >= 3.0:  # High R:R targets get penalty in stable markets
                    volatility_penalty = (rr_ratio - 2.0) * 8
                    adjusted_probability -= volatility_penalty
                elif rr_ratio < 2.0:  # Low R:R targets get bonus (more realistic)
                    volatility_bonus = (2.0 - rr_ratio) * 12
                    adjusted_probability += volatility_bonus
            
            # Additional adjustment based on price standard deviation
            if price_std > 8:  # Very high price volatility
                # Reduce probability for all targets except very close ones
                if rr_ratio > 2.5:
                    adjusted_probability -= 10
            elif price_std < 2:  # Very low price volatility
                # Reduce probability for distant targets
                if rr_ratio > 3.0:
                    adjusted_probability -= 15
            
            # Ensure reasonable bounds
            return max(25, min(90, int(adjusted_probability)))
            
        except Exception as e:
            return int(base_probability)

    def _adjust_resistance_probability(self, base_probability: float, volatility_metrics: Dict[str, float]) -> float:
        """Adjust resistance/support level probability based on volatility"""
        try:
            adjusted_probability = base_probability
            
            # In high volatility, resistance/support levels are more likely to be broken
            # In low volatility, they're more likely to hold
            volatility_percentile = volatility_metrics.get('volatility_percentile', 0.5)
            
            if volatility_percentile > 0.8:  # Very high volatility
                adjusted_probability -= 20  # Much less likely to hold
            elif volatility_percentile > 0.6:  # High volatility
                adjusted_probability -= 10  # Less likely to hold
            elif volatility_percentile < 0.2:  # Low volatility
                adjusted_probability += 15  # More likely to hold
            elif volatility_percentile < 0.4:  # Below average volatility
                adjusted_probability += 8  # Somewhat more likely to hold
            
            return max(25, min(85, int(adjusted_probability)))
            
        except Exception as e:
            return int(base_probability)

    def _calculate_fibonacci_targets_long(self, df: pd.DataFrame, current_price: float, risk: float, volatility_metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """Calculate Fibonacci extension targets for long positions"""
        try:
            targets = []
            
            # Find recent swing high and low
            recent_high = df['high'].iloc[-30:].max()
            recent_low = df['low'].iloc[-30:].min()
            
            if recent_high > recent_low:
                fib_range = recent_high - recent_low
                fib_extensions = [1.272, 1.618, 2.0, 2.618]
                
                for fib in fib_extensions:
                    fib_target = recent_high + (fib_range * (fib - 1.0))
                    if fib_target > current_price * 1.02 and risk > 0:
                        rr_ratio = (fib_target - current_price) / risk
                        if rr_ratio > 1.2:
                            # Adjust probability based on volatility
                            base_probability = max(35, 65 - int(fib * 10))
                            volatility_adjusted_probability = self._adjust_probability_for_volatility(
                                base_probability, volatility_metrics, rr_ratio
                            )
                            
                            targets.append({
                                "price": fib_target,
                                "reason": f"Fibonacci {fib} Extension",
                                "probability": volatility_adjusted_probability,
                                "percent": ((fib_target / current_price) - 1) * 100
                            })
            
            return targets
            
        except Exception as e:
            return []

    def _calculate_fibonacci_targets_short(self, df: pd.DataFrame, current_price: float, risk: float, volatility_metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """Calculate Fibonacci extension targets for short positions"""
        try:
            targets = []
            
            # Find recent swing high and low
            recent_high = df['high'].iloc[-30:].max()
            recent_low = df['low'].iloc[-30:].min()
            
            if recent_high > recent_low:
                fib_range = recent_high - recent_low
                fib_extensions = [1.272, 1.618, 2.0, 2.618]
                
                for fib in fib_extensions:
                    fib_target = recent_low - (fib_range * (fib - 1.0))
                    if fib_target < current_price * 0.98 and risk > 0:
                        rr_ratio = (current_price - fib_target) / risk
                        if rr_ratio > 1.2:
                            # Adjust probability based on volatility
                            base_probability = max(35, 65 - int(fib * 10))
                            volatility_adjusted_probability = self._adjust_probability_for_volatility(
                                base_probability, volatility_metrics, rr_ratio
                            )
                            
                            targets.append({
                                "price": fib_target,
                                "reason": f"Fibonacci {fib} Extension",
                                "probability": volatility_adjusted_probability,
                                "percent": ((fib_target / current_price) - 1) * 100
                            })
            
            return targets
            
        except Exception as e:
            return []

    def _calculate_atr_based_targets_long(self, current_price: float, risk: float, volatility_metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """Calculate ATR-based targets for long positions"""
        try:
            targets = []
            atr_14d = volatility_metrics.get('atr_14day', current_price * 0.02)
            
            # ATR multipliers for targets
            atr_multipliers = [2, 3, 4, 6, 8]
            
            for i, multiplier in enumerate(atr_multipliers):
                target_price = current_price + (atr_14d * multiplier)
                rr_ratio = (target_price - current_price) / risk if risk > 0 else 0
                
                if rr_ratio > 1.0:  # Only include if R:R > 1:1
                    base_probability = max(30, 70 - (i * 8))
                    volatility_adjusted_probability = self._adjust_probability_for_volatility(
                        base_probability, volatility_metrics, rr_ratio
                    )
                    
                    targets.append({
                        "price": target_price,
                        "reason": f"ATR {multiplier}x Target",
                        "probability": volatility_adjusted_probability,
                        "percent": ((target_price / current_price) - 1) * 100
                    })
            
            return targets
            
        except Exception as e:
            return []

    def _calculate_atr_based_targets_short(self, current_price: float, risk: float, volatility_metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """Calculate ATR-based targets for short positions"""
        try:
            targets = []
            atr_14d = volatility_metrics.get('atr_14day', current_price * 0.02)
            
            # ATR multipliers for targets
            atr_multipliers = [2, 3, 4, 6, 8]
            
            for i, multiplier in enumerate(atr_multipliers):
                target_price = current_price - (atr_14d * multiplier)
                rr_ratio = (current_price - target_price) / risk if risk > 0 else 0
                
                if rr_ratio > 1.0:  # Only include if R:R > 1:1
                    base_probability = max(30, 70 - (i * 8))
                    volatility_adjusted_probability = self._adjust_probability_for_volatility(
                        base_probability, volatility_metrics, rr_ratio
                    )
                    
                    targets.append({
                        "price": target_price,
                        "reason": f"ATR {multiplier}x Target",
                        "probability": volatility_adjusted_probability,
                        "percent": ((target_price / current_price) - 1) * 100
                    })
            
            return targets
            
        except Exception as e:
            return []

    def _generate_fallback_long_targets(self, current_price: float, risk: float) -> List[Dict[str, Any]]:
        """Fallback target generation for long positions"""
        try:
            targets = []
            rr_ratios = [1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
            
            for i, rr in enumerate(rr_ratios):
                target_price = current_price + (risk * rr)
                probability = max(30, 80 - (i * 8))
                
                targets.append({
                    "price": target_price,
                    "reason": f"{rr:.1f}:1 Risk-Reward",
                    "probability": probability,
                    "percent": ((target_price / current_price) - 1) * 100
                })
            
            return targets
            
        except Exception as e:
            return []

    def _generate_fallback_short_targets(self, current_price: float, risk: float) -> List[Dict[str, Any]]:
        """Fallback target generation for short positions"""
        try:
            targets = []
            rr_ratios = [1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
            
            for i, rr in enumerate(rr_ratios):
                target_price = current_price - (risk * rr)
                probability = max(30, 80 - (i * 8))
                
                targets.append({
                    "price": target_price,
                    "reason": f"{rr:.1f}:1 Risk-Reward",
                    "probability": probability,
                    "percent": ((target_price / current_price) - 1) * 100
                })
            
            return targets
            
        except Exception as e:
            return []


    def _optimize_target_sequence_inline(self, targets: List[Dict[str, Any]], direction: str, current_price: float) -> List[Dict[str, Any]]:
        """Validate and optimize target sequence"""
        try:
            # Filter out invalid targets
            valid_targets = []
            for target in targets:
                price = target["price"]
                if direction == "LONG" and price > current_price * 1.005:  # At least 0.5% profit
                    valid_targets.append(target)
                elif direction == "SHORT" and price < current_price * 0.995:  # At least 0.5% profit
                    valid_targets.append(target)
            
            # Sort targets by proximity to current price
            if direction == "LONG":
                valid_targets.sort(key=lambda x: x["price"])
            else:
                valid_targets.sort(key=lambda x: x["price"], reverse=True)
            
            # Remove duplicate price levels (keep the one with better reason)
            unique_targets = []
            seen_prices = set()
            
            for target in valid_targets:
                price_key = round(target["price"], 4)  # Round to avoid floating point issues
                if price_key not in seen_prices:
                    seen_prices.add(price_key)
                    unique_targets.append(target)
            
            return unique_targets
            
        except Exception as e:
            return targets

    def _simple_fallback_targets(self, current_price: float, direction: str) -> List[Dict[str, Any]]:
        """Simple fallback target calculation"""
        try:
            targets = []
            
            if direction == "LONG":
                percentages = [2, 5, 8, 12, 18, 25]
                for i, pct in enumerate(percentages):
                    target_price = current_price * (1 + pct/100)
                    targets.append({
                        "price": target_price,
                        "reason": f"{pct}% Target",
                        "probability": max(30, 80 - (i * 8)),
                        "percent": pct
                    })
            else:  # SHORT
                percentages = [2, 5, 8, 12, 18, 25]
                for i, pct in enumerate(percentages):
                    target_price = current_price * (1 - pct/100)
                    targets.append({
                        "price": target_price,
                        "reason": f"{pct}% Target",
                        "probability": max(30, 80 - (i * 8)),
                        "percent": -pct
                    })
            
            return targets
            
        except Exception as e:
            return []

    def _calculate_enhanced_atr(self, df: pd.DataFrame) -> float:
        """Calculate enhanced ATR with adaptive period"""
        try:
            if 'atr' in df.columns:
                # Use existing ATR but enhance with volatility adjustment
                base_atr = df['atr'].iloc[-1]
                
                # Adjust for recent volatility changes
                recent_atr = df['atr'].iloc[-5:].mean()
                longer_atr = df['atr'].iloc[-20:].mean()
                
                # If recent volatility is higher, use it; otherwise use longer average
                if recent_atr > longer_atr * 1.2:
                    return recent_atr
                else:
                    return (recent_atr + longer_atr) / 2
            else:
                # Calculate ATR manually with adaptive period
                high_low = df['high'] - df['low']
                high_close = abs(df['high'] - df['close'].shift())
                low_close = abs(df['low'] - df['close'].shift())
                
                true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                
                # Use adaptive period based on market volatility
                volatility = true_range.std()
                period = 14 if volatility < true_range.mean() else 10
                
                return true_range.rolling(window=period).mean().iloc[-1]
                
        except Exception as e:
            return df['close'].iloc[-1] * 0.02  # Fallback to 2%

    def _get_volatility_regime(self, df: pd.DataFrame) -> str:
        """Determine current volatility regime"""
        try:
            if 'atr_percent' in df.columns:
                current_vol = df['atr_percent'].iloc[-5:].mean()
                historical_vol = df['atr_percent'].iloc[-50:].mean()
                
                if current_vol < historical_vol * 0.7:
                    return 'low_vol'
                elif current_vol > historical_vol * 1.3:
                    return 'high_vol'
                else:
                    return 'normal_vol'
            
            # Fallback using price standard deviation
            price_returns = df['close'].pct_change().dropna()
            current_vol = price_returns.iloc[-10:].std() * 100
            historical_vol = price_returns.iloc[-50:].std() * 100
            
            if current_vol < historical_vol * 0.7:
                return 'low_vol'
            elif current_vol > historical_vol * 1.3:
                return 'high_vol'
            else:
                return 'normal_vol'
                
        except Exception as e:
            return 'normal_vol'

    def _calculate_dynamic_long_stop(self, df: pd.DataFrame, current_price: float, atr: float, 
                                    support_levels: List[float], risk_multiplier: float) -> float:
        """Calculate dynamic stop loss for long positions"""
        try:
            # Method 1: ATR-based with dynamic multiplier
            atr_stop = current_price - (atr * risk_multiplier)
            
            # Method 2: Nearest significant support
            valid_supports = [s for s in support_levels if s < current_price * 0.98]
            nearest_support = max(valid_supports) if valid_supports else current_price * 0.95
            
            # Method 3: Recent swing low with buffer
            recent_low = df['low'].iloc[-20:].min()
            swing_stop = recent_low * 0.995  # 0.5% below recent low
            
            # Method 4: Trailing stop based on recent price action
            if len(df) >= 10:
                recent_high = df['high'].iloc[-10:].max()
                trailing_stop = recent_high * (1 - (atr / current_price) * 2)
            else:
                trailing_stop = current_price * 0.96
            
            # Choose the most conservative (highest) stop that still provides reasonable risk
            potential_stops = [atr_stop, nearest_support, swing_stop, trailing_stop]
            valid_stops = [stop for stop in potential_stops if stop < current_price]
            
            if valid_stops:
                chosen_stop = max(valid_stops)  # Most conservative
                
                # Ensure stop loss isn't too close (min 0.8%) or too far (max 6%)
                min_stop = current_price * 0.992
                max_stop = current_price * 0.94
                
                chosen_stop = min(max(chosen_stop, max_stop), min_stop)
                return chosen_stop
            
            return current_price * 0.97  # Fallback 3% stop
            
        except Exception as e:
            return current_price * 0.97

    def _generate_dynamic_long_targets(self, df: pd.DataFrame, current_price: float, stop_loss: float,
                                    resistance_levels: List[float], target_multiplier: float) -> List[Dict[str, Any]]:
        """Generate dynamic targets for long positions"""
        try:
            targets = []
            risk = current_price - stop_loss
            
            # Base targets using risk-reward ratios
            rr_ratios = [1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
            for i, rr in enumerate(rr_ratios):
                target_price = current_price + (risk * rr * target_multiplier)
                probability = max(30, 85 - (i * 8))  # Decreasing probability
                
                targets.append({
                    "price": target_price,
                    "reason": f"{rr:.1f}:1 Risk-Reward",
                    "probability": probability,
                    "percent": ((target_price / current_price) - 1) * 100
                })
            
            # Add resistance-based targets
            valid_resistances = [r for r in resistance_levels if r > current_price * 1.01]
            for i, resistance in enumerate(valid_resistances[:4]):
                rr_ratio = (resistance - current_price) / risk
                if rr_ratio > 1.2:  # Only if decent risk-reward
                    probability = max(40, 80 - (i * 10))
                    targets.append({
                        "price": resistance,
                        "reason": "Key Resistance Level",
                        "probability": probability,
                        "percent": ((resistance / current_price) - 1) * 100
                    })
            
            # Add Fibonacci extension targets
            if len(df) >= 30:
                recent_high = df['high'].iloc[-30:].max()
                recent_low = df['low'].iloc[-30:].min()
                
                if recent_high > recent_low:
                    fib_range = recent_high - recent_low
                    fib_extensions = [1.272, 1.618, 2.0, 2.618]
                    
                    for fib in fib_extensions:
                        fib_target = recent_high + (fib_range * (fib - 1.0))
                        if fib_target > current_price * 1.02:
                            rr_ratio = (fib_target - current_price) / risk
                            if rr_ratio > 1.5:
                                probability = max(35, 70 - int(fib * 15))
                                targets.append({
                                    "price": fib_target,
                                    "reason": f"Fibonacci {fib} Extension",
                                    "probability": probability,
                                    "percent": ((fib_target / current_price) - 1) * 100
                                })
            
            return targets
            
        except Exception as e:
            return []

    def _find_structural_swing_low(self, df: pd.DataFrame, current_price: float) -> Optional[float]:
        """
        Find the most recent and structurally significant swing low below the current price.
        
        Args:
            df: DataFrame with price data
            current_price: Current price
            
        Returns:
            Price of the structural swing low or None if not found
        """
        try:
            # First try to find pivot lows using existing logic
            pivot_highs, pivot_lows = self._detect_pivots(df, left=5, right=5)
            swings = self._build_swings(df, pivot_highs, pivot_lows)
            
            # Filter for swing lows below current price
            swing_lows = [s for s in swings if s['type'] == 'L' and s['price'] < current_price]
            
            if swing_lows:
                # Return the most recent swing low
                return max(swing_lows, key=lambda s: s['idx'])['price']
            
            return None
        except Exception as e:
            print(f"Error finding structural swing low: {e}")
            return None

    def _find_structural_swing_high(self, df: pd.DataFrame, current_price: float) -> Optional[float]:
        """
        Find the most recent and structurally significant swing high above the current price.
        
        Args:
            df: DataFrame with price data
            current_price: Current price
            
        Returns:
            Price of the structural swing high or None if not found
        """
        try:
            # First try to find pivot highs using existing logic
            pivot_highs, pivot_lows = self._detect_pivots(df, left=5, right=5)
            swings = self._build_swings(df, pivot_highs, pivot_lows)
            
            # Filter for swing highs above current price
            swing_highs = [s for s in swings if s['type'] == 'H' and s['price'] > current_price]
            
            if swing_highs:
                # Return the most recent swing high
                return min(swing_highs, key=lambda s: s['idx'])['price']
            
            return None
        except Exception as e:
            print(f"Error finding structural swing high: {e}")
            return None

    def _cluster_targets(self, potential_targets: List[Dict[str, Any]], tolerance: float = 0.005) -> List[Dict[str, Any]]:
        """
        Cluster targets that are very close to each other and prioritize clusters with multiple sources.
        
        Args:
            potential_targets: List of potential target dictionaries
            tolerance: Tolerance for clustering (default 0.5%)
            
        Returns:
            List of clustered and prioritized targets
        """
        if not potential_targets:
            return []
        
        # Sort targets by price
        potential_targets.sort(key=lambda x: x['price'])
        
        # Cluster targets
        clusters = []
        current_cluster = [potential_targets[0]]
        
        for target in potential_targets[1:]:
            cluster_avg = sum(t['price'] for t in current_cluster) / len(current_cluster)
            if abs(target['price'] - cluster_avg) / cluster_avg <= tolerance:
                current_cluster.append(target)
            else:
                clusters.append(current_cluster)
                current_cluster = [target]
        
        # Don't forget the last cluster
        if current_cluster:
            clusters.append(current_cluster)
        
        # Create final targets from clusters
        final_targets = []
        for cluster in clusters:
            # Calculate average price for the cluster
            avg_price = sum(t['price'] for t in cluster) / len(cluster)
            
            # Count sources in cluster
            sources = set(t.get('source', '') for t in cluster)
            source_count = len([s for s in sources if s])  # Count non-empty sources
            
            # Create target with enhanced probability based on source count
            base_target = cluster[0].copy()
            base_target['price'] = avg_price
            
            # Boost probability for targets with multiple sources
            if source_count > 1:
                base_target['probability'] = min(95, base_target.get('probability', 70) + (source_count * 10))
                base_target['reason'] = f"{base_target.get('reason', '')} [Multi-source cluster]"
            
            final_targets.append(base_target)
        
        # Sort by probability descending
        final_targets.sort(key=lambda x: x.get('probability', 0), reverse=True)
        return final_targets



    def find_swing_low_before_index(self, swings: List[Dict], index: int) -> Optional[Dict]:
        """
        Find the most recent swing low that occurred before the given index.
        
        Args:
            swings: List of swing points
            index: The index to search before
            
        Returns:
            The swing low dictionary or None if not found
        """
        # Filter for swing lows before the given index
        swing_lows_before = [s for s in swings if s['type'] == 'L' and s['idx'] < index]
        
        # Return the most recent one (highest index)
        if swing_lows_before:
            return max(swing_lows_before, key=lambda s: s['idx'])
        return None

    def find_swing_high_before_index(self, swings: List[Dict], index: int) -> Optional[Dict]:
        """
        Find the most recent swing high that occurred before the given index.
        
        Args:
            swings: List of swing points
            index: The index to search before
            
        Returns:
            The swing high dictionary or None if not found
        """
        # Filter for swing highs before the given index
        swing_highs_before = [s for s in swings if s['type'] == 'H' and s['idx'] < index]
        
        # Return the most recent one (highest index)
        if swing_highs_before:
            return max(swing_highs_before, key=lambda s: s['idx'])
        return None

    def find_swing_low_after_index(self, swings: List[Dict], index: int) -> Optional[Dict]:
        """
        Find the first swing low that occurs after the given index.
        
        Args:
            swings: List of swing points
            index: The index to search after
            
        Returns:
            The swing low dictionary or None if not found
        """
        # Filter for swing lows after the given index
        swing_lows_after = [s for s in swings if s['type'] == 'L' and s['idx'] > index]
        
        # Return the earliest one (lowest index)
        if swing_lows_after:
            return min(swing_lows_after, key=lambda s: s['idx'])
        return None

    def find_swing_high_after_index(self, swings: List[Dict], index: int) -> Optional[Dict]:
        """
        Find the first swing high that occurs after the given index.
        
        Args:
            swings: List of swing points
            index: The index to search after
            
        Returns:
            The swing high dictionary or None if not found
        """
        # Filter for swing highs after the given index
        swing_highs_after = [s for s in swings if s['type'] == 'H' and s['idx'] > index]
        
        # Return the earliest one (lowest index)
        if swing_highs_after:
            return min(swing_highs_after, key=lambda s: s['idx'])
        return None

    def is_valid_setup(self, entry: float, stop_loss: float, targets: List[Dict]) -> bool:
        """
        Validate that a setup has proper risk-reward parameters.
        
        Args:
            entry: Entry price
            stop_loss: Stop loss price
            targets: List of target dictionaries
            
        Returns:
            True if setup is valid, False otherwise
        """
        if not targets:
            return False
            
        # Ensure there's a valid target with a 'price' key
        if 'price' not in targets[0]:
            return False
        first_target = targets[0]['price']
        
        # Calculate risk and reward
        risk = abs(entry - stop_loss)
        reward = abs(first_target - entry)
        
        # Ensure risk and reward are positive
        if risk <= 0 or reward <= 0:
            return False
            
        # Check for minimum 1.5 risk-reward ratio
        risk_reward_ratio = reward / risk
        return risk_reward_ratio >= 1.5

    
    def get_btc_dominance(self) -> float:
        """
        Fetch current Bitcoin dominance from CoinGecko
        
        Returns:
            Bitcoin dominance as a percentage
        """
        try:
            url = f"{self.alternative_base_url}/global"
            response = requests.get(url)
            data = response.json()
            
            if "data" in data and "market_cap_percentage" in data["data"]:
                btc_dominance = data["data"]["market_cap_percentage"]["btc"]
                return btc_dominance
            else:
                return 60.0  # Default if API fails
                
        except Exception as e:
            print(f"Error fetching BTC dominance: {str(e)}")
            return 60.0  # Default if API fails
    
    def calculate_correlation(self, symbol: str, end_date: datetime = None) -> float:
        """
        Calculate correlation between a symbol and Bitcoin
        
        Args:
            symbol: The symbol to calculate correlation for
            end_date: If specified, fetch data up to this date
            
        Returns:
            Correlation coefficient (-1 to 1)
        """
        try:
            if symbol == "BTC":
                return 1.0  # Perfect correlation with itself
            
            # Check if we already calculated this
            if symbol in self.correlation_matrix:
                return self.correlation_matrix[symbol]
            
            # Get data
            symbol_data = self.get_historical_data(symbol, "1d", 30, end_date=end_date)
            btc_data = self.get_historical_data("BTC", "1d", 30, end_date=end_date)
            
            if symbol_data is None or btc_data is None:
                return 0.5  # Default if data fetch fails
            
            # Calculate returns
            symbol_returns = symbol_data['close'].pct_change().dropna()
            btc_returns = btc_data['close'].pct_change().dropna()
            
            # Match lengths
            min_len = min(len(symbol_returns), len(btc_returns))
            symbol_returns = symbol_returns.tail(min_len)
            btc_returns = btc_returns.tail(min_len)
            
            # Calculate correlation
            correlation = symbol_returns.corr(btc_returns)
            
            # Handle NaN result
            if pd.isna(correlation):
                correlation = 0.5
            
            # Cache the result
            self.correlation_matrix[symbol] = correlation
            
            return correlation
            
        except Exception as e:
            print(f"Error calculating correlation for {symbol}: {str(e)}")
            return 0.5  # Default if calculation fails

    
    def update_correlations(self, symbols: List[str] = None) -> None:
        """
        Update correlation data for all symbols or a specific list
        
        Args:
            symbols: List of symbols to update (optional)
        """
        if symbols is None:
            symbols = self.symbols
        
        print(f"Updating correlation data for {len(symbols)} symbols...")
        
        # Get BTC dominance first
        self.btc_dominance = self.get_btc_dominance()
        print(f"BTC Dominance: {self.btc_dominance:.2f}%")
        
        # Update correlations
        for symbol in symbols:
            try:
                correlation = self.calculate_correlation(symbol)
                self.correlation_matrix[symbol] = correlation
                print(f"{symbol} correlation with BTC: {correlation:.2f}")
            except Exception as e:
                print(f"Error updating correlation for {symbol}: {str(e)}")
                self.correlation_matrix[symbol] = 0.5
        
        print("Correlation data updated.")
    
    def analyze_symbol_smc(self, symbol: str):
        """
        Performs a full Smart Money Concepts analysis, identifying market
        structure, zones, liquidity, and generating potential trade setups.
        """
        print(f"--- SMC Analysis for {symbol} ---")

        df = self.get_historical_data(symbol, '1d', limit=300)

        if df is None or len(df) < 50:
            print("Insufficient historical data for SMC analysis.")
            return

        # 1. Build Market Structure
        pivot_highs, pivot_lows = self._detect_pivots(df, left=5, right=5)
        swings = self._build_swings(df, pivot_highs, pivot_lows)

        if len(swings) < 4:
            print("Not enough confirmed swing points to build a reliable structure.")
            return

        # 2. Classify Structure and Detect Events
        structure, _ = self._classify_structure(swings)
        structural_events = self._detect_bos_choch(df, swings)
        print(f"Current Market Structure: {structure.upper()}")

        # 3. Detect Institutional Zones
        order_blocks = self._detect_order_blocks(df)
        fvgs = self._detect_fvgs(df)

        # 4. Identify Liquidity Pools
        swing_highs = [s for s in swings if s['type'] == 'H']
        swing_lows = [s for s in swings if s['type'] == 'L']
        liquidity_highs = self._cluster_equal_levels(swing_highs)
        liquidity_lows = self._cluster_equal_levels(swing_lows)

        if liquidity_highs:
            print("\nLiquidity Pools (Highs):")
            for pool in reversed(liquidity_highs[-3:]):
                print(f"  - Equal Highs around ${pool['level']:.4f}")
        if liquidity_lows:
            print("\nLiquidity Pools (Lows):")
            for pool in reversed(liquidity_lows[-3:]):
                print(f"  - Equal Lows around ${pool['level']:.4f}")

        # 5. Build and Display Trade Setups
        trade_setups = self._smc_trade_builder(df, swings, structural_events, order_blocks, fvgs, liquidity_highs, liquidity_lows)

        if not trade_setups:
            print("\nNo high-probability SMC setups found at the current price.")
        else:
            print("\n--- Potential SMC Trade Setups Found ---")
            for setup in trade_setups:
                print(f"\n  Signal:          {setup['side']} {symbol}")
                print(f"  Reason:          {setup['pattern']}")
                print(f"  Entry Price:     ${setup['entry']:.4f} (Current)")
                print(f"  Stop Loss:       ${setup['stop_loss']:.4f}")
                if setup['targets']:
                    print(f"  Target 1:        ${setup['targets'][0]['price']:.4f} ({setup['targets'][0]['reason']})")
                    risk_reward = (abs(setup['targets'][0]['price'] - setup['entry'])) / (abs(setup['entry'] - setup['stop_loss']))
                    print(f"  Risk/Reward:     ~1:{risk_reward:.2f}")

        print("\n--- End of SMC Analysis ---")

    def _detect_pivots(self, df: pd.DataFrame, left: int = 5, right: int = 5) -> Tuple[List[bool], List[bool]]:
        """
        Detects pivot highs and lows in the dataframe. A pivot high is a candle's high
        that is greater than the highs of 'left' candles before and 'right' candles after.
        A pivot low is the opposite.

        Args:
            df: DataFrame with OHLC data.
            left: Number of bars to the left to check.
            right: Number of bars to the right to check.

        Returns:
            A tuple of two boolean lists: (pivot_highs, pivot_lows).
        """
        highs = df['high'].values
        lows = df['low'].values
        n = len(df)

        pivot_highs = [False] * n
        pivot_lows = [False] * n

        for i in range(left, n - right):
            is_pivot_high = highs[i] == max(highs[i - left : i + right + 1])
            if is_pivot_high:
                # Ensure it's the first occurrence if multiple bars have the same high
                if highs[i-1] < highs[i]:
                    pivot_highs[i] = True

            is_pivot_low = lows[i] == min(lows[i - left : i + right + 1])
            if is_pivot_low:
                # Ensure it's the first occurrence if multiple bars have the same low
                if lows[i-1] > lows[i]:
                    pivot_lows[i] = True

        return pivot_highs, pivot_lows

    def _build_swings(self, df: pd.DataFrame, pivot_highs: List[bool], pivot_lows: List[bool], zz_threshold: float = 0.015) -> List[Dict]:
        """
        Builds a confirmed list of swing points from raw pivots, filtering out minor moves.
        This creates a clean "zigzag" line representing market structure.

        Args:
            df: DataFrame with OHLC data.
            pivot_highs: Boolean list of pivot highs from _detect_pivots.
            pivot_lows: Boolean list of pivot lows from _detect_pivots.
            zz_threshold: The minimum percentage move required to confirm a new swing point.

        Returns:
            A list of dictionaries, where each dictionary represents a confirmed swing point.
        """
        swings = []
        last_swing = None

        for i in range(len(df)):
            current_point = None
            if pivot_highs[i]:
                current_point = {'idx': i, 'type': 'H', 'price': float(df['high'].iloc[i])}
            elif pivot_lows[i]:
                current_point = {'idx': i, 'type': 'L', 'price': float(df['low'].iloc[i])}

            if current_point is None:
                continue

            if not last_swing:
                swings.append(current_point)
                last_swing = current_point
                continue

            # If the new point is of a different type (e.g., a high after a low)
            if current_point['type'] != last_swing['type']:
                move_pct = abs(current_point['price'] - last_swing['price']) / last_swing['price']
                if move_pct >= zz_threshold:
                    swings.append(current_point)
                    last_swing = current_point
            # If the new point is of the same type (e.g., a new high after a previous high)
            else:
                is_better_swing = (current_point['type'] == 'H' and current_point['price'] > last_swing['price']) or \
                                  (current_point['type'] == 'L' and current_point['price'] < last_swing['price'])
                if is_better_swing:
                    swings[-1] = current_point  # Replace the last swing with the better one
                    last_swing = current_point

        return swings

    def _classify_structure(self, swings: List[Dict], min_points: int = 4) -> Tuple[str, List[Dict]]:
        """
        Classifies the current market structure (bullish, bearish, or range)
        based on the sequence of the last few swing points.

        Args:
            swings: A list of confirmed swing points from _build_swings.
            min_points: The minimum number of swings required to make a determination.

        Returns:
            A tuple containing the structure type ('bull', 'bear', 'range') and the last
            four swing points used for the analysis.
        """
        if len(swings) < min_points:
            return 'range', []

        # Analyze the last 4 swing points to define the most recent structure
        last_4_swings = swings[-4:]
        highs = [s for s in last_4_swings if s['type'] == 'H']
        lows = [s for s in last_4_swings if s['type'] == 'L']

        if len(highs) >= 2 and len(lows) >= 2:
            is_bullish = highs[-1]['price'] > highs[-2]['price'] and lows[-1]['price'] > lows[-2]['price']
            is_bearish = highs[-1]['price'] < highs[-2]['price'] and lows[-1]['price'] < lows[-2]['price']

            if is_bullish:
                return 'bull', last_4_swings
            if is_bearish:
                return 'bear', last_4_swings

        return 'range', last_4_swings


    def _is_impulsive_candle(self, df: pd.DataFrame, i: int, lookback: int = 5, body_mult: float = 1.2, range_mult: float = 1.1) -> bool:
        """
        Determines if a candle at index 'i' is impulsive (shows displacement) compared
        to the average of previous candles.

        Args:
            df: DataFrame with OHLC data.
            i: The index of the candle to check.
            lookback: How many previous candles to average for comparison.
            body_mult: The multiplier for how much larger the candle's body must be.
            range_mult: The multiplier for how much larger the candle's range must be.

        Returns:
            True if the candle is considered impulsive, False otherwise.
        """
        if i < lookback:
            return False

        body = abs(df['close'].iloc[i] - df['open'].iloc[i])
        candle_range = df['high'].iloc[i] - df['low'].iloc[i]

        # Calculate average body and range of the lookback period
        prev_candles = df.iloc[i - lookback : i]
        avg_body = (abs(prev_candles['close'] - prev_candles['open'])).mean()
        avg_range = (prev_candles['high'] - prev_candles['low']).mean()

        # Avoid division by zero if average body/range is 0
        if avg_body == 0 or avg_range == 0:
            return False

        return (body > body_mult * avg_body) and (candle_range > range_mult * avg_range)

    def _detect_bos_choch(self, df: pd.DataFrame, swings: List[Dict], close_break: bool = True) -> List[Dict]:
        """
        Detects Breaks of Structure (BOS) and the first Change of Character (CHoCH).

        Args:
            df: DataFrame with OHLC data.
            swings: A list of confirmed swing points.
            close_break: If True, requires a candle close beyond the level for a valid break.

        Returns:
            A list of dictionaries, where each dictionary represents a structural event.
        """
        events = []
        if len(swings) < 3:
            return events

        # First, determine the overall trend from the full swing list
        all_highs = [s['price'] for s in swings if s['type'] == 'H']
        all_lows = [s['price'] for s in swings if s['type'] == 'L']

        overall_trend = 'range'
        if len(all_highs) > 2 and len(all_lows) > 2:
            if all_highs[-1] > all_highs[-2] and all_lows[-1] > all_lows[-2]:
                overall_trend = 'bull'
            elif all_highs[-1] < all_highs[-2] and all_lows[-1] < all_lows[-2]:
                overall_trend = 'bear'

        choch_detected = {'up': False, 'down': False}

        for k in range(2, len(swings)):
            # We are checking if the move from swings[k-1] to swings[k] broke a prior structure point
            prior_structure_swing = swings[k-2]

            # Potential Bullish Break (new high breaks prior high)
            if swings[k]['type'] == 'H' and prior_structure_swing['type'] == 'H':
                level_to_break = prior_structure_swing['price']

                # Check candles between the old high and the new high
                for t in range(prior_structure_swing['idx'] + 1, swings[k]['idx'] + 1):
                    broke = df['close'].iloc[t] > level_to_break if close_break else df['high'].iloc[t] > level_to_break
                    if broke:
                        event_type = 'BOS' if overall_trend == 'bull' else 'CHoCH'
                        if event_type == 'CHoCH' and choch_detected['up']: event_type = 'BOS' # Can only be CHoCH once

                        events.append({'type': event_type, 'dir': 'up', 'level': level_to_break, 'idx': t})
                        if event_type == 'CHoCH': choch_detected['up'] = True
                        break

            # Potential Bearish Break (new low breaks prior low)
            elif swings[k]['type'] == 'L' and prior_structure_swing['type'] == 'L':
                level_to_break = prior_structure_swing['price']

                # Check candles between the old low and the new low
                for t in range(prior_structure_swing['idx'] + 1, swings[k]['idx'] + 1):
                    broke = df['close'].iloc[t] < level_to_break if close_break else df['low'].iloc[t] < level_to_break
                    if broke:
                        event_type = 'BOS' if overall_trend == 'bear' else 'CHoCH'
                        if event_type == 'CHoCH' and choch_detected['down']: event_type = 'BOS'

                        events.append({'type': event_type, 'dir': 'down', 'level': level_to_break, 'idx': t})
                        if event_type == 'CHoCH': choch_detected['down'] = True
                        break

        return events

    def _detect_order_blocks(self, df: pd.DataFrame) -> List[Dict]:
        """
        Detects bullish and bearish Order Blocks (OBs). An OB is the last opposing
        candle(s) before a strong, impulsive move.

        Args:
            df: DataFrame with OHLCV data.

        Returns:
            A list of dictionaries, each representing a detected Order Block zone.
        """
        zones = []
        for i in range(1, len(df)):
            if not self._is_impulsive_candle(df, i):
                continue

            is_bullish_impulse = df['close'].iloc[i] > df['open'].iloc[i]

            # Find the last opposing candle before the impulse
            if is_bullish_impulse:
                # Look for the last bearish candle before the bullish impulse
                if df['close'].iloc[i-1] < df['open'].iloc[i-1]:
                    ob = {
                        'type': 'bull',
                        'idx': i - 1,
                        'low': df['low'].iloc[i-1],
                        'high': df['high'].iloc[i-1]
                    }
                    zones.append(ob)
            else: # Bearish impulse
                # Look for the last bullish candle before the bearish impulse
                if df['close'].iloc[i-1] > df['open'].iloc[i-1]:
                    ob = {
                        'type': 'bear',
                        'idx': i - 1,
                        'low': df['low'].iloc[i-1],
                        'high': df['high'].iloc[i-1]
                    }
                    zones.append(ob)
        return zones

    def _detect_fvgs(self, df: pd.DataFrame) -> List[Dict]:
        """
        Detects Fair Value Gaps (FVGs) or imbalances. An FVG is a three-candle
        pattern where there is a gap between the first candle's high and the third candle's low (or vice-versa).

        Args:
            df: DataFrame with OHLC data.

        Returns:
            A list of dictionaries, each representing a detected FVG.
        """
        fvgs = []
        for i in range(1, len(df) - 1):
            prev_high = df['high'].iloc[i-1]
            next_low = df['low'].iloc[i+1]

            # Bullish FVG (gap between prev high and next low)
            if next_low > prev_high:
                fvgs.append({
                    'type': 'bull',
                    'idx': i,
                    'low': prev_high,
                    'high': next_low
                })

            prev_low = df['low'].iloc[i-1]
            next_high = df['high'].iloc[i+1]

            # Bearish FVG (gap between prev low and next high)
            if next_high < prev_low:
                fvgs.append({
                    'type': 'bear',
                    'idx': i,
                    'low': next_high,
                    'high': prev_low
                })

        return fvgs

    def _detect_liquidity_sweep(self, df: pd.DataFrame, liquidity_pools: List[Dict]) -> Union[Dict, None]:
        """
        Checks if the most recent candle has swept a liquidity pool.
        A sweep is defined as price trading just beyond a pool and then closing back inside the range.

        Args:
            df: The OHLC DataFrame.
            liquidity_pools: A list of identified liquidity pools (highs or lows).

        Returns:
            A dictionary describing the sweep if one occurred, otherwise None.
        """
        if not liquidity_pools or len(df) < 2:
            return None

        last_candle = df.iloc[-1]

        for pool in liquidity_pools:
            level = pool['level']

            # Check for a sweep of a high liquidity pool (bearish signal)
            if last_candle['high'] > level and last_candle['close'] < level:
                # Price went above the highs but failed to stay there
                return {'type': 'sweep_high', 'level': level, 'side': 'SHORT'}

            # Check for a sweep of a low liquidity pool (bullish signal)
            if last_candle['low'] < level and last_candle['close'] > level:
                # Price went below the lows but was bought back up
                return {'type': 'sweep_low', 'level': level, 'side': 'LONG'}

        return None

    def _generate_smc_targets(self, direction: str, entry_price: float, stop_loss: float, swing_leg_high: float, swing_leg_low: float, bos_level: float, liquidity_highs: List[Dict], liquidity_lows: List[Dict]) -> List[Dict]:
        """
        Builds a prioritized list of potential trade targets for SMC setups based on a clear hierarchy.
        """
        potential_targets = []

        # PRIORITY 1: LIQUIDITY POOLS (Nearest first)
        if direction == "LONG":
            pools = sorted([p for p in liquidity_highs if p['level'] > entry_price], key=lambda p: p['level'])
            for i, pool in enumerate(pools):
                potential_targets.append({
                    "price": pool['level'], "reason": "Liquidity Pool", "probability": max(70, 85 - i*5),
                    "percent": ((pool['level'] / entry_price) - 1) * 100 if entry_price > 0 else 0
                })
        else:  # SHORT
            pools = sorted([p for p in liquidity_lows if p['level'] < entry_price], key=lambda p: p['level'], reverse=True)
            for i, pool in enumerate(pools):
                potential_targets.append({
                    "price": pool['level'], "reason": "Liquidity Pool", "probability": max(70, 85 - i*5),
                    "percent": ((pool['level'] / entry_price) - 1) * 100 if entry_price > 0 else 0
                })

        # PRIORITY 2: BREAK OF STRUCTURE (BOS) LEVEL
        potential_targets.append({
            "price": bos_level, "reason": "Break of Structure Level", "probability": 75,
            "percent": ((bos_level / entry_price) - 1) * 100 if entry_price > 0 else 0
        })

        # PRIORITY 3: EXTERNAL RANGE LIQUIDITY
        external_target = swing_leg_high if direction == "LONG" else swing_leg_low
        potential_targets.append({
            "price": external_target, "reason": "External Range Liquidity", "probability": 70,
            "percent": ((external_target / entry_price) - 1) * 100 if entry_price > 0 else 0
        })

        # PRIORITY 4: FIBONACCI EXTENSIONS (if needed)
        if len(potential_targets) < 10:
            swing_range = swing_leg_high - swing_leg_low
            if swing_range > 0:
                fib_ratios = [1.272, 1.618, 2.0]
                for i, ratio in enumerate(fib_ratios):
                    if direction == "LONG":
                        target_price = swing_leg_high + (swing_range * (ratio - 1))
                    else:
                        target_price = swing_leg_low - (swing_range * (ratio - 1))
                    potential_targets.append({
                        "price": target_price, "reason": f"{ratio} Fib Extension", "probability": 65 - i*5,
                        "percent": ((target_price / entry_price) - 1) * 100 if entry_price > 0 else 0
                    })

        # FINALIZE: Remove duplicates, sort, and filter for profitability
        unique_targets = []
        seen_prices = set()
        for target in potential_targets:
            is_duplicate = False
            for seen_price in seen_prices:
                if abs(target['price'] - seen_price) / seen_price < 0.01: # 1% tolerance for duplicates
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_targets.append(target)
                seen_prices.add(target['price'])

        # Filter for profitable targets and sort by price
        if direction == "LONG":
            profitable_targets = sorted([t for t in unique_targets if t['price'] > entry_price], key=lambda x: x['price'])
        else: # SHORT
            profitable_targets = sorted([t for t in unique_targets if t['price'] < entry_price], key=lambda x: x['price'], reverse=True)

        return profitable_targets[:10]

    def _smc_trade_builder(self, df: pd.DataFrame, swings: List[Dict], structural_events: List[Dict], order_blocks: List[Dict], fvgs: List[Dict], liquidity_highs: List[Dict], liquidity_lows: List[Dict]) -> List[Dict]:
        """
        Overhauled SMC trade builder that explicitly follows the institutional trading model.
        Identifies the operative trend, defines trading range, calculates equilibrium,
        isolates high-probability zones, and generates setups based on price mitigation.
        """
        setups = []
        # Focus only on the most recent structural events (e.g., in the last 90 candles)
        recent_events = [e for e in structural_events if e['idx'] > len(df) - 90]

        # Find the LAST confirmed Break of Structure
        last_bos = next((e for e in reversed(recent_events) if e['type'] == 'BOS'), None)
        if not last_bos:
            return [] # No trend to follow

        # --- Case 1: Bullish Trend Continuation (BOS Up) ---
        if last_bos['dir'] == 'up':
            # Find the swing leg that created the BOS
            origin_swing_low = self.find_swing_low_before_index(swings, last_bos['idx'])
            swing_high_for_range = self.find_swing_high_after_index(swings, last_bos['idx'])
            if not origin_swing_low or not swing_high_for_range: 
                return []
            
            # Define the Premium/Discount Array
            equilibrium = origin_swing_low['price'] + (swing_high_for_range['price'] - origin_swing_low['price']) * 0.5

            # Find valid zones in the Discount Array
            discount_zones = [z for z in order_blocks + fvgs if z['type'] == 'bull' and origin_swing_low['idx'] <= z['idx'] < swing_high_for_range['idx'] and z['high'] <= equilibrium]
            
            # Check if current price is mitigating a discount zone
            for zone in reversed(sorted(discount_zones, key=lambda z: z['low'])):
                if zone['low'] <= df['close'].iloc[-1] <= zone['high']:
                    # SETUP FOUND
                    entry = df['close'].iloc[-1]
                    stop_loss = origin_swing_low['price'] * 0.995 # Place SL below the structural low
                    
                    # Prioritize liquidity pools above as targets
                    targets = self._generate_smc_targets(
                        direction="LONG",
                        entry_price=entry,
                        stop_loss=stop_loss,
                        swing_leg_high=swing_high_for_range['price'],
                        swing_leg_low=origin_swing_low['price'],
                        bos_level=last_bos['level'],
                        liquidity_highs=liquidity_highs,
                        liquidity_lows=liquidity_lows
                    )

                    if self.is_valid_setup(entry, stop_loss, targets):
                        setups.append({
                            'side': 'LONG', 
                            'entry': entry, 
                            'stop_loss': stop_loss, 
                            'targets': targets, 
                            'pattern': 'SMC Discount Mitigation',
                            'bos_event': last_bos
                        })
                    break # Only take the first mitigated zone

        # --- Case 2: Bearish Trend Continuation (BOS Down) ---
        elif last_bos['dir'] == 'down':
            # Find the swing leg that created the BOS
            origin_swing_high = self.find_swing_high_before_index(swings, last_bos['idx'])
            swing_low_for_range = self.find_swing_low_after_index(swings, last_bos['idx'])
            if not origin_swing_high or not swing_low_for_range: 
                return []
            
            # Define the Premium/Discount Array
            equilibrium = swing_low_for_range['price'] + (origin_swing_high['price'] - swing_low_for_range['price']) * 0.5
            
            # Find valid zones in the Premium Array
            premium_zones = [z for z in order_blocks + fvgs if z['type'] == 'bear' and origin_swing_high['idx'] <= z['idx'] < swing_low_for_range['idx'] and z['low'] >= equilibrium]
            
            # Check for mitigation
            for zone in sorted(premium_zones, key=lambda z: z['high']):
                if zone['low'] <= df['close'].iloc[-1] <= zone['high']:
                    # SETUP FOUND
                    entry = df['close'].iloc[-1]
                    stop_loss = origin_swing_high['price'] * 1.005 # Place SL above the structural high
                    
                    # Prioritize liquidity pools below as targets
                    targets = self._generate_smc_targets(
                        direction="SHORT",
                        entry_price=entry,
                        stop_loss=stop_loss,
                        swing_leg_high=origin_swing_high['price'],
                        swing_leg_low=swing_low_for_range['price'],
                        bos_level=last_bos['level'],
                        liquidity_highs=liquidity_highs,
                        liquidity_lows=liquidity_lows
                    )
                    
                    if self.is_valid_setup(entry, stop_loss, targets):
                        setups.append({
                            'side': 'SHORT',
                            'entry': entry,
                            'stop_loss': stop_loss,
                            'targets': targets,
                            'pattern': 'SMC Premium Mitigation',
                            'bos_event': last_bos
                        })
                    break
                    
        return setups

    def _cluster_equal_levels(self, points: List[Dict], tol_ratio: float = 0.0015) -> List[Dict]:
        """
        Clusters a list of swing points (highs or lows) that are very close in price,
        identifying potential liquidity pools.

        Args:
            points: A list of swing point dictionaries (e.g., [{'idx':..., 'type':'H', 'price':...}]).
            tol_ratio: The tolerance percentage for considering prices "equal".

        Returns:
            A list of dictionaries, each representing a liquidity pool with its average level
            and the swing points it contains.
        """
        if not points:
            return []

        pts = sorted(points, key=lambda x: x['price'])
        clusters = []
        if not pts:
            return []

        current_cluster = [pts[0]]
        for p in pts[1:]:
            # Check if the current point is close enough to the last point in the cluster
            if abs(p['price'] - current_cluster[-1]['price']) <= tol_ratio * current_cluster[-1]['price']:
                current_cluster.append(p)
            else:
                if len(current_cluster) >= 2:
                    clusters.append(list(current_cluster))
                current_cluster = [p]

        if len(current_cluster) >= 2:
            clusters.append(current_cluster)

        pools = [{
            'level': sum(x['price'] for x in c) / len(c),
            'members': c
        } for c in clusters]

        return pools

    def _is_data_clean(self, df: pd.DataFrame, symbol: str, verbose: bool = False) -> bool:
        """
        Performs several checks to ensure the quality and integrity of the fetched data.

        Args:
            df: The DataFrame to check.
            symbol: The symbol name for logging purposes.
            verbose: If True, print detailed reasons for failure.

        Returns:
            True if the data is clean, False otherwise.
        """
        if df is None or df.empty:
            if verbose: print(f"  - Data Integrity Check FAILED for {symbol}: DataFrame is empty.")
            return False

        # 1. Check for missing or zero close prices
        total_rows = len(df)
        missing_or_zero = df['close'].isnull().sum() + (df['close'] == 0).sum()
        if (missing_or_zero / total_rows) > 0.05:
            if verbose: print(f"  - Data Integrity Check FAILED for {symbol}: >5% rows have null or zero close price.")
            return False

        # 2. Check for flat price data
        if df['close'].std() == 0:
            if verbose: print(f"  - Data Integrity Check FAILED for {symbol}: Price data is flat (zero standard deviation).")
            return False

        # 3. Check for excessive single-candle price jumps
        max_daily_change = (df['high'] / df['low'] - 1).max() * 100
        if max_daily_change > 75: # Allow up to a 75% wick-to-wick change in a single candle
            if verbose: print(f"  - Data Integrity Check FAILED for {symbol}: Excessive single-candle price change detected ({max_daily_change:.2f}%).")
            return False

        return True

    def _evaluate_bos_clarity(self, df: pd.DataFrame, bos_event: Dict, lookback: int = 10) -> float:
        """
        Scores the clarity of a Break of Structure based on the breakout candle's
        body size and volume compared to the recent past.

        Returns a score from 0.0 to 1.0.
        """
        i = bos_event['idx']
        if i < lookback: 
            return 0.3

        body = abs(df['close'].iloc[i] - df['open'].iloc[i])
        avg_body = (abs(df['close'].iloc[i-lookback:i] - df['open'].iloc[i-lookback:i])).mean()

        vol = df['volume'].iloc[i]
        avg_vol = df['volume'].iloc[i-lookback:i].mean()

        body_score = min(body / max(avg_body, 1e-9), 2.0) / 2.0  # Cap at 2x avg
        vol_score = min(vol / max(avg_vol, 1e-9), 3.0) / 3.0    # Cap at 3x avg

        return (0.6 * body_score) + (0.4 * vol_score)

    def _evaluate_zone_freshness(self, df: pd.DataFrame, zone: Dict, swings: List[Dict]) -> int:
        """
        Evaluates the freshness of an Order Block or FVG by counting how many times
        price has tapped into it since it was formed.

        Returns the number of taps (0 for a fresh, "virgin" zone).
        """
        taps = 0
        zone_start_idx = zone.get('idx', 0)

        # Define the price range of the zone
        zone_high = zone['high']
        zone_low = zone['low']

        # Check all candles after the zone was formed
        for i in range(zone_start_idx + 1, len(df)):
            candle_high = df['high'].iloc[i]
            candle_low = df['low'].iloc[i]
            # A tap occurs if the candle's wick or body enters the zone
            if max(candle_low, zone_low) <= min(candle_high, zone_high):
                taps += 1
        return taps

    def _score_smc_setup(self, df: pd.DataFrame, setup: Dict, swings: List[Dict], bos_event: Dict) -> float:
        """
        Calculates a final confluence score for a given SMC setup with enhanced scoring factors.
        
        Scoring Factors:
        - BOS Clarity (+20 max): How impulsive was the break of structure?
        - Zone Freshness (+15 / -10): Is this the first time price has returned to the zone?
        - Discount/Premium Alignment (+10): Is the zone correctly in the discount/premium array?
        - FVG Confluence (+10): Is there overlapping OB/FVG confluence?
        
        Returns a quality score float from 0-100.
        """
        score = 50.0  # Start with a base score
        
        # 1. Score BOS Clarity (+0 to +20 points)
        bos_clarity_score = self._evaluate_bos_clarity(df, bos_event)
        score += bos_clarity_score * 20

        # 2. Score Zone Freshness (+15 for fresh, -10 for used)
        # Find the zone that was mitigated
        entry_price = setup['entry']
        mitigated_zone = None
        
        # Get all zones for this setup's side
        all_zones = []
        if setup['side'] == 'LONG':
            # For longs, look for bullish zones
            all_zones = [z for z in self._detect_order_blocks(df) + self._detect_fvgs(df) if z['type'] == 'bull']
        else:  # SHORT
            # For shorts, look for bearish zones
            all_zones = [z for z in self._detect_order_blocks(df) + self._detect_fvgs(df) if z['type'] == 'bear']
        
        # Find the zone that contains the entry price
        for zone in all_zones:
            if zone['low'] <= entry_price <= zone['high']:
                mitigated_zone = zone
                break
        
        if mitigated_zone:
            taps = self._evaluate_zone_freshness(df, mitigated_zone, swings)
            if taps == 0:
                score += 15  # Virgin zone bonus
            else:
                score -= min(taps * 10, 30)  # Penalize for each previous tap, cap at -30

        # 3. Score Discount/Premium alignment (+10 points)
        pattern_name = setup.get('pattern', '')
        if "Discount" in pattern_name or "Premium" in pattern_name:
            score += 10

        # 4. Check for FVG Confluence (+10 points)
        # This check is for an FVG *inside* an Order Block, a strong confluence.
        if mitigated_zone and 'type' in mitigated_zone:
            # Check if an FVG exists that overlaps with the identified zone
            all_fvgs = self._detect_fvgs(df)
            overlapping_fvgs = [
                fvg for fvg in all_fvgs
                if fvg['type'] == mitigated_zone['type'] and 
                max(mitigated_zone['low'], fvg['low']) < min(mitigated_zone['high'], fvg['high'])
            ]
            if overlapping_fvgs:
                score += 10

        return min(100, max(0, score))


    def analyze_single_symbol_smc(self, symbol: str, end_date: datetime = None) -> Dict[str, Any]:
        """
        Performs a full SMC analysis on a single symbol and returns a structured
        result dictionary suitable for signal generation and saving.
        """
        df = self.get_historical_data(symbol, '1d', limit=300, end_date=end_date)

        # Data Integrity Check
        if not self._is_data_clean(df, symbol, verbose=True): # Always verbose for single analysis
            return {"symbol": symbol, "error": "Data integrity check failed."}

        if df is None or len(df) < 50:
            return {"symbol": symbol, "error": "Insufficient data for SMC analysis"}

        # 1. Generate all SMC components
        pivot_highs, pivot_lows = self._detect_pivots(df, left=5, right=5)
        swings = self._build_swings(df, pivot_highs, pivot_lows)
        if len(swings) < 4:
            return {"symbol": symbol, "error": "Not enough swing points for SMC analysis"}

        structural_events = self._detect_bos_choch(df, swings)
        order_blocks = self._detect_order_blocks(df)
        fvgs = self._detect_fvgs(df)
        swing_highs = [s for s in swings if s['type'] == 'H']
        swing_lows = [s for s in swings if s['type'] == 'L']
        liquidity_highs = self._cluster_equal_levels(swing_highs)
        liquidity_lows = self._cluster_equal_levels(swing_lows)

        # 2. Look for Trade Setups (including new Sweep & Shift)
        trade_setups = self._smc_trade_builder(df, swings, structural_events, order_blocks, fvgs, liquidity_highs, liquidity_lows)

        # Check for a liquidity sweep setup
        sweep_high_setup = self._detect_liquidity_sweep(df, liquidity_highs)
        if sweep_high_setup:
             stop_loss = df['high'].iloc[-1] * 1.005
             target = next((pool['level'] for pool in sorted(liquidity_lows, key=lambda x: x['level'], reverse=True) if pool['level'] < df['close'].iloc[-1]), None)
             if target:
                trade_setups.append({'side': 'SHORT', 'entry': df['close'].iloc[-1], 'stop_loss': stop_loss, 'targets': [{'price': target, 'reason': 'Opposing Liquidity', 'probability': 75}], 'pattern': 'SMC Liquidity Sweep High'})

        sweep_low_setup = self._detect_liquidity_sweep(df, liquidity_lows)
        if sweep_low_setup:
            stop_loss = df['low'].iloc[-1] * 0.995
            target = next((pool['level'] for pool in sorted(liquidity_highs, key=lambda x: x['level']) if pool['level'] > df['close'].iloc[-1]), None)
            if target:
                trade_setups.append({'side': 'LONG', 'entry': df['close'].iloc[-1], 'stop_loss': stop_loss, 'targets': [{'price': target, 'reason': 'Opposing Liquidity', 'probability': 75}], 'pattern': 'SMC Liquidity Sweep Low'})

        if not trade_setups:
            return {"symbol": symbol, "error": "No SMC setup found"}

        # 3. Score and select the best setup
        best_setup = None
        highest_score = -1
        for setup in trade_setups:
            bos_event = next((e for e in structural_events if setup.get('targets') and e['level'] == setup['targets'][0]['price']), {'idx': len(df)-1, 'level': 0})
            score = self._score_smc_setup(df, setup, swings, bos_event)
            if score > highest_score:
                highest_score = score
                best_setup = setup

        if highest_score < 60: # Minimum threshold
             return {"symbol": symbol, "error": "No high-quality SMC setup found"}

        # 4. Format and return the result
        result = {
            "symbol": symbol,
            "timestamp": (end_date or datetime.now(pytz.timezone("America/Vancouver"))).strftime("%Y-%m-%d %H:%M:%S"),
            "detected_pattern": best_setup['pattern'],
            "detected_strategies": [best_setup['pattern']],
            "confidence": highest_score,
            "quality_score": highest_score,
            "direction": best_setup['side'],
            "current_price": best_setup['entry'],
            "entry_range": [best_setup['entry'] * 0.995, best_setup['entry'] * 1.005],
            "optimal_entry": best_setup['entry'],
            "targets": best_setup['targets'],
            "stop_loss": best_setup['stop_loss'],
            "trends": {}, "dow_phase": "N/A"
        }
        return result

    def analyze_single_symbol(self, symbol: str, end_date: datetime = None) -> Dict[str, Any]:
        """
        Complete analysis of a single symbol with improved strategy detection
        
        Args:
            symbol: The crypto symbol to analyze
            end_date: If specified, analyze data up to this date
            
        Returns:
            Dictionary with analysis results
        """
        try:
            print(f"Analyzing {symbol}...")
            
            # Get data for different timeframes
            timeframes = ["1h", "4h", "1d", "1w"]
            dataframes = {}
            
            for tf in timeframes:
                df = self.get_historical_data(symbol, tf, end_date=end_date)
                if df is not None and len(df) >= 30:
                    dataframes[tf] = self.calculate_technical_indicators(df)
                else:
                    print(f"Insufficient data for {symbol} on {tf} timeframe")
            
            # If we couldn't get data for any timeframe, return error
            if not dataframes:
                return {
                    "symbol": symbol,
                    "error": "Insufficient data",
                    "timestamp": datetime.now(pytz.timezone("America/Vancouver")).strftime("%Y-%m-%d %H:%M:%S")
                }
            
            # Update BTC trend data if not already present
            if self.btc_trend is None or symbol == "BTC":
                self.update_btc_trend()
            
            # Use daily data for support/resistance and pattern detection (or fallback to available data)
            main_tf = "1d" if "1d" in dataframes else list(dataframes.keys())[0]
            main_df = dataframes[main_tf]
            
            # Get current price - use hourly data for precise price at exact past time if end_date is provided
            if end_date:
                # Fetch hourly data to get price at exact time
                hourly_df = self.get_historical_data(symbol, "1h", end_date=end_date)
                if hourly_df is not None and not hourly_df.empty:
                    current_price = hourly_df['close'].iloc[-1]  # Last close up to the exact time
                else:
                    current_price = main_df['close'].iloc[-1]  # Fallback to daily
            else:
                current_price = main_df['close'].iloc[-1]
            
            # Calculate support and resistance levels
            support_levels_with_strength, resistance_levels_with_strength = self.identify_support_resistance(dataframes)
            
            # Check for all 27 trading strategies
            strategy_results = []
            strategies = [
                "Pullback (1D) in Uptrend",
                "Pullback (1W) in Uptrend",
                "Momentum & UpTrend (v.1)",
                "Momentum & UpTrend (v.2)",
                "Fresh Bullish Momentum (MACD Signal Line) Crossover",
                "Early Bullish Momentum (MACD Histogram) Inflection",
                "UpTrend and Fresh Momentum Inflection",
                "Bullish Momentum with RSI confirmation (v.1)",
                "Bullish Momentum with RSI confirmation (v.2)",
                "Bullish EMA 12/50 crossovers",
                "Strong UpTrend",
                "UpTrend",
                "Short-Term Trend Upgrade",
                "Very Oversold",
                "Oversold in UpTrend",
                "Oversold with Momentum Shift",
                "New Local High",  
                "New Local Low",  
                "Bullish Trading in Range (v.1)",
                "Bullish Trading in Range (v.2)",
                "Within 5% of ATH",
                "Within 5% of ATH and not very overbought",
                "Within 5% of ATH and recent bullish MACD crossover",
                "Within 5% of ATH and bullish inflection in MACD Histogram",
                "Recent ATH, pulled back but MACD is starting to inflect bullish",
                "Recent ATH, still within 10% of ATH, and not very overbought",
                "Recent ATH"
            ]
            
            for strategy_name in strategies:
                result = self._detect_strategy_block(main_df, strategy_name, dataframes, 
                                                support_levels_with_strength, resistance_levels_with_strength)
                if result["detected"]:
                    strategy_results.append(result)
            
            # Also detect chart patterns
            pattern_result = self.detect_chart_patterns(main_df, support_levels_with_strength, resistance_levels_with_strength, dataframes)
            
            # Check for divergence using the superior, confirmed method
            divergence_result = self._detect_divergence_signal(main_df, "Divergence Signal", dataframes)
            
            # Add these to our results if detected
            if pattern_result.get("detected", False):
                strategy_results.append(pattern_result)
                
            if divergence_result.get("pattern", "no_divergence") != "no_divergence":
                strategy_results.append({
                    "detected": True,
                    "pattern": divergence_result["pattern"],
                    "confidence": divergence_result["confidence"],
                    "direction": divergence_result["direction"]
                })
            
            # If no strategies were detected, use the most general one
            if not strategy_results:
                # Analyze trends for different timeframes
                trends = {}
                for tf, df_item in dataframes.items():
                    trends[tf] = self.analyze_trend(df_item, tf)
                    
                direction = "NEUTRAL"
                # Determine a default direction based on trend analysis
                if trends.get("1d", {}).get("trend", "") == "BULLISH":
                    direction = "LONG"
                elif trends.get("1d", {}).get("trend", "") == "BEARISH":
                    direction = "SHORT"
                    
                strategy_results.append({
                    "detected": True,
                    "pattern": "General Trend Analysis",
                    "confidence": 50,  # Lower confidence for generic analysis
                    "direction": direction
                })
            
            # Sort results by confidence and take the highest
            strategy_results.sort(key=lambda x: x["confidence"], reverse=True)
            best_strategy = strategy_results[0]
            
            # Extract information from the best strategy
            pattern = best_strategy["pattern"]
            pattern_confidence = best_strategy["confidence"]
            direction = best_strategy["direction"]
            
            # Analyze trends for different timeframes
            trends = {}
            for tf, df_item in dataframes.items():
                trends[tf] = self.analyze_trend(df_item, tf)
            
            # Check for BTC correlation
            btc_correlation = self.calculate_correlation(symbol, end_date=end_date)
            
            # Check BTC dominance (with a fallback in case it's None)
            if self.btc_dominance is None:
                self.btc_dominance = self.get_btc_dominance()
            
            # If still None after trying to get it, use default value
            if self.btc_dominance is None:
                self.btc_dominance = 60.0  # Default estimate if API fails
            
            # Calculate targets and stop loss
            targets, stop_loss = self.calculate_targets_and_stop(main_df, direction, support_levels_with_strength, resistance_levels_with_strength, pattern, pattern_details=best_strategy.get("details"))
            
            # Calculate risk-reward ratio for first target
            if targets and stop_loss > 0:
                if direction == "LONG":
                    risk = current_price - stop_loss
                    reward = targets[0]["price"] - current_price
                    risk_reward_ratio = reward / risk if risk > 0 else 0
                else:
                    risk = stop_loss - current_price
                    reward = current_price - targets[0]["price"]
                    risk_reward_ratio = reward / risk if risk > 0 else 0
            else:
                risk_reward_ratio = 0
            
            # Calculate entry range (1% of current price)
            entry_range_low = current_price * 0.99
            entry_range_high = current_price * 1.01
            
            # Calculate optimal entry based on pattern and current price position
            optimal_entry = self._calculate_optimal_entry(main_df, current_price, direction, pattern, 
                                                    support_levels_with_strength, resistance_levels_with_strength)
            
            # Build the result dictionary
            result = {
                "symbol": symbol,
                "timestamp": (end_date or datetime.now(pytz.timezone("America/Vancouver"))).strftime("%Y-%m-%d %H:%M:%S"),
                "detected_pattern": pattern,
                "detected_strategies": [s["pattern"] for s in strategy_results[:3]], # Top 3 detected strategies
                "confidence": pattern_confidence,
                "direction": direction,
                "current_price": current_price,
                "entry_range": [entry_range_low, entry_range_high],
                "optimal_entry": optimal_entry,
                "targets": targets,
                "stop_loss": stop_loss,
                "risk_reward_ratio": risk_reward_ratio,
                "trends": trends,
                "btc_correlation": btc_correlation,
                "btc_dominance": self.btc_dominance,
                "support_levels": support_levels_with_strength,
                "resistance_levels": resistance_levels_with_strength,
                "dow_phase": self._dow_theory_phase(main_df)
            }
            result["quality_score"] = self._calculate_quality_score(result)
                        
            return result
            
        except Exception as e:
            print(f"Error analyzing {symbol}: {str(e)}")
            
            return {
                "symbol": symbol,
                "error": str(e),
                "timestamp": datetime.now(pytz.timezone("America/Vancouver")).strftime("%Y-%m-%d %H:%M:%S")
            }

    
    def analyze_all_symbols_smc(self, end_date: datetime = None, score_threshold: int = 70, verbose: bool = False):
        """
        Analyzes all predefined symbols using Smart Money Concepts, scores the setups,
        and saves high-quality signals. Includes optional verbose logging.
        """
        all_results = []
        print(f"Analyzing {len(self.symbols)} symbols with SMC (Quality Threshold: {score_threshold})...")

        for symbol in self.symbols:
            print(f"Analyzing {symbol}...")

            df = self.get_historical_data(symbol, '1d', limit=300, end_date=end_date)

            # Data Integrity Check
            if not self._is_data_clean(df, symbol, verbose):
                continue

            if df is None or len(df) < 50:
                if verbose: print(f"  - Skipped: Insufficient data for {symbol}")
                continue

            # Generate all SMC components
            pivot_highs, pivot_lows = self._detect_pivots(df, left=5, right=5)
            swings = self._build_swings(df, pivot_highs, pivot_lows)
            if len(swings) < 4:
                if verbose: print(f"  - Skipped: Not enough swing points ({len(swings)} found).")
                continue

            structure, _ = self._classify_structure(swings)
            structural_events = self._detect_bos_choch(df, swings)
            order_blocks = self._detect_order_blocks(df)
            fvgs = self._detect_fvgs(df)

            swing_highs = [s for s in swings if s['type'] == 'H']
            swing_lows = [s for s in swings if s['type'] == 'L']
            liquidity_highs = self._cluster_equal_levels(swing_highs)
            liquidity_lows = self._cluster_equal_levels(swing_lows)

            if verbose:
                print(f"  - Swings Found: {len(swings)}")
                print(f"  - Structure: {structure.upper()}")
                print(f"  - Structural Events: {len(structural_events)}")
                print(f"  - Order Blocks: {len(order_blocks)}")
                print(f"  - FVGs: {len(fvgs)}")
                print(f"  - Liquidity Highs: {len(liquidity_highs)}")
                print(f"  - Liquidity Lows: {len(liquidity_lows)}")

            # Find potential trade setups using all components
            trade_setups = self._smc_trade_builder(df, swings, structural_events, order_blocks, fvgs, liquidity_highs, liquidity_lows)

            if verbose and not trade_setups:
                print("  - No initial setups built by the trade builder.")

            if trade_setups:
                for setup in trade_setups:
                    # Find the specific BOS event that led to this setup for scoring
                    bos_event = setup.get('bos_event')
                    if not bos_event:
                        if verbose: print(f"  - Discarding setup (no associated BOS event): {setup['pattern']}")
                        continue

                    # Score the setup
                    quality_score = self._score_smc_setup(df, setup, swings, bos_event)

                    if verbose:
                        print(f"  - Scoring Setup: {setup['pattern']} | Score: {quality_score:.2f}")

                    if quality_score >= score_threshold:
                        if verbose: print(f"    - PASSED threshold. Saving signal.")
                        # Prepare result for CSV saving
                        result = {
                            "symbol": symbol,
                            "timestamp": (end_date or datetime.now(pytz.timezone("America/Vancouver"))).strftime("%Y-%m-%d %H:%M:%S"),
                            "detected_pattern": setup['pattern'],
                            "confidence": quality_score, # Use the quality score as confidence
                            "quality_score": quality_score,
                            "direction": setup['side'],
                            "current_price": setup['entry'],
                            "entry_range": [setup['entry'] * 0.995, setup['entry'] * 1.005],
                            "optimal_entry": setup['entry'],
                            "targets": setup['targets'],
                            "stop_loss": setup['stop_loss'],
                            "trends": {}, "dow_phase": "N/A" # Placeholders
                        }
                        all_results.append(result)
                    elif verbose:
                        print(f"    - FAILED threshold ({score_threshold}). Discarding.")

        for result in all_results:
            self.save_signal_to_csv(result)

        print(f"\nSMC analysis complete. Found and saved {len(all_results)} high-quality signals.")
        return all_results

    def _calculate_optimal_entry(self, df: pd.DataFrame, current_price: float, direction: str, pattern: str,
                                 support_levels_with_strength: List[Tuple[float, int]],
                                 resistance_levels_with_strength: List[Tuple[float, int]]) -> float:
        """
        Calculate optimal entry price based on pattern and price position
        
        Args:
            df: DataFrame with price data
            current_price: Current price
            direction: Trade direction ('LONG' or 'SHORT')
            pattern: Detected pattern
            support_levels_with_strength: List of support levels with their strength scores
            resistance_levels_with_strength: List of resistance levels with their strength scores
            
        Returns:
            Optimal entry price
        """
        try:
            # Start by fetching the comprehensive volatility metrics for the main dataframe.
            volatility_metrics = self._calculate_comprehensive_volatility(df)
            atr = volatility_metrics.get('atr_14day', current_price * 0.02)
            volatility_percentile = volatility_metrics.get('volatility_percentile', 0.5)
            optimal_entry = current_price

            # --- LONG Entry Logic ---
            if direction == "LONG":
                # Case 1: Breakout or High Momentum Patterns
                if "breakout" in pattern.lower() or "new local high" in pattern.lower():
                    if resistance_levels_with_strength:
                        breakout_level = resistance_levels_with_strength[0][0] # The level that was just broken
                        # In high volatility, we can't wait for a deep pullback. Enter closer to the breakout point.
                        if volatility_percentile > 0.75:
                            # Aggressive entry: a very shallow pullback, just 0.25 * ATR from the breakout level
                            optimal_entry = breakout_level + (atr * 0.25)
                        else:
                            # Normal volatility: wait for a retest of the breakout level
                            optimal_entry = breakout_level
                    else:
                        optimal_entry = current_price

                # Case 2: Pullback or Mean Reversion Patterns
                elif "pullback" in pattern.lower() or "oversold" in pattern.lower() or "support bounce" in pattern.lower():
                    if support_levels_with_strength:
                        # We expect price to come to us. Be more patient.
                        # Target the upper third of the zone between the current price and the key support.
                        key_support_level = support_levels_with_strength[0][0]
                        optimal_entry = key_support_level + (current_price - key_support_level) * 0.66
                    else:
                        optimal_entry = current_price - (atr * 0.5)


                # Default Case: For other bullish patterns (e.g., wedges, flags)
                else:
                    # A standard, shallow pullback of 0.5 * ATR from the current price.
                    optimal_entry = current_price - (atr * 0.5)

            # --- SHORT Entry Logic ---
            elif direction == "SHORT":
                # Case 1: Breakdown or High Momentum Patterns
                if "breakdown" in pattern.lower() or "new local low" in pattern.lower():
                    if support_levels_with_strength:
                        breakdown_level = support_levels_with_strength[0][0]
                        if volatility_percentile > 0.75:
                            # Aggressive entry for volatile breakdowns
                            optimal_entry = breakdown_level - (atr * 0.25)
                        else:
                            # Normal volatility: wait for a retest of the breakdown level
                            optimal_entry = breakdown_level
                    else:
                        optimal_entry = current_price

                # Case 2: Pullback (to resistance) or Mean Reversion Patterns
                elif "overbought" in pattern.lower() or "resistance rejection" in pattern.lower():
                    if resistance_levels_with_strength:
                        # Be patient and wait for the price to come to the level.
                        key_resistance_level = resistance_levels_with_strength[0][0]
                        optimal_entry = key_resistance_level - (key_resistance_level - current_price) * 0.66
                    else:
                        optimal_entry = current_price + (atr * 0.5)

                # Default Case: For other bearish patterns
                else:
                    optimal_entry = current_price + (atr * 0.5)

            # --- Final Validation ---
            # The optimal entry must be clamped within the calculated entry range.
            if direction == "LONG":
                # The optimal entry cannot be lower than the bottom of the entry range (current_price * 0.99)
                # And it cannot be higher than the current price itself for a long entry.
                entry_range_low = current_price * 0.99
                return min(max(optimal_entry, entry_range_low), current_price)

            elif direction == "SHORT":
                # The optimal entry cannot be higher than the top of the entry range (current_price * 1.01)
                # And it cannot be lower than the current price itself for a short entry.
                entry_range_high = current_price * 1.01
                return max(min(optimal_entry, entry_range_high), current_price)

            return optimal_entry # Should not be reached if direction is always LONG or SHORT
        except Exception as e:
            print(f"Error calculating optimal entry: {str(e)}")
            return current_price
    
    def save_signal_to_csv(self, result: Dict[str, Any]) -> None:
        """
        Save a trading signal to CSV file
        
        Args:
            result: Dictionary with signal information
        """
        try:
            if "error" in result:
                print(f"Cannot save signal with error: {result['error']}")
                return
            
            # Get values from result
            symbol = result["symbol"]
            timestamp = result["timestamp"]
            detected_pattern = result["detected_pattern"]
            confidence = result["confidence"]
            quality_score = result["quality_score"]
            direction = result["direction"]
            current_price = result["current_price"]
            entry_range_low, entry_range_high = result["entry_range"]
            optimal_entry = result["optimal_entry"]
            targets = result["targets"]
            stop_loss = result["stop_loss"]
            risk_reward_ratio = result.get("risk_reward_ratio", 0)
            trends = result["trends"]
            dow_phase = result["dow_phase"]
            
            # Get Vancouver time
            date_time = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
            signal_date = date_time.strftime("%Y-%m-%d")
            signal_time = date_time.strftime("%H:%M")
            
            # Create row dictionary for CSV
            row = {
                'quality_score' : quality_score,
                'confidence': confidence,
                'blank': detected_pattern,
                'signal_date': signal_date,
                'signal_time': signal_time,
                'crypto_pair': f"{symbol}/USDT",
                'direction': direction,
                # 'detected_pattern': detected_pattern,
                # 'detected_strategy': '',
                'startentryrange': entry_range_low,
                'endentryrange': entry_range_high,
                'middleofentryrange': optimal_entry,
                
            }
            
            # Add targets (up to 10)
            for i in range(1, 11):
                if i <= len(targets):
                    row[f'Target{i}'] = targets[i-1]["price"]
                    row[f'Target{i}Probability'] = targets[i-1]["probability"]
                    row[f'Target{i}Reason'] = targets[i-1]["reason"]
                else:
                    row[f'Target{i}'] = ""
                    row[f'Target{i}Probability'] = ""
                    row[f'Target{i}Reason'] = ""
            
            # Add stop loss and risk-reward ratio
            row['StopLoss'] = stop_loss
            row['RiskRewardRatio'] = risk_reward_ratio
            
            # Add trend information
            row['1h_Trend'] = trends.get('1h', {}).get('trend', 'NEUTRAL')
            row['4h_Trend'] = trends.get('4h', {}).get('trend', 'NEUTRAL')
            row['1d_Trend'] = trends.get('1d', {}).get('trend', 'NEUTRAL')
            row['1w_Trend'] = trends.get('1w', {}).get('trend', 'NEUTRAL')

            row['dowphase'] = dow_phase
            
            # Write to CSV
            with open(self.signals_path, 'a', newline='') as csvfile:
                fieldnames = list(row.keys())
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writerow(row)
            
            print(f"Signal for {symbol} saved to CSV.")
            
        except Exception as e:
            print(f"Error saving signal to CSV: {str(e)}")
    
    def analyze_all_symbols(self) -> List[Dict[str, Any]]:
        """
        Analyze all symbols and return best signals
        
        Returns:
            List of best trading signals
        """
        all_results = []
        
        print(f"Analyzing {len(self.symbols)} symbols...")
        
        # Update correlation data if needed
        if not self.correlation_matrix:
            self.update_correlations()

        # Update BTC trend information
        self.update_btc_trend()
        
        # Process symbols
        for symbol in self.symbols:
            try:
                result = self.analyze_single_symbol(symbol)
                
                if "error" not in result and result.get("confidence", 0) >= 50:
                    all_results.append(result)
                    
                    # Print brief update
                    direction = result.get("direction", "NEUTRAL")
                    confidence = result.get("confidence", 0)
                    pattern = result.get("detected_pattern", "unknown")
                    print(f"{symbol}: {direction} {pattern} ({confidence:.1f}% confidence)")
                
            except Exception as e:
                print(f"Error analyzing {symbol}: {str(e)}")
        
        # Filter for best signals
        filtered_results = self.filter_signals(all_results)
        
        # Save signals to CSV
        for result in filtered_results:
            self.save_signal_to_csv(result)
        
        print(f"Analysis complete. Found {len(filtered_results)} high-confidence signals.")
        return filtered_results

    def analyze_all_symbols_at_past_date(self, past_datetime: datetime) -> List[Dict[str, Any]]:
        """
        Analyze all symbols at a specific past date and time.
        """
        all_results = []

        print(f"Analyzing {len(self.symbols)} symbols at {past_datetime}...")

        # Update correlation data if needed
        if not self.correlation_matrix:
            self.update_correlations()

        # Update BTC trend information
        self.update_btc_trend()

        # Process symbols
        for symbol in self.symbols:
            try:
                # We need to create a new analyze_single_symbol_at_past_date or modify the existing one
                # For now, let's create a new one to not break existing functionality
                result = self.analyze_single_symbol(symbol, end_date=past_datetime)

                if "error" not in result and result.get("confidence", 0) >= 60:
                    all_results.append(result)

                    # Print brief update
                    direction = result.get("direction", "NEUTRAL")
                    confidence = result.get("confidence", 0)
                    pattern = result.get("detected_pattern", "unknown")
                    print(f"{symbol}: {direction} {pattern} ({confidence:.1f}% confidence)")

            except Exception as e:
                print(f"Error analyzing {symbol}: {str(e)}")

        # Filter for best signals
        filtered_results = self.filter_signals(all_results)

        # Save signals to CSV
        for result in filtered_results:
            self.save_signal_to_csv(result)

        print(f"Analysis complete. Found {len(filtered_results)} high-confidence signals.")
        return filtered_results

    def _calculate_quality_score(self, result: Dict[str, Any]) -> float:
        """Calculates the comprehensive quality score for a single trade signal."""
        # --- Quality Score Calculation ---

        # Component 1: Base Pattern Confidence (Weight: 40%)
        base_confidence_score = result.get("confidence", 0)

        # Component 2: Multi-Timeframe Trend Alignment (Weight: 30%)
        trend_alignment_score = 0
        trends = result.get("trends", {})
        direction = result.get("direction", "NEUTRAL")
        tf_weights = {"1h": 0.15, "4h": 0.30, "1d": 0.40, "1w": 0.15}

        for tf, weight in tf_weights.items():
            tf_trend = trends.get(tf, {}).get("trend", "NEUTRAL")
            if (direction == "LONG" and tf_trend == "BULLISH") or \
               (direction == "SHORT" and tf_trend == "BEARISH"):
                trend_alignment_score += 100 * weight
            elif (direction == "LONG" and tf_trend == "BEARISH") or \
                 (direction == "SHORT" and tf_trend == "BULLISH"):
                trend_alignment_score -= 100 * weight

        trend_alignment_score = (trend_alignment_score + 100) / 2

        # Component 3: Risk-to-Reward Ratio (Weight: 20%)
        rr_ratio = result.get("risk_reward_ratio", 0)
        rr_score = 0
        if rr_ratio >= 5:
            rr_score = 100
        elif rr_ratio >= 3:
            rr_score = 90
        elif rr_ratio >= 2:
            rr_score = 75
        elif rr_ratio >= 1.5:
            rr_score = 60
        else:
            rr_score = 20

        # Component 4: S/R Zone Strength (Weight: 10%)
        sr_strength_score = 50
        if direction == "LONG" and result.get("support_levels"):
            sr_strength_score = 50 + (result["support_levels"][0][1] * 5)
        elif direction == "SHORT" and result.get("resistance_levels"):
            sr_strength_score = 50 + (result["resistance_levels"][0][1] * 5)
        sr_strength_score = min(100, sr_strength_score)

        # --- Final Weighted Quality Score ---
        quality_score = (base_confidence_score * 0.40) + \
                        (trend_alignment_score * 0.30) + \
                        (rr_score * 0.20) + \
                        (sr_strength_score * 0.10)

        return quality_score
    
    def filter_signals(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Filter and rank signals by a comprehensive quality score.
        
        Args:
            results: List of signal results.
            
        Returns:
            Filtered and sorted list of high-quality trade setups.
        """
        if not results:
            return []
        
        try:
            # Initial filtering by minimum confidence
            min_confidence = 50
            filtered = [r for r in results if r.get("confidence", 0) >= min_confidence]
            
            for result in filtered:
                result["quality_score"] = self._calculate_quality_score(result)

            # Sort by the new, more comprehensive quality_score
            sorted_results = sorted(filtered, key=lambda x: x.get("quality_score", 0), reverse=True)

            # Return top signals (limit to top 15)
            return sorted_results[:15]
            
        except Exception as e:
            print(f"Error filtering signals: {str(e)}")
            traceback.print_exc()
            return results[:5]
        
    def calculate_signal_confidence(self, pattern_name: str, direction: str,
                                df: pd.DataFrame,
                                support_levels: List[Tuple[float, int]],
                                resistance_levels: List[Tuple[float, int]],
                                trends: Dict[str, Dict[str, Any]],
                                base_confidence_override: Optional[float] = None) -> float:
        """
        Calculate confidence score for trading signals with realistic distribution for crypto markets
        """
        try:
            pattern_lower = pattern_name.lower()
            if base_confidence_override is not None:
                base_confidence = base_confidence_override
            else:
                # More conservative base success rates (reduced by 8-12 points)
                pattern_success_rates = {
                    # Enhanced Strategy Blocks
                    "Pullback (1D) in Uptrend": 65,
                    "Pullback (1W) in Uptrend": 63,
                    "Momentum & UpTrend (v.1)": 62,
                    "Momentum & UpTrend (v.2)": 62,
                    "Fresh Bullish Momentum (MACD Signal Line) Crossover": 59,
                    "Early Bullish Momentum (MACD Histogram) Inflection": 57,
                    "UpTrend and Fresh Momentum Inflection": 63,
                    "Bullish Momentum with RSI confirmation (v.1)": 64,
                    "Bullish Momentum with RSI confirmation (v.2)": 64,
                    "Bullish EMA 12/50 crossovers": 61,
                    "Strong UpTrend": 69,
                    "UpTrend": 62,
                    "Short-Term Trend Upgrade": 55,
                    "Very Oversold": 58,
                    "Oversold in UpTrend": 66,
                    "Oversold with Momentum Shift": 63,
                    "New Local High": 59,
                    "New Local Low": 59,
                    "Bullish Trading in Range (v.1)": 56,
                    "Bullish Trading in Range (v.2)": 56,
                    "Within 5% of ATH": 55,
                    "Within 5% of ATH and not very overbought": 59,
                    "Within 5% of ATH and recent bullish MACD crossover": 62,
                    "Within 5% of ATH and bullish inflection in MACD Histogram": 61,
                    "Recent ATH, pulled back but MACD is starting to inflect bullish": 63,
                    "Recent ATH, still within 10% of ATH, and not very overbought": 61,
                    "Recent ATH": 52,

                    # Chart patterns (more conservative)
                    "inverse_head_and_shoulders": 71,
                    "head_and_shoulders": 69,
                    "double_bottom": 69,
                    "channel_up": 60,
                    "channel_down": 59,
                    "descending_triangle": 57,
                    "double_top": 56,
                    "ascending_triangle": 55,
                    "flag": 55,
                    "bull_flag": 55,
                    "bear_flag": 55,
                    "falling_wedge": 53,
                    "rising_wedge": 52,
                    "triangle": 49,
                    "rectangle": 45,
                    "pennant": 43,

                    # Others
                    "bullish_rsi_divergence": 59,
                    "bearish_rsi_divergence": 59,
                    "horizontal_resistance": 52,
                    "horizontal_support": 52,
                    "resistance_rejection": 50,
                    "support_bounce": 50,
                }

                # Find pattern base rate with LOWER default
                base_confidence = 50  # Reduced from 60
                pattern_lower = pattern_name.lower()

                for pattern_key, success_rate in pattern_success_rates.items():
                    if pattern_key.lower() in pattern_lower or pattern_lower in pattern_key.lower():
                        base_confidence = success_rate
                        break
            
            # LOWER CAP to allow more room for adjustments
            base_confidence = min(base_confidence, 65)  # Reduced from 78
            
            # 1. STRICTER Trend alignment score (range: -18 to +10 points)
            trend_alignment_score = 0
            trend_weights = {
                "1d": 0.45,
                "4h": 0.35, 
                "1h": 0.15,
                "1w": 0.05
            }
            
            aligned_trends = 0
            opposing_trends = 0
            total_trends = 0
            
            for tf, weight in trend_weights.items():
                if tf in trends and "trend" in trends[tf]:
                    total_trends += 1
                    tf_trend = trends[tf]["trend"]
                    trend_strength = min(90, trends[tf].get("strength", 50))
                    
                    if (direction == "LONG" and tf_trend == "BULLISH") or \
                    (direction == "SHORT" and tf_trend == "BEARISH"):
                        aligned_trends += 1
                        # REDUCED bonus points
                        alignment_strength = (trend_strength / 50) * weight
                        trend_alignment_score += weight * (trend_strength / 50) * 4  # Reduced from 8
                    elif (direction == "LONG" and tf_trend == "BEARISH") or \
                        (direction == "SHORT" and tf_trend == "BULLISH"):
                        opposing_trends += 1
                        opposition_strength = (trend_strength / 50) * weight
                        trend_alignment_score -= weight * (trend_strength / 50) * 15  # Increased penalty
            
            # REDUCED perfect alignment bonus
            if total_trends >= 3 and aligned_trends == total_trends:
                trend_alignment_score += 3  # Reduced from 6
            elif total_trends >= 2 and opposing_trends == 0:
                trend_alignment_score += 1  # Reduced from 3
            
            trend_alignment_score = max(-18, min(10, trend_alignment_score))
            
            # 2. STRICTER Indicator confluence score (range: -15 to +8 points)
            indicator_score = 0
            
            # RSI analysis - STRICTER scoring
            if 'rsi14' in df.columns:
                rsi = df['rsi14'].iloc[-1]
                rsi_prev = df['rsi14'].iloc[-2] if len(df) > 2 else 50
                rsi_momentum = rsi - rsi_prev
                
                if direction == "LONG":
                    if rsi < 20:  # Extremely oversold
                        indicator_score += 4  # Reduced from 7
                    elif rsi < 30:  # Strongly oversold  
                        indicator_score += 3  # Reduced from 5
                    elif rsi < 40:  # Moderately oversold
                        indicator_score += 1  # Reduced from 3
                    if rsi < 35 and rsi_momentum > 3:  # Stronger momentum required
                        indicator_score += 2  # Reduced from 3
                    elif "uptrend" in pattern_lower and rsi < 50:
                        indicator_score += 1  # Reduced from 2
                    elif rsi > 80:  # Extremely overbought
                        indicator_score -= 8  # Increased penalty
                    elif rsi > 70:  # Overbought
                        indicator_score -= 5  # Increased from 3
                
                elif direction == "SHORT":
                    if rsi > 80:
                        indicator_score += 4
                    elif rsi > 70:
                        indicator_score += 3
                    elif rsi > 60:
                        indicator_score += 1
                    if rsi > 65 and rsi_momentum < -3:
                        indicator_score += 2
                    elif rsi < 20:
                        indicator_score -= 8
                    elif rsi < 30:
                        indicator_score -= 5
            
            # STRICTER MACD scoring
            if all(col in df.columns for col in ['macd', 'macd_signal', 'macd_hist']):
                macd = df['macd'].iloc[-1]
                macd_signal = df['macd_signal'].iloc[-1]
                macd_hist = df['macd_hist'].iloc[-1]
                macd_hist_prev = df['macd_hist'].iloc[-2] if len(df) > 2 else 0
                
                # Reduced divergence bonus
                if len(df) >= 20:
                    recent_price_high = df['high'].iloc[-10:].max()
                    recent_price_low = df['low'].iloc[-10:].min()
                    recent_macd_high = df['macd'].iloc[-10:].max()
                    recent_macd_low = df['macd'].iloc[-10:].min()
                    
                    if direction == "LONG":
                        prev_price_low = df['low'].iloc[-20:-10].min()
                        prev_macd_low = df['macd'].iloc[-20:-10].min()
                        if recent_price_low < prev_price_low and recent_macd_low > prev_macd_low:
                            indicator_score += 2  # Reduced from 6
                    
                    elif direction == "SHORT":
                        prev_price_high = df['high'].iloc[-20:-10].max()
                        prev_macd_high = df['macd'].iloc[-20:-10].max()
                        if recent_price_high > prev_price_high and recent_macd_high < prev_macd_high:
                            indicator_score += 2  # Reduced from 6
                
                # Standard MACD - REDUCED bonuses
                if direction == "LONG":
                    if macd > macd_signal and macd_hist > 0:
                        indicator_score += 2  # Reduced from 4
                    elif macd_hist > 0 and macd_hist > macd_hist_prev:
                        indicator_score += 1  # Reduced from 3
                    elif macd_hist < 0 and macd_hist > macd_hist_prev:
                        indicator_score += 1  # Reduced from 2
                    elif macd < macd_signal and macd_hist < 0:
                        indicator_score -= 6  # Increased penalty
                
                elif direction == "SHORT":
                    if macd < macd_signal and macd_hist < 0:
                        indicator_score += 2
                    elif macd_hist < 0 and macd_hist < macd_hist_prev:
                        indicator_score += 1
                    elif macd_hist > 0 and macd_hist < macd_hist_prev:
                        indicator_score += 1
                    elif macd > macd_signal and macd_hist > 0:
                        indicator_score -= 6
            
            # MA alignment - reduced bonuses
            ma_alignment_score = 0
            if all(col in df.columns for col in ['ema20', 'ema50', 'ema200']):
                if df['ema20'].iloc[-1] > df['ema50'].iloc[-1] > df['ema200'].iloc[-1]:
                    ma_alignment_score = 2 if direction == "LONG" else -6  # Reduced bonus, increased penalty
                elif df['ema20'].iloc[-1] < df['ema50'].iloc[-1] < df['ema200'].iloc[-1]:
                    ma_alignment_score = 2 if direction == "SHORT" else -6
            
            indicator_score += ma_alignment_score
            indicator_score = max(-15, min(8, indicator_score))
            
            # 3. ENHANCED S/R confluence scoring
            sr_confluence_score = 0
            current_price = df['close'].iloc[-1]

            if direction == "LONG":
                if support_levels:
                    nearest_support_price, nearest_support_strength = support_levels[0]
                    distance_to_support_pct = (current_price - nearest_support_price) / current_price
                    if distance_to_support_pct < 0.015:
                        sr_confluence_score += nearest_support_strength * 2.5

                if resistance_levels:
                    nearest_resistance_price, nearest_resistance_strength = resistance_levels[0]
                    distance_to_resistance_pct = (nearest_resistance_price - current_price) / current_price
                    if distance_to_resistance_pct < 0.015 and nearest_resistance_strength >= 4:
                        sr_confluence_score -= 15

            elif direction == "SHORT":
                if resistance_levels:
                    nearest_resistance_price, nearest_resistance_strength = resistance_levels[0]
                    distance_to_resistance_pct = (nearest_resistance_price - current_price) / current_price
                    if distance_to_resistance_pct < 0.015:
                        sr_confluence_score += nearest_resistance_strength * 2.5

                if support_levels:
                    nearest_support_price, nearest_support_strength = support_levels[0]
                    distance_to_support_pct = (current_price - nearest_support_price) / current_price
                    if distance_to_support_pct < 0.015 and nearest_support_strength >= 4:
                        sr_confluence_score -= 15

            sr_confluence_score = max(-20, min(25, sr_confluence_score))

            # 4. STRICTER Volume confirmation (range: -6 to +4 points)
            volume_score = 0
            
            if 'volume' in df.columns and len(df) > 15:
                current_volume = df['volume'].iloc[-1]
                recent_avg_volume = df['volume'].iloc[-11:-1].mean()
                longer_avg_volume = df['volume'].iloc[-21:-11].mean() if len(df) > 21 else recent_avg_volume
                
                if current_volume > 0 and recent_avg_volume > 0:
                    volume_ratio = current_volume / recent_avg_volume
                    volume_trend = recent_avg_volume / longer_avg_volume if longer_avg_volume > 0 else 1
                    
                    if volume_ratio > 4.0:
                        volume_score += 4
                    elif volume_ratio > 2.5:
                        volume_score += 3
                    elif volume_ratio > 1.8:
                        volume_score += 2
                    elif volume_ratio > 1.3:
                        volume_score += 1
                    elif volume_ratio < 0.5:
                        volume_score -= 6
                    
                    if volume_trend > 1.5:
                        volume_score += 1
                    elif volume_trend < 0.6:
                        volume_score -= 2
            
            volume_score = max(-6, min(4, volume_score))
            
            # 5. REDUCED pattern-specific scoring (range: -3 to +5 points)
            pattern_specific_score = 0
            
            if "pullback" in pattern_lower and "uptrend" in pattern_lower:
                if len(df) > 20:
                    recent_lows = []
                    for i in range(5, 20, 5):
                        if i < len(df):
                            local_min = df['low'].iloc[max(0, i-2):min(len(df), i+3)].min()
                            recent_lows.append(local_min)
                    
                    if len(recent_lows) >= 2 and all(recent_lows[i] >= recent_lows[i-1] for i in range(1, len(recent_lows))):
                        pattern_specific_score += 4
            
            elif "macd" in pattern_lower:
                if "crossover" in pattern_lower:
                    if 'macd' in df.columns:
                        macd = df['macd'].iloc[-1]
                        if (direction == "LONG" and macd < 0) or (direction == "SHORT" and macd > 0):
                            pattern_specific_score += 3
                
                elif "histogram" in pattern_lower:
                    if 'macd_hist' in df.columns:
                        hist = df['macd_hist'].iloc[-1]
                        hist_prev = df['macd_hist'].iloc[-2] if len(df) > 2 else 0
                        hist_change = abs(hist - hist_prev) / abs(hist_prev) if hist_prev != 0 else 0
                        
                        if hist_change > 0.3:
                            pattern_specific_score += 4
                        elif hist_change > 0.15:
                            pattern_specific_score += 2
            
            elif "rsi" in pattern_lower or "oversold" in pattern_lower:
                if 'rsi14' in df.columns:
                    rsi = df['rsi14'].iloc[-1]
                    rsi_prev = df['rsi14'].iloc[-2] if len(df) > 2 else 50
                    
                    if rsi < 35 and rsi > rsi_prev and direction == "LONG":
                        pattern_specific_score += 3
                    elif rsi > 65 and rsi < rsi_prev and direction == "SHORT":
                        pattern_specific_score += 3
            
            elif "ath" in pattern_lower:
                if 'volume' in df.columns and len(df) > 10:
                    recent_vol_avg = df['volume'].iloc[-5:].mean()
                    prior_vol_avg = df['volume'].iloc[-10:-5].mean()
                    
                    if recent_vol_avg > prior_vol_avg * 1.5:
                        pattern_specific_score += 4
            
            pattern_specific_score = max(-3, min(5, pattern_specific_score))
            
            # --- Final Confidence Calculation ---
            base_weight = 0.40
            trend_weight = 0.25
            indicator_weight = 0.10
            sr_confluence_weight = 0.25

            weighted_confidence = (base_confidence * base_weight) + \
                                  ((50 + trend_alignment_score) * trend_weight) + \
                                  ((50 + indicator_score) * indicator_weight) + \
                                  ((50 + sr_confluence_score) * sr_confluence_weight)

            # Add volume and pattern-specific scores as final adjustments
            final_confidence = weighted_confidence + volume_score + pattern_specific_score

            # Keep the existing volatility penalty
            if 'atr_percent' in df.columns:
                atr_percent = df['atr_percent'].iloc[-1]
                if atr_percent > 20:
                    volatility_penalty = 25
                elif atr_percent > 15:
                    volatility_penalty = 20
                elif atr_percent > 12:
                    volatility_penalty = 15
                elif atr_percent > 8:
                    volatility_penalty = 10
                elif atr_percent > 5:
                    volatility_penalty = 5
                else:
                    volatility_penalty = 0
                final_confidence -= volatility_penalty
            
            # --- Gap Analysis Adjustment ---
            gap_analysis = self._analyze_price_gaps(df)
            if gap_analysis["gap_type"] == "up" and direction == "LONG":
                final_confidence += gap_analysis["score"] * 0.15 # Add 15% of the gap score
            elif gap_analysis["gap_type"] == "down" and direction == "SHORT":
                final_confidence += gap_analysis["score"] * 0.15

            # Keep the existing randomization
            pattern_hash = sum(ord(c) for c in pattern_name)
            symbol_string = df.index.name if df.index.name else "default"
            symbol_hash = sum(ord(c) for c in symbol_string)
            combined_hash = (pattern_hash + symbol_hash) % 100
            
            if combined_hash < 15:
                random_adjustment = -6
            elif combined_hash < 30:
                random_adjustment = -3
            elif combined_hash < 45:
                random_adjustment = -1
            elif combined_hash < 55:
                random_adjustment = 0
            elif combined_hash < 70:
                random_adjustment = +1
            elif combined_hash < 85:
                random_adjustment = +3
            else:
                random_adjustment = +6
            final_confidence += random_adjustment

            return max(30, min(95, final_confidence))
            
        except Exception as e:
            print(f"Error calculating signal confidence: {str(e)}")
            return 50  # Lower default


    
    def format_signal_output(self, result: Dict[str, Any]) -> str:
        """
        Format signal result as a readable string
        
        Args:
            result: Dictionary with signal information
            
        Returns:
            Formatted string with signal information
        """
        try:
            if "error" in result:
                return f"Error analyzing {result.get('symbol', 'unknown')}: {result['error']}"
            
            # Get values from result with safe defaults for None values
            symbol = result.get("symbol", "Unknown")
            timestamp = result.get("timestamp", "N/A")
            detected_pattern = result.get("detected_pattern", "Unknown Pattern")
            confidence = result.get("confidence", 0)
            direction = result.get("direction", "NEUTRAL")
            current_price = result.get("current_price", 0)
            dow_phase = result.get("dow_phase", 0)
            
            # Handle potential None values for entry ranges
            entry_range = result.get("entry_range", [0, 0])
            if entry_range is None or len(entry_range) < 2:
                entry_range_low, entry_range_high = 0, 0
            else:
                entry_range_low, entry_range_high = entry_range
            
            optimal_entry = result.get("optimal_entry", 0)
            targets = result.get("targets", [])
            stop_loss = result.get("stop_loss", 0)
            risk_reward_ratio = result.get("risk_reward_ratio", 0)
            trends = result.get("trends", {})
            btc_correlation = result.get("btc_correlation", 0)
            
            # Handle None for btc_dominance which is causing the error
            btc_dominance = result.get("btc_dominance")
            if btc_dominance is None:
                btc_dominance = 0.0
            
            # Format BTC correlation description
            if btc_correlation > 0.8:
                corr_desc = "strong positive"
            elif btc_correlation > 0.5:
                corr_desc = "moderate positive"
            elif btc_correlation > 0.2:
                corr_desc = "weak positive"
            elif btc_correlation > -0.2:
                corr_desc = "neutral"
            elif btc_correlation > -0.5:
                corr_desc = "weak negative"
            elif btc_correlation > -0.8:
                corr_desc = "moderate negative"
            else:
                corr_desc = "strong negative"
            
            # Build the output string with safe formatting
            output = f"""
    ==================== {symbol} Signal ====================
    Timestamp: {timestamp} (Vancouver time)

    SIGNAL SUMMARY:
    Pattern: {detected_pattern}
    Direction: {direction}
    Confidence: {confidence:.1f}%

    PRICE INFORMATION:
    Current Price: ${current_price:.8f}
    Entry Range: ${entry_range_low:.8f} - ${entry_range_high:.8f}
    Optimal Entry: ${optimal_entry:.8f}

    TARGETS:
    """
            # Add targets (safely handle any None or invalid values)
            for i, target in enumerate(targets, 1):
                if not isinstance(target, dict):
                    continue
                    
                target_price = target.get('price', 0)
                target_percent = target.get('percent', 0)
                target_reason = target.get('reason', 'Unknown')
                target_probability = target.get('probability', 0)
                
                output += f"""Target {i}: ${target_price:.8f} ({target_percent:.1f}%)
    Reason: {target_reason}
    Probability: {target_probability}%
    """
            
            # Add stop loss and risk information (with safe handling for None values)
            risk_percent = 0
            if direction == "LONG" and current_price > 0 and stop_loss is not None:
                risk_percent = ((current_price - stop_loss) / current_price) * 100
            elif direction == "SHORT" and current_price > 0 and stop_loss is not None:
                risk_percent = ((stop_loss - current_price) / current_price) * 100
            
            output += f"""
    RISK MANAGEMENT:
    Stop Loss: ${stop_loss:.8f} ({risk_percent:.1f}%)
    Reward-to-Risk Ratio: {risk_reward_ratio:.2f}

    Dow Phase: {dow_phase}

    TREND ANALYSIS:
    """
            # Add trend info with safe handling
            for tf in ['1h', '4h', '1d', '1w']:
                if tf in trends:
                    tf_trend = trends[tf].get('trend', 'NEUTRAL')
                    tf_strength = trends[tf].get('strength', 0)
                    output += f"{tf} Trend: {tf_trend} ({tf_strength:.1f}%)\n"
                else:
                    output += f"{tf} Trend: UNKNOWN (0.0%)\n"
            
            # Add market context with safe handling of None values
            output += f"""
    MARKET CONTEXT:
    BTC Correlation: {btc_correlation:.2f} ({corr_desc})
    BTC Dominance: {btc_dominance:.1f}%
    """
            
            return output
            
        except Exception as e:
            print(f"Error formatting signal output: {str(e)}")
            return f"Error formatting signal for {result.get('symbol', 'unknown')}: {str(e)}"
    
    def analyze_custom_strategy(self, symbol: str, strategy_type: str, strategy_name: str = None) -> Dict[str, Any]:
        """
        Analyze a symbol with a specific strategy
        
        Args:
            symbol: The crypto symbol to analyze
            strategy_type: The type of strategy (e.g., 'Strategy Blocks', 'Chart Patterns')
            strategy_name: The specific strategy to check (optional, will prompt if None)
            
        Returns:
            Dictionary with analysis results
        """
        try:
            print(f"\nAnalyzing {symbol} with {strategy_type}...")
            
            # If strategy_name is None, show menu to select strategy
            if strategy_name is None:
                strategy_name = self._select_strategy_from_menu(strategy_type)
                if strategy_name is None:
                    return {
                        "symbol": symbol,
                        "error": "Strategy selection cancelled",
                        "timestamp": datetime.now(pytz.timezone("America/Vancouver")).strftime("%Y-%m-%d %H:%M:%S")
                    }
            
            # Get data for different timeframes
            timeframes = ["1h", "4h", "1d", "1w"]
            dataframes = {}
            
            for tf in timeframes:
                df = self.get_historical_data(symbol, tf)
                if df is not None and len(df) >= 30:
                    dataframes[tf] = self.calculate_technical_indicators(df)
                else:
                    print(f"Insufficient data for {symbol} on {tf} timeframe")
            
            # If we couldn't get data for any timeframe, return error
            if not dataframes:
                return {
                    "symbol": symbol,
                    "error": "Insufficient data",
                    "timestamp": datetime.now(pytz.timezone("America/Vancouver")).strftime("%Y-%m-%d %H:%M:%S")
                }
            
            # Update BTC trend data if not already present
            if self.btc_trend is None:
                self.update_btc_trend()
            
            # Main analysis timeframe (use daily by default)
            main_tf = "1d" if "1d" in dataframes else list(dataframes.keys())[0]
            main_df = dataframes[main_tf]
            
            # Get current price
            current_price = main_df['close'].iloc[-1]
            
            # Calculate support and resistance levels
            support_levels_with_strength, resistance_levels_with_strength = self.identify_support_resistance(dataframes)
            
            # Map strategy type to detector method
            strategy_detectors = {
                "Strategy Blocks": self._detect_strategy_block,
                "Trading Chart Patterns": self._detect_chart_pattern,
                "Candlestick Patterns": self._detect_candlestick_pattern,
                "Trend and Momentum": self._detect_trend_momentum,
                "Leading Indicator (Oscillators)": self._detect_oscillator_signal,
                "Divergence Signal": self._detect_divergence_signal
            }
            
            # Check if strategy type exists
            if strategy_type not in strategy_detectors:
                return {
                    "symbol": symbol,
                    "error": f"Unknown strategy type: {strategy_type}",
                    "timestamp": datetime.now(pytz.timezone("America/Vancouver")).strftime("%Y-%m-%d %H:%M:%S")
                }
            
            # Use appropriate detector
            detector = strategy_detectors[strategy_type]
            
            print(f"\nAnalyzing {symbol} with {strategy_type} - {strategy_name}...")
            
            # Detect the specified strategy with support and resistance levels
            strategy_result = detector(dataframes[main_tf], strategy_name, dataframes, support_levels_with_strength, resistance_levels_with_strength)
            
            # Even if the strategy is not detected with high confidence, we'll still proceed
            # This ensures we give the user some analysis rather than an empty result
            
            # Direction from strategy (default to LONG if not specified)
            direction = strategy_result.get("direction", "LONG") 
            if direction == "NEUTRAL":
                direction = "LONG"  # Default to LONG for NEUTRAL cases to generate targets
            
            # Analyze trends for different timeframes
            trends = {}
            for tf, df_item in dataframes.items():
                trends[tf] = self.analyze_trend(df_item, tf)
            
            # Check for BTC correlation
            btc_correlation = self.calculate_correlation(symbol)
            
            # Check BTC dominance (with fallback)
            if self.btc_dominance is None:
                self.btc_dominance = self.get_btc_dominance()
                
            # If still None, use default
            if self.btc_dominance is None:
                self.btc_dominance = 60.0  # Default estimate
            
            # Calculate targets and stop loss - ALWAYS calculate these regardless of detection confidence
            targets, stop_loss = self.calculate_targets_and_stop(main_df, direction, support_levels_with_strength, resistance_levels_with_strength)
            
            # Calculate risk-reward ratio for first target
            risk_reward_ratio = 0
            if targets and stop_loss > 0:
                if direction == "LONG":
                    risk = current_price - stop_loss
                    if risk > 0:  # Prevent division by zero
                        reward = targets[0]["price"] - current_price
                        risk_reward_ratio = reward / risk
                else:  # SHORT
                    risk = stop_loss - current_price
                    if risk > 0:  # Prevent division by zero
                        reward = current_price - targets[0]["price"]
                        risk_reward_ratio = reward / risk
            
            # Calculate entry range (1% of current price)
            entry_range_low = current_price * 0.99
            entry_range_high = current_price * 1.01
            
            # Calculate optimal entry based on pattern and current price position
            optimal_entry = self._calculate_optimal_entry(main_df, current_price, direction, strategy_name, 
                                                    support_levels_with_strength, resistance_levels_with_strength)
            
            # Build the result dictionary
            result = {
                "symbol": symbol,
                "timestamp": datetime.now(pytz.timezone("America/Vancouver")).strftime("%Y-%m-%d %H:%M:%S"),
                "detected_pattern": strategy_name,
                "detected_strategy": strategy_name,
                "confidence": strategy_result.get("confidence", 60),  # Default to moderate confidence if not provided
                "direction": direction,
                "current_price": current_price,
                "entry_range": [entry_range_low, entry_range_high],
                "optimal_entry": optimal_entry,
                "targets": targets,
                "stop_loss": stop_loss,
                "risk_reward_ratio": risk_reward_ratio,
                "trends": trends,
                "btc_correlation": btc_correlation,
                "btc_dominance": self.btc_dominance,
                "support_levels": support_levels_with_strength,
                "resistance_levels": resistance_levels_with_strength,
            }
            
            return result
            
        except Exception as e:
            print(f"Error analyzing {symbol} with strategy {strategy_name}: {str(e)}")
            traceback.print_exc()
            
            return {
                "symbol": symbol,
                "error": str(e),
                "timestamp": datetime.now(pytz.timezone("America/Vancouver")).strftime("%Y-%m-%d %H:%M:%S")
            }
    
    def _select_strategy_from_menu(self, strategy_type: str) -> str:
        """
        Show a menu of strategies based on the strategy type and allow user to select one
        
        Args:
            strategy_type: Type of strategy to show options for
            
        Returns:
            Selected strategy name or None if cancelled
        """
        strategies = {
            "Strategy Blocks": [
                "1.Pullback (1D) in Uptrend",
                "2.Pullback (1W) in Uptrend",
                "3.Momentum & UpTrend (v.1)",
                "4.Momentum & UpTrend (v.2)",
                "5.Fresh Bullish Momentum (MACD Signal Line) Crossover",
                "6.Early Bullish Momentum (MACD Histogram) Inflection",
                "7.UpTrend and Fresh Momentum Inflection",
                "8.Bullish Momentum with RSI confirmation (v.1)",
                "9.Bullish Momentum with RSI confirmation (v.2)",
                "10.Bullish EMA 12/50 crossovers",
                "11.Strong UpTrend",
                "12.UpTrend",
                "13.Short-Term Trend Upgrade",
                "14.Very Oversold",
                "15.Oversold in UpTrend",
                "16.Oversold with Momentum Shift",
                "17.New Local High",  
                "18.New Local Low",  
                "19.Bullish Trading in Range (v.1)",
                "20.Bullish Trading in Range (v.2)",
                "21.Within 5% of ATH",
                "22.Within 5% of ATH and not very overbought",
                "23.Within 5% of ATH and recent bullish MACD crossover",
                "24.Within 5% of ATH and bullish inflection in MACD Histogram",
                "25.Recent ATH, pulled back but MACD is starting to inflect bullish",
                "26.Recent ATH, still within 10% of ATH, and not very overbought",
                "27.Recent ATH"
            ],
            "Trading Chart Patterns": [
                "Pattern Breakouts",
                "Emerging Patterns",
                "Pattern Breakouts and Uptrend / Downtrend",
                "Horizontal Resistance",
                "Horizontal Support",
                "Ascending Triangle",
                "Descending Triangle",
                "Channel Up",
                "Channel Down",
                "Inverse Head And Shoulders",
                "Head And Shoulders",
                "Double Top",
                "Double Bottom",
                "Triangle",
                "Rising Wedge",
                "Falling Wedge",
                "Triple Top",
                "Triple Bottom",
                "Rectangle",
                "Flag",
                "Pennant",
                "Point Retracement",
                "Point Extension",
                "ABCD",
                "Gartley",
                "BUTTERFLY",
                "Drive",
                "Consecutive Candles",
                "Big Movement"
            ],
            "Candlestick Patterns": [
                "Hammer",
                "Inverted Hammer",
                "Dragonfly Doji",
                "Perfect Dragonfly",
                "Spinning Top",
                "Hanging Man",
                "Shooting Star",
                "Gravestone Doji",
                "Perfect Gravestone",
                "Doji",
                "Kicker",
                "Engulfing",
                "Harami",
                "Piercing Line",
                "Tweezer Bottom",
                "Dark Cloud Cover",
                "Tweezer Top",
                "Morning Star",
                "Morning Doji Star",
                "Abandoned Baby",
                "Three White Soldiers",
                "Three Line Strike",
                "Three Inside Up",
                "Three Outside Up",
                "Evening Star",
                "Evening Doji Star",
                "Three Black Crows",
                "Three Inside Down",
                "Three Outside Down"
            ],
            "Trend and Momentum": [
                "MA Ribbon",
                "Price / SMA (5 and 10) Crossover",
                "Price / SMA (10 and 20) Crossover",
                "Price / SMA (30 and 50) Crossover",
                "Price / SMA (100 and 200) Crossover",
                "Price / EMA (9 and 12) Crossover",
                "Price / EMA (12 and 26) Crossover",
                "Price / EMA (50 and 100) Crossover",
                "Price / EMA (100 and 200) Crossover",
                "SMA 5 / 10 Crossover",
                "SMA 10 / 20 Crossover",
                "SMA 20 / 30 Crossover",
                "SMA 30 / 50 Crossover",
                "SMA 50 / 200 (Golden / Death) Crossover",
                "SMA 100 / 200 Crossover",
                "EMA 9 / 12 Crossover",
                "EMA 12 / 26 Crossover",
                "EMA 26 / 50 Crossover",
                "EMA 50 / 100 Crossover",
                "EMA 50 / 200 Crossover",
                "EMA 100 / 200 Crossover",
                "RVOL Spike in Uptrend / Downtrend",
                "Unusual Volume Gainers / Decliners",
                "Bollinger Band - Price Broke Upper / Lower Band",
                "On Balance Volume (OBV) Trend",
                "Top Gainers",
                "Top Losers"
            ],
            "Leading Indicator (Oscillators)": [
                "Relative Strength Index (9)",
                "Relative Strength Index (14)",
                "Relative Strength Index (25)",
                "Stochastic RSI Fast (3, 3, 14, 14)",
                "Williams Percent Range (14)",
                "Bull Power",
                "Bear Power"
            ],
            "Divergence Signal": [
                "RSI divergence"
            ]
        }
        
        if strategy_type not in strategies:
            print(f"Unknown strategy type: {strategy_type}")
            return None
        
        print(f"\nSelect {strategy_type} option:")
        options = strategies[strategy_type]
        
        for option in options:
            print(f"{option}")
        
        try:
            choice = int(input(f"\nEnter option number (1-{len(options)}): "))
            if 1 <= choice <= len(options):
                selected_option = options[choice-1]
                # Remove numbering if it exists
                if "." in selected_option and selected_option[0].isdigit():
                    selected_option = selected_option.split(".", 1)[1]
                return selected_option
            else:
                print("Invalid choice.")
                return None
        except ValueError:
            print("Invalid input. Please enter a number.")
            return None

    def _detect_strategy_block(self, df: pd.DataFrame, strategy_name: str, 
                         all_timeframes: Dict[str, pd.DataFrame],
                         support_levels: List[Tuple[float, int]] = None,
                         resistance_levels: List[Tuple[float, int]] = None) -> Dict[str, Any]:
        """
        Detect specific strategy block patterns
        """
        result = {
            "detected": False,
            "pattern": strategy_name,
            "confidence": 0,
            "direction": "NEUTRAL"
        }
        
        try:
            strategy_handlers = {
                "pullback (1d) in uptrend": self._handle_pullback_uptrend,
                "pullback (1w) in uptrend": self._handle_pullback_uptrend,
                "momentum & uptrend (v.1)": self._handle_momentum_uptrend,
                "momentum & uptrend (v.2)": self._handle_momentum_uptrend,
                "fresh bullish momentum (macd signal line) crossover": self._handle_fresh_macd_crossover,
                "early bullish momentum (macd histogram) inflection": self._handle_early_momentum_inflection,
                "uptrend and fresh momentum inflection": self._handle_uptrend_momentum_inflection,
                "bullish momentum with rsi confirmation (v.1)": self._handle_momentum_rsi_confirmation,
                "bullish momentum with rsi confirmation (v.2)": self._handle_momentum_rsi_confirmation,
                "bullish ema 12/50 crossovers": self._handle_ema_12_50_crossover,
                "strong uptrend": self._handle_strong_uptrend,
                "uptrend": self._handle_uptrend,
                "short-term trend upgrade": self._handle_short_term_trend_upgrade,
                "very oversold": self._handle_very_oversold,
                "oversold in uptrend": self._handle_oversold_in_uptrend,
                "oversold with momentum shift": self._handle_oversold_momentum_shift,
                "new local high": self._handle_new_local_extrema,
                "new local low": self._handle_new_local_extrema,
                "bullish trading in range (v.1)": self._handle_bullish_trading_in_range,
                "bullish trading in range (v.2)": self._handle_bullish_trading_in_range,
                "within 5% of ath": self._handle_ath_proximity,
                "within 5% of ath and not very overbought": self._handle_ath_proximity_not_overbought,
                "within 5% of ath and recent bullish macd crossover": self._handle_ath_proximity_macd_crossover,
                "within 5% of ath and bullish inflection in macd histogram": self._handle_ath_proximity_macd_inflection,
                "recent ath, pulled back but macd is starting to inflect bullish": self._handle_recent_ath_pullback_macd_inflection,
                "recent ath, still within 10% of ath, and not very overbought": self._handle_recent_ath_not_overbought,
                "recent ath": self._handle_recent_ath,
            }

            key = strategy_name.lower().strip()
            
            handler = None
            for handler_key, handler_func in strategy_handlers.items():
                if handler_key in key:
                    handler = handler_func
                    break
            
            if not handler:
                print(f"Warning: No handler found for strategy '{strategy_name}'")
                return {"detected": False, "pattern": strategy_name, "confidence": 0, "direction": "NEUTRAL"}
                
            # Call the handler to get the detection result
            handler_result = handler(df, key, all_timeframes, support_levels, resistance_levels)

            if handler_result and handler_result.get("detected"):
                # Get trends once, only if a signal is detected
                trends = {}
                if all_timeframes:
                    for tf in ["1h", "4h", "1d", "1w"]:
                        if tf in all_timeframes:
                            trends[tf] = self.analyze_trend(all_timeframes[tf], tf)

                final_confidence = self.calculate_signal_confidence(
                    pattern_name=strategy_name,
                    direction=handler_result["direction"],
                    df=df,
                    support_levels=support_levels,
                    resistance_levels=resistance_levels,
                    trends=trends,
                    base_confidence_override=handler_result.get("base_confidence")
                )

                return {
                    "detected": True,
                    "pattern": strategy_name,
                    "confidence": final_confidence,
                    "direction": handler_result["direction"]
                }

            return result

        except Exception as e:
            print(f"Error in strategy block detection '{strategy_name}': {str(e)}")
            traceback.print_exc()
            return result

    # --- Strategy Handler Methods (Refactored) ---

    def _handle_pullback_uptrend(self, df, key, all_timeframes, supports, resistances):
        timeframe = "1d" if "1d" in key else "1w"

        if timeframe not in all_timeframes:
            return {"detected": False}

        df_tf = all_timeframes[timeframe]

        current_price = df_tf['close'].iloc[-1]
        prev_price = df_tf['close'].iloc[-2] if len(df_tf) > 2 else current_price

        uptrend = False
        if 'ema50' in df_tf.columns and 'ema200' in df_tf.columns:
            uptrend = df_tf['ema50'].iloc[-1] > df_tf['ema200'].iloc[-1]

        if timeframe == "1d":
            pullback = current_price < prev_price
        else:
            week_ago_price = df_tf['close'].iloc[-8] if len(df_tf) > 8 else current_price
            pullback = current_price < week_ago_price

        near_support = False
        for level, strength in supports:
            if abs(current_price - level) / level < 0.03:
                near_support = True
                break

        if uptrend and pullback and near_support:
            base_confidence = 65
            if timeframe == "1w":
                base_confidence += 5
            return {"detected": True, "direction": "LONG", "base_confidence": base_confidence}

        return {"detected": False}

    def _handle_momentum_uptrend(self, df, key, all_timeframes, supports, resistances):
        if not ('ema50' in df.columns and 'ema200' in df.columns):
            return {"detected": False}
        if not (df['ema50'].iloc[-1] > df['ema200'].iloc[-1]):
            return {"detected": False}

        if 'v.1' in key:
            if not ('macd' in df.columns and 'macd_signal' in df.columns):
                return {"detected": False}
            
            macd = df['macd'].iloc[-1]
            macd_prev = df['macd'].iloc[-2]
            signal = df['macd_signal'].iloc[-1]
            signal_prev = df['macd_signal'].iloc[-2]

            if macd_prev <= signal_prev and macd > signal:
                return {"detected": True, "direction": "LONG", "base_confidence": 62}

        elif 'v.2' in key:
            if 'macd_hist' not in df.columns:
                return {"detected": False}
                
            hist = df['macd_hist'].iloc[-1]
            hist_prev = df['macd_hist'].iloc[-2]
            
            if hist > hist_prev and hist > 0:
                return {"detected": True, "direction": "LONG", "base_confidence": 62}

        return {"detected": False}

    def _handle_fresh_macd_crossover(self, df, key, all_timeframes, supports, resistances):
        if not all(col in df.columns for col in ['macd', 'macd_signal']):
            return {"detected": False}

        macd = df['macd'].iloc[-1]
        macd_prev = df['macd'].iloc[-2] if len(df) > 2 else 0
        signal = df['macd_signal'].iloc[-1]
        signal_prev = df['macd_signal'].iloc[-2] if len(df) > 2 else 0

        if macd_prev <= signal_prev and macd > signal:
            base_confidence = 59
            if macd < 0:
                base_confidence += 10
            return {"detected": True, "direction": "LONG", "base_confidence": base_confidence}
            
        return {"detected": False}

    def _handle_early_momentum_inflection(self, df, key, all_timeframes, supports, resistances):
        if 'macd_hist' not in df.columns:
            return {"detected": False}
            
        hist = df['macd_hist'].iloc[-1]
        hist_prev = df['macd_hist'].iloc[-2] if len(df) > 2 else 0
        hist_prev2 = df['macd_hist'].iloc[-3] if len(df) > 3 else 0

        if hist_prev < hist and hist_prev <= hist_prev2 and hist_prev < 0:
            base_confidence = 57
            if hist > 0:
                base_confidence += 8
            return {"detected": True, "direction": "LONG", "base_confidence": base_confidence}
            
        return {"detected": False}

    def _handle_uptrend_momentum_inflection(self, df, key, all_timeframes, supports, resistances):
        uptrend = False
        if 'ema50' in df.columns and 'ema200' in df.columns:
            uptrend = df['ema50'].iloc[-1] > df['ema200'].iloc[-1]

        hist_inflection = False
        if 'macd_hist' in df.columns:
            hist = df['macd_hist'].iloc[-1]
            hist_prev = df['macd_hist'].iloc[-2] if len(df) > 2 else 0
            hist_inflection = hist > hist_prev

        if uptrend and hist_inflection:
            return {"detected": True, "direction": "LONG", "base_confidence": 63}

        return {"detected": False}

    def _handle_momentum_rsi_confirmation(self, df, key, all_timeframes, supports, resistances):
        if not all(col in df.columns for col in ['macd', 'macd_signal', 'rsi14']):
            return {"detected": False}

        macd = df['macd'].iloc[-1]
        signal = df['macd_signal'].iloc[-1]
        rsi = df['rsi14'].iloc[-1]

        if 'v.1' in key:
            if macd > signal and rsi > 50:
                return {"detected": True, "direction": "LONG", "base_confidence": 64}
                
        elif 'v.2' in key:
            if macd > signal and rsi > 60:
                return {"detected": True, "direction": "LONG", "base_confidence": 68}
                
        return {"detected": False}

    def _handle_ema_12_50_crossover(self, df, key, all_timeframes, supports, resistances):
        if not ('ema12' in df.columns and 'ema50' in df.columns):
            return {"detected": False}

        ema12 = df['ema12'].iloc[-1]
        ema50 = df['ema50'].iloc[-1]
        ema12_prev = df['ema12'].iloc[-2] if len(df) > 2 else 0
        ema50_prev = df['ema50'].iloc[-2] if len(df) > 2 else 0
        price = df['close'].iloc[-1]

        if ema12_prev <= ema50_prev and ema12 > ema50 and price > ema12:
            return {"detected": True, "direction": "LONG", "base_confidence": 61}

        return {"detected": False}

    def _handle_strong_uptrend(self, df, key, all_timeframes, supports, resistances):
        if not all(col in df.columns for col in ['ema20', 'ema50', 'ema200']):
            return {"detected": False}

        strong_uptrend = (df['ema20'].iloc[-1] > df['ema50'].iloc[-1] > df['ema200'].iloc[-1] and
                          df['close'].iloc[-1] > df['ema20'].iloc[-1])

        if strong_uptrend:
            return {"detected": True, "direction": "LONG", "base_confidence": 69}

        return {"detected": False}

    def _handle_uptrend(self, df, key, all_timeframes, supports, resistances):
        if not ('ema50' in df.columns and 'ema200' in df.columns):
            return {"detected": False}

        uptrend = df['ema50'].iloc[-1] > df['ema200'].iloc[-1]

        if uptrend:
            return {"detected": True, "direction": "LONG", "base_confidence": 62}

        return {"detected": False}

    def _handle_short_term_trend_upgrade(self, df, key, all_timeframes, supports, resistances):
        if not ('ema12' in df.columns and 'ema26' in df.columns):
            return {"detected": False}

        ema12 = df['ema12'].iloc[-1]
        ema26 = df['ema26'].iloc[-1]
        ema12_prev = df['ema12'].iloc[-2] if len(df) > 2 else 0
        ema26_prev = df['ema26'].iloc[-2] if len(df) > 2 else 0

        if ema12_prev <= ema26_prev and ema12 > ema26:
            return {"detected": True, "direction": "LONG", "base_confidence": 55}

        return {"detected": False}

    def _handle_very_oversold(self, df, key, all_timeframes, supports, resistances):
        if 'rsi14' not in df.columns:
            return {"detected": False}

        rsi = df['rsi14'].iloc[-1]
        if rsi < 25:
            return {"detected": True, "direction": "LONG", "base_confidence": 58}

        return {"detected": False}

    def _handle_oversold_in_uptrend(self, df, key, all_timeframes, supports, resistances):
        uptrend = False
        if 'ema50' in df.columns and 'ema200' in df.columns:
            uptrend = df['ema50'].iloc[-1] > df['ema200'].iloc[-1]

        oversold = False
        if 'rsi14' in df.columns:
            oversold = df['rsi14'].iloc[-1] < 40

        if uptrend and oversold:
            return {"detected": True, "direction": "LONG", "base_confidence": 66}

        return {"detected": False}

    def _handle_oversold_momentum_shift(self, df, key, all_timeframes, supports, resistances):
        oversold = False
        if 'rsi14' in df.columns:
            oversold = df['rsi14'].iloc[-1] < 40

        momentum_shift = False
        if 'macd_hist' in df.columns:
            hist = df['macd_hist'].iloc[-1]
            hist_prev = df['macd_hist'].iloc[-2] if len(df) > 2 else 0
            momentum_shift = hist > hist_prev

        uptrend = False
        if 'ema50' in df.columns and 'ema200' in df.columns:
            uptrend = df['ema50'].iloc[-1] > df['ema200'].iloc[-1]

        if oversold and momentum_shift and uptrend:
            return {"detected": True, "direction": "LONG", "base_confidence": 63}

        return {"detected": False}

    def _handle_new_local_extrema(self, df, key, all_timeframes, supports, resistances):
        lookback = 20

        if "high" in key:
            if len(df) > lookback + 1:
                current_price = df['close'].iloc[-1]
                lookback_high = df['high'].iloc[-(lookback+1):-1].max()
                if current_price > lookback_high:
                    return {"detected": True, "direction": "LONG", "base_confidence": 59}

        elif "low" in key:
            if len(df) > lookback + 1:
                current_price = df['close'].iloc[-1]
                lookback_low = df['low'].iloc[-(lookback+1):-1].min()
                if current_price < lookback_low:
                    return {"detected": True, "direction": "SHORT", "base_confidence": 59}

        return {"detected": False}

    def _handle_bullish_trading_in_range(self, df, key, all_timeframes, supports, resistances):
        sideways_price = False
        if 'sma10' in df.columns:
            sma10_values = df['sma10'].iloc[-10:]
            x = np.arange(len(sma10_values))
            slope, _ = np.polyfit(x, sma10_values, 1)
            normalized_slope = slope / sma10_values.mean() if sma10_values.mean() > 0 else 0
            sideways_price = abs(normalized_slope) < 0.01

        long_term_uptrend = False
        if 'ema200' in df.columns:
            long_term_uptrend = df['ema200'].iloc[-1] > df['ema200'].iloc[-20] if len(df) >= 20 else False

        rsi_favorable = False
        if 'rsi14' in df.columns:
            rsi_favorable = df['rsi14'].iloc[-1] < 50

        if sideways_price and long_term_uptrend and rsi_favorable:
            recent_high = df['high'].iloc[-20:].max()
            recent_low = df['low'].iloc[-20:].min()
            current_price = df['close'].iloc[-1]

            if (recent_high - recent_low) > 0:
                position_ratio = (current_price - recent_low) / (recent_high - recent_low)
                if position_ratio < 0.3:
                    return {"detected": True, "direction": "LONG", "base_confidence": 56}

        return {"detected": False}

    def _handle_ath_proximity(self, df, key, all_timeframes, supports, resistances):
        if len(df) > 20:
            all_time_high = df['high'].max()
            current_price = df['close'].iloc[-1]
            distance_to_ath = (all_time_high - current_price) / all_time_high

            if distance_to_ath <= 0.05:
                 return {"detected": True, "direction": "LONG", "base_confidence": 55}

        return {"detected": False}

    def _handle_ath_proximity_not_overbought(self, df, key, all_timeframes, supports, resistances):
        if len(df) > 20 and 'rsi14' in df.columns:
            all_time_high = df['high'].max()
            current_price = df['close'].iloc[-1]
            distance_to_ath = (all_time_high - current_price) / all_time_high

            if distance_to_ath <= 0.05 and df['rsi14'].iloc[-1] < 70:
                return {"detected": True, "direction": "LONG", "base_confidence": 59}

        return {"detected": False}

    def _handle_ath_proximity_macd_crossover(self, df, key, all_timeframes, supports, resistances):
        if len(df) > 20 and all(c in df.columns for c in ['macd', 'macd_signal']):
            all_time_high = df['high'].max()
            current_price = df['close'].iloc[-1]
            distance_to_ath = (all_time_high - current_price) / all_time_high

            macd = df['macd'].iloc[-1]
            macd_prev = df['macd'].iloc[-2]
            signal = df['macd_signal'].iloc[-1]
            signal_prev = df['macd_signal'].iloc[-2]

            if distance_to_ath <= 0.05 and (macd_prev <= signal_prev and macd > signal):
                return {"detected": True, "direction": "LONG", "base_confidence": 62}

        return {"detected": False}

    def _handle_ath_proximity_macd_inflection(self, df, key, all_timeframes, supports, resistances):
        if len(df) > 20 and all(c in df.columns for c in ['macd_hist']):
            all_time_high = df['high'].max()
            current_price = df['close'].iloc[-1]
            distance_to_ath = (all_time_high - current_price) / all_time_high

            hist = df['macd_hist'].iloc[-1]
            hist_prev = df['macd_hist'].iloc[-2]

            if distance_to_ath <= 0.05 and hist > hist_prev:
                return {"detected": True, "direction": "LONG", "base_confidence": 61}

        return {"detected": False}

    def _handle_recent_ath_pullback_macd_inflection(self, df, key, all_timeframes, supports, resistances):
        if len(df) > 30 and 'macd_hist' in df.columns:
            all_time_high = df['high'].max()
            all_time_high_idx = df['high'].idxmax()

            if all_time_high_idx >= len(df) - 20: # Recent ATH
                current_price = df['close'].iloc[-1]
                pullback = current_price < all_time_high * 0.9
                
                hist = df['macd_hist'].iloc[-1]
                hist_prev = df['macd_hist'].iloc[-2]
                macd_inflection = hist > hist_prev
                
                if pullback and macd_inflection:
                    return {"detected": True, "direction": "LONG", "base_confidence": 63}
                    
        return {"detected": False}

    def _handle_recent_ath_not_overbought(self, df, key, all_timeframes, supports, resistances):
        if len(df) > 30 and 'rsi14' in df.columns:
            all_time_high = df['high'].max()
            all_time_high_idx = df['high'].idxmax()

            if all_time_high_idx >= len(df) - 20: # Recent ATH
                current_price = df['close'].iloc[-1]
                within_10_pct = (all_time_high - current_price) / all_time_high <= 0.10
                not_overbought = df['rsi14'].iloc[-1] < 70
                
                if within_10_pct and not_overbought:
                    return {"detected": True, "direction": "LONG", "base_confidence": 61}

        return {"detected": False}

    def _handle_recent_ath(self, df, key, all_timeframes, supports, resistances):
        if len(df) > 30:
            all_time_high = df['high'].max()
            all_time_high_idx = df['high'].idxmax()

            if all_time_high_idx >= len(df) - 20:
                return {"detected": True, "direction": "LONG", "base_confidence": 52}
                
        return {"detected": False}



    def _score_hs_volume_profile(self, df, ls, head, rs, l1, l2) -> int:
        """
        Score Head and Shoulders volume profile for pattern validation
        """
        volume_score = 0

        try:
            # Get volume slices for each part of the pattern
            volume_head = df['volume'].iloc[ls['idx']:rs['idx']].mean()
            volume_rs = df['volume'].iloc[head['idx']:rs['idx']].mean()
            breakout_volume = df['volume'].iloc[-1]
            avg_volume_20d = df['volume'].iloc[-21:-1].mean()

            # Reward for diminishing volume into the right shoulder (waning buying pressure)
            if volume_rs < volume_head * 0.8:
                volume_score += 15

            # Reward heavily for a high-volume breakout
            if breakout_volume > avg_volume_20d * 1.75:
                volume_score += 20

            return volume_score
            
        except Exception as e:
            print(f"Error in volume profile scoring: {e}")
            return 0

    def _score_hs_momentum(self, df: pd.DataFrame, recent_df: pd.DataFrame, head: dict, rs: dict, is_inverse: bool) -> int:
        """
        Score Head and Shoulders momentum divergence for pattern validation
        """
        momentum_score = 0
        
        try:
            if 'rsi14' not in df.columns:
                return 0

            # Translate the index from recent_df to the main df before looking up RSI
            main_df_start_index = df.index.get_loc(recent_df.index[0])

            rsi_head = df['rsi14'].iloc[main_df_start_index + head['idx']]
            rsi_rs = df['rsi14'].iloc[main_df_start_index + rs['idx']]

            if not is_inverse: # Standard Head & Shoulders
                # Check for classic bearish divergence
                if rs['price'] <= head['price'] and rsi_rs < rsi_head:
                    momentum_score += 20
            else: # Inverse Head & Shoulders
                # Check for classic bullish divergence
                if rs['price'] >= head['price'] and rsi_rs > rsi_head:
                    momentum_score += 20

            return momentum_score
            
        except Exception as e:
            print(f"Error in momentum scoring: {e}")
            return 0

    def calculate_enhanced_confidence(self, pattern_name: str, direction: str, df: pd.DataFrame,
                                     support_levels: List[Tuple[float, int]] = None,
                                     resistance_levels: List[Tuple[float, int]] = None,
                                     trends: Dict[str, Dict] = None,
                                     base_confidence_override: float = None) -> float:
        """
        Calculate confidence score using temp72.py's proven approach for realistic crypto market distribution
        """
        try:
            pattern_lower = pattern_name.lower()
            if base_confidence_override is not None:
                base_confidence = base_confidence_override
            else:
                # More conservative base success rates (temp72.py approach) - INCREASED for more signals
                pattern_success_rates = {
                    # Enhanced Strategy Blocks - INCREASED by 8-12 points
                    "Pullback (1D) in Uptrend": 73,
                    "Pullback (1W) in Uptrend": 71,
                    "Momentum & UpTrend (v.1)": 70,
                    "Momentum & UpTrend (v.2)": 70,
                    "Fresh Bullish Momentum (MACD Signal Line) Crossover": 67,
                    "Early Bullish Momentum (MACD Histogram) Inflection": 65,
                    "UpTrend and Fresh Momentum Inflection": 71,
                    "Bullish Momentum with RSI confirmation (v.1)": 72,
                    "Bullish Momentum with RSI confirmation (v.2)": 72,
                    "Bullish EMA 12/50 crossovers": 69,
                    "Strong UpTrend": 77,
                    "UpTrend": 70,
                    "Very Oversold": 66,
                    "Oversold in UpTrend": 74,
                    "Oversold with Momentum Shift": 71,
                    "New Local High": 67,
                    "New Local Low": 67,
                    "Bullish Trading in Range (v.1)": 64,
                    "Bullish Trading in Range (v.2)": 64,
                    "Within 5% of ATH": 63,
                    "Recent ATH": 60,

                    # Chart patterns (more conservative) - INCREASED by 8-10 points
                    "inverse_head_and_shoulders": 79,
                    "head_and_shoulders": 77,
                    "double_bottom": 77,
                    "channel_up": 68,
                    "channel_down": 67,
                    "descending_triangle": 65,
                    "double_top": 64,
                    "ascending_triangle": 63,
                    "flag": 63,
                    "bull_flag": 63,
                    "bear_flag": 63,
                    "falling_wedge": 61,
                    "rising_wedge": 60,
                    "triangle": 57,
                    "rectangle": 53,
                    "pennant": 51,

                    # Others - INCREASED by 8-10 points
                    "bullish_rsi_divergence": 67,
                    "bearish_rsi_divergence": 67,
                    "horizontal_resistance": 60,
                    "horizontal_support": 60,
                    "resistance_rejection": 58,
                    "support_bounce": 58,
                }

                # Find pattern base rate with HIGHER default
                base_confidence = 58  # INCREASED from 50
                pattern_lower = pattern_name.lower()

                for pattern_key, success_rate in pattern_success_rates.items():
                    if pattern_key.lower() in pattern_lower or pattern_lower in pattern_key.lower():
                        base_confidence = success_rate
                        break
            
            # HIGHER CAP to allow more room for adjustments
            base_confidence = min(base_confidence, 73)  # INCREASED from 65
            
            # 1. ENHANCED Trend alignment score (range: -12 to +15 points) - MORE FAVORABLE
            trend_alignment_score = 0
            trend_weights = {
                "1d": 0.45,
                "4h": 0.35, 
                "1h": 0.15,
                "1w": 0.05
            }
            
            aligned_trends = 0
            opposing_trends = 0
            total_trends = 0
            
            if trends:
                for tf, weight in trend_weights.items():
                    if tf in trends and "trend" in trends[tf]:
                        total_trends += 1
                        tf_trend = trends[tf]["trend"]
                        trend_strength = min(90, trends[tf].get("strength", 50))
                        
                        if (direction == "LONG" and tf_trend == "BULLISH") or \
                        (direction == "SHORT" and tf_trend == "BEARISH"):
                            aligned_trends += 1
                            # INCREASED bonus points for aligned trends
                            trend_alignment_score += weight * (trend_strength / 50) * 6  # INCREASED from 4
                        elif (direction == "LONG" and tf_trend == "BEARISH") or \
                            (direction == "SHORT" and tf_trend == "BULLISH"):
                            opposing_trends += 1
                            trend_alignment_score -= weight * (trend_strength / 50) * 10  # REDUCED penalty from 15
                
                # INCREASED perfect alignment bonus
                if total_trends >= 3 and aligned_trends == total_trends:
                    trend_alignment_score += 6  # INCREASED from 3
                elif total_trends >= 2 and opposing_trends == 0:
                    trend_alignment_score += 3  # INCREASED from 1
            
            trend_alignment_score = max(-12, min(15, trend_alignment_score))  # REDUCED penalty range
            
            # 2. ENHANCED Indicator confluence score (range: -10 to +12 points) - MORE FAVORABLE
            indicator_score = 0
            
            # RSI analysis - MORE FAVORABLE scoring
            if 'rsi14' in df.columns:
                rsi = df['rsi14'].iloc[-1]
                rsi_prev = df['rsi14'].iloc[-2] if len(df) > 2 else 50
                rsi_momentum = rsi - rsi_prev
                
                if direction == "LONG":
                    if rsi < 20:  # Extremely oversold
                        indicator_score += 6  # INCREASED from 4
                    elif rsi < 30:  # Strongly oversold  
                        indicator_score += 5  # INCREASED from 3
                    elif rsi < 40:  # Moderately oversold
                        indicator_score += 3  # INCREASED from 1
                    if rsi < 35 and rsi_momentum > 3:  # Stronger momentum required
                        indicator_score += 3  # INCREASED from 2
                    elif "uptrend" in pattern_lower and rsi < 50:
                        indicator_score += 2  # INCREASED from 1
                    elif rsi > 80:  # Extremely overbought
                        indicator_score -= 6  # REDUCED penalty from 8
                    elif rsi > 70:  # Overbought
                        indicator_score -= 3  # REDUCED penalty from 5
                
                elif direction == "SHORT":
                    if rsi > 80:
                        indicator_score += 6  # INCREASED
                    elif rsi > 70:
                        indicator_score += 5  # INCREASED
                    elif rsi > 60:
                        indicator_score += 3  # INCREASED
                    if rsi > 65 and rsi_momentum < -3:
                        indicator_score += 3  # INCREASED
                    elif rsi < 20:
                        indicator_score -= 6  # REDUCED penalty
                    elif rsi < 30:
                        indicator_score -= 3  # REDUCED penalty
            
            # MORE FAVORABLE MACD scoring
            if all(col in df.columns for col in ['macd', 'macd_signal', 'macd_hist']):
                macd = df['macd'].iloc[-1]
                macd_signal = df['macd_signal'].iloc[-1]
                macd_hist = df['macd_hist'].iloc[-1]
                macd_hist_prev = df['macd_hist'].iloc[-2] if len(df) > 2 else 0
                
                # Standard MACD - INCREASED bonuses
                if direction == "LONG":
                    if macd > macd_signal and macd_hist > 0:
                        indicator_score += 4  # INCREASED from 2
                    elif macd_hist > 0 and macd_hist > macd_hist_prev:
                        indicator_score += 3  # INCREASED from 1
                    elif macd_hist < 0 and macd_hist > macd_hist_prev:
                        indicator_score += 2  # INCREASED from 1
                    elif macd < macd_signal and macd_hist < 0:
                        indicator_score -= 4  # REDUCED penalty from 6
                
                elif direction == "SHORT":
                    if macd < macd_signal and macd_hist < 0:
                        indicator_score += 4  # INCREASED
                    elif macd_hist < 0 and macd_hist < macd_hist_prev:
                        indicator_score += 3  # INCREASED
                    elif macd_hist > 0 and macd_hist < macd_hist_prev:
                        indicator_score += 2  # INCREASED
                    elif macd > macd_signal and macd_hist > 0:
                        indicator_score -= 4  # REDUCED penalty
            
            # MA alignment - MORE FAVORABLE bonuses
            ma_alignment_score = 0
            if all(col in df.columns for col in ['ema20', 'ema50', 'ema200']):
                if df['ema20'].iloc[-1] > df['ema50'].iloc[-1] > df['ema200'].iloc[-1]:
                    ma_alignment_score = 4 if direction == "LONG" else -4  # INCREASED bonus, REDUCED penalty
                elif df['ema20'].iloc[-1] < df['ema50'].iloc[-1] < df['ema200'].iloc[-1]:
                    ma_alignment_score = 4 if direction == "SHORT" else -4  # INCREASED bonus, REDUCED penalty
            
            indicator_score += ma_alignment_score
            indicator_score = max(-10, min(12, indicator_score))  # MORE FAVORABLE range
            
            # 3. ENHANCED S/R confluence scoring
            sr_confluence_score = 0
            current_price = df['close'].iloc[-1]

            if direction == "LONG":
                if support_levels:
                    nearest_support_price, nearest_support_strength = support_levels[0]
                    distance_to_support_pct = (current_price - nearest_support_price) / current_price
                    if distance_to_support_pct < 0.015:
                        sr_confluence_score += nearest_support_strength * 2.5

                if resistance_levels:
                    nearest_resistance_price, nearest_resistance_strength = resistance_levels[0]
                    distance_to_resistance_pct = (nearest_resistance_price - current_price) / current_price
                    if distance_to_resistance_pct < 0.015 and nearest_resistance_strength >= 4:
                        sr_confluence_score -= 15

            elif direction == "SHORT":
                if resistance_levels:
                    nearest_resistance_price, nearest_resistance_strength = resistance_levels[0]
                    distance_to_resistance_pct = (nearest_resistance_price - current_price) / current_price
                    if distance_to_resistance_pct < 0.015:
                        sr_confluence_score += nearest_resistance_strength * 2.5

                if support_levels:
                    nearest_support_price, nearest_support_strength = support_levels[0]
                    distance_to_support_pct = (current_price - nearest_support_price) / current_price
                    if distance_to_support_pct < 0.015 and nearest_support_strength >= 4:
                        sr_confluence_score -= 15

            sr_confluence_score = max(-20, min(25, sr_confluence_score))

            # 4. ENHANCED Volume confirmation (range: -4 to +6 points) - MORE FAVORABLE
            volume_score = 0
            
            if 'volume' in df.columns and len(df) > 15:
                current_volume = df['volume'].iloc[-1]
                recent_avg_volume = df['volume'].iloc[-11:-1].mean()
                longer_avg_volume = df['volume'].iloc[-21:-11].mean() if len(df) > 21 else recent_avg_volume
                
                if current_volume > 0 and recent_avg_volume > 0:
                    volume_ratio = current_volume / recent_avg_volume
                    volume_trend = recent_avg_volume / longer_avg_volume if longer_avg_volume > 0 else 1
                    
                    if volume_ratio > 4.0:
                        volume_score += 6  # INCREASED from 4
                    elif volume_ratio > 2.5:
                        volume_score += 4  # INCREASED from 3
                    elif volume_ratio > 1.8:
                        volume_score += 3  # INCREASED from 2
                    elif volume_ratio > 1.3:
                        volume_score += 2  # INCREASED from 1
                    elif volume_ratio > 1.0:  # NEW: neutral to slightly positive volume
                        volume_score += 1  # NEW bonus for normal volume
                    elif volume_ratio < 0.5:
                        volume_score -= 4  # REDUCED penalty from 6
                    
                    if volume_trend > 1.5:
                        volume_score += 2  # INCREASED from 1
                    elif volume_trend < 0.6:
                        volume_score -= 1  # REDUCED penalty from 2
            
            volume_score = max(-4, min(6, volume_score))  # MORE FAVORABLE range

            # 5. REDUCED pattern-specific scoring (range: -3 to +5 points)
            pattern_specific_score = 0
            
            if "pullback" in pattern_lower and "uptrend" in pattern_lower:
                if len(df) > 20:
                    recent_lows = []
                    for i in range(5, 20, 5):
                        if i < len(df):
                            local_min = df['low'].iloc[max(0, i-2):min(len(df), i+3)].min()
                            recent_lows.append(local_min)
                    
                    if len(recent_lows) >= 2 and all(recent_lows[i] >= recent_lows[i-1] for i in range(1, len(recent_lows))):
                        pattern_specific_score += 4
            
            elif "macd" in pattern_lower:
                if "crossover" in pattern_lower:
                    if 'macd' in df.columns:
                        macd = df['macd'].iloc[-1]
                        if (direction == "LONG" and macd < 0) or (direction == "SHORT" and macd > 0):
                            pattern_specific_score += 3
                
                elif "histogram" in pattern_lower:
                    if 'macd_hist' in df.columns:
                        hist = df['macd_hist'].iloc[-1]
                        hist_prev = df['macd_hist'].iloc[-2] if len(df) > 2 else 0
                        hist_change = abs(hist - hist_prev) / abs(hist_prev) if hist_prev != 0 else 0
                        
                        if hist_change > 0.3:
                            pattern_specific_score += 4
                        elif hist_change > 0.15:
                            pattern_specific_score += 2
            
            elif "rsi" in pattern_lower or "oversold" in pattern_lower:
                if 'rsi14' in df.columns:
                    rsi = df['rsi14'].iloc[-1]
                    rsi_prev = df['rsi14'].iloc[-2] if len(df) > 2 else 50
                    
                    if rsi < 35 and rsi > rsi_prev and direction == "LONG":
                        pattern_specific_score += 3
                    elif rsi > 65 and rsi < rsi_prev and direction == "SHORT":
                        pattern_specific_score += 3
            
            elif "ath" in pattern_lower:
                if 'volume' in df.columns and len(df) > 10:
                    recent_vol_avg = df['volume'].iloc[-5:].mean()
                    prior_vol_avg = df['volume'].iloc[-10:-5].mean()
                    
                    if recent_vol_avg > prior_vol_avg * 1.5:
                        pattern_specific_score += 4
            
            pattern_specific_score = max(-3, min(5, pattern_specific_score))
            
            # --- Final Confidence Calculation ---
            base_weight = 0.40
            trend_weight = 0.25
            indicator_weight = 0.10
            sr_confluence_weight = 0.25

            weighted_confidence = (base_confidence * base_weight) + \
                                  ((50 + trend_alignment_score) * trend_weight) + \
                                  ((50 + indicator_score) * indicator_weight) + \
                                  ((50 + sr_confluence_score) * sr_confluence_weight)

            # Add volume and pattern-specific scores as final adjustments
            final_confidence = weighted_confidence + volume_score + pattern_specific_score

            # REDUCED volatility penalty to increase overall scores
            if 'atr_percent' in df.columns:
                atr_percent = df['atr_percent'].iloc[-1]
                if atr_percent > 25:
                    volatility_penalty = 15  # REDUCED from 25
                elif atr_percent > 20:
                    volatility_penalty = 12  # REDUCED from 20
                elif atr_percent > 15:
                    volatility_penalty = 8   # REDUCED from 15
                elif atr_percent > 12:
                    volatility_penalty = 5   # REDUCED from 10
                elif atr_percent > 8:
                    volatility_penalty = 2   # REDUCED from 5
                else:
                    volatility_penalty = 0
                final_confidence -= volatility_penalty

            # More favorable randomization
            pattern_hash = sum(ord(c) for c in pattern_name)
            symbol_string = df.index.name if df.index.name else "default"
            symbol_hash = sum(ord(c) for c in symbol_string)
            combined_hash = (pattern_hash + symbol_hash) % 100
            
            if combined_hash < 15:
                random_adjustment = -3  # REDUCED from -6
            elif combined_hash < 30:
                random_adjustment = -1  # REDUCED from -3
            elif combined_hash < 45:
                random_adjustment = 0   # INCREASED from -1
            elif combined_hash < 55:
                random_adjustment = +1  # INCREASED from 0
            elif combined_hash < 70:
                random_adjustment = +2  # INCREASED from +1
            elif combined_hash < 85:
                random_adjustment = +4  # INCREASED from +3
            else:
                random_adjustment = +6  # UNCHANGED
            final_confidence += random_adjustment

            return max(40, min(95, final_confidence))  # INCREASED minimum from 30 to 40

        except Exception as e:
            # logger.error(f"Error in confidence scoring: {e}")
            return None

            final_confidence += random_adjustment

            return max(30, min(95, final_confidence))
            
        except Exception as e:
            print(f"Error calculating signal confidence: {str(e)}")
            return 50  # Lower default
    
    def _calculate_sophisticated_trend_alignment(self, direction, trends):
        """Sophisticated trend alignment with DRAMATICALLY HIGH IMPACT scoring (-20 to +40)"""
        if not trends:
            return -20  # Dramatically stronger penalty for missing trend data
            
        bonus = 0
        aligned_count = 0
        conflict_count = 0
        
        # DRAMATICALLY HIGHER weighted scoring by timeframe importance
        timeframe_weights = {"1h": 3, "4h": 6, "1d": 12, "1w": 15}  # Dramatically increased weights
        
        for tf, weight in timeframe_weights.items():
            if tf in trends:
                trend_data = trends[tf]
                expected_trend = "BULLISH" if direction == "LONG" else "BEARISH"
                actual_trend = trend_data.get("trend", "NEUTRAL")
                
                if actual_trend == expected_trend:
                    bonus += weight * 1.5  # Dramatically increased multiplier
                    aligned_count += 1
                elif actual_trend == "NEUTRAL":
                    bonus += weight * 0.1  # Dramatically reduced partial credit
                else:
                    bonus -= weight * 2.0  # DRAMATICALLY stronger penalty for conflict
                    conflict_count += 1
        
        # DRAMATICALLY LARGER alignment bonuses
        if aligned_count == 4:
            bonus += 20  # Massive perfect alignment bonus
        elif aligned_count >= 3:
            bonus += 12  # Dramatically increased
        elif aligned_count <= 1:
            bonus -= 15  # Dramatically increased poor alignment penalty
        
        # DRAMATICALLY STRONGER conflict penalties
        if conflict_count >= 3:
            bonus -= 25  # Massive conflict penalty
        elif conflict_count >= 2:
            bonus -= 15  # Dramatically increased
        
        return max(-20, min(40, bonus))  # Dramatically expanded range
    
    def _calculate_sophisticated_volume_analysis(self, df, direction):
        """Sophisticated volume analysis with DRAMATICALLY HIGH IMPACT (-15 to +30)"""
        try:
            if 'volume' not in df.columns:
                return -15  # Dramatically stronger penalty for missing volume
                
            bonus = 0
            recent_vol = df['volume'].iloc[-1]
            avg_vol_20 = df['volume'].iloc[-20:].mean()
            avg_vol_5 = df['volume'].iloc[-5:].mean()
            
            # DRAMATICALLY HIGHER volume surge impact
            vol_ratio = recent_vol / avg_vol_20 if avg_vol_20 > 0 else 1
            if vol_ratio > 5.0:
                bonus += 20  # Absolutely extreme volume mega bonus
            elif vol_ratio > 4.0:
                bonus += 15  # Dramatic increase
            elif vol_ratio > 3.0:
                bonus += 12  # Dramatic increase
            elif vol_ratio > 2.0:
                bonus += 8   # Dramatic increase
            elif vol_ratio > 1.5:
                bonus += 4   # Dramatic increase
            elif vol_ratio > 1.2:
                bonus += 2   # Dramatic increase
            elif vol_ratio < 0.2:
                bonus -= 15  # Dramatically stronger penalty
            elif vol_ratio < 0.3:
                bonus -= 12  # Dramatically stronger penalty
            elif vol_ratio < 0.6:
                bonus -= 6   # Dramatically stronger penalty
            
            # DRAMATICALLY STRONGER volume trend impact
            vol_trend_ratio = avg_vol_5 / avg_vol_20 if avg_vol_20 > 0 else 1
            if vol_trend_ratio > 3.0:
                bonus += 10  # Dramatic increase
            elif vol_trend_ratio > 2.0:
                bonus += 7   # Dramatic increase
            elif vol_trend_ratio > 1.5:
                bonus += 5   # Dramatic increase
            elif vol_trend_ratio > 1.2:
                bonus += 2   # Dramatic increase
            elif vol_trend_ratio < 0.3:
                bonus -= 10  # Dramatically stronger penalty
            elif vol_trend_ratio < 0.5:
                bonus -= 7   # Dramatically stronger penalty
            elif vol_trend_ratio < 0.7:
                bonus -= 4   # Dramatically stronger penalty
                
            return max(-15, min(30, bonus))  # Dramatically expanded range
            
        except Exception:
            return -5  # Penalty for calculation errors
    
    def _calculate_sophisticated_sr_confluence(self, df, direction, support_levels, resistance_levels):
        """Sophisticated S/R confluence analysis with DRAMATICALLY HIGH IMPACT (0 to +25)"""
        try:
            if not support_levels and not resistance_levels:
                return 0
                
            bonus = 0
            current_price = df['close'].iloc[-1]
            
            # DRAMATICALLY HIGHER proximity impact with tighter levels
            proximity_levels = [0.002, 0.005, 0.010, 0.020]  # 0.2%, 0.5%, 1.0%, 2.0% - tighter levels
            proximity_scores = [15, 10, 6, 3]  # DRAMATICALLY HIGHER scores
            
            if direction == "LONG" and support_levels:
                for level, strength in support_levels[:5]:  # Check top 5 supports
                    distance_pct = abs(current_price - level) / current_price
                    for prox, score in zip(proximity_levels, proximity_scores):
                        if distance_pct <= prox:
                            # DRAMATICALLY STRONGER strength multiplier
                            strength_multiplier = min(3.0, strength / 6)  # Increased multiplier
                            bonus += score * strength_multiplier
                            break
                            
            elif direction == "SHORT" and resistance_levels:
                for level, strength in resistance_levels[:5]:  # Check top 5 resistances
                    distance_pct = abs(current_price - level) / current_price
                    for prox, score in zip(proximity_levels, proximity_scores):
                        if distance_pct <= prox:
                            strength_multiplier = min(3.0, strength / 6)  # Increased multiplier
                            bonus += score * strength_multiplier
                            break
                            
            return min(25, bonus)  # Dramatically expanded range
            
        except Exception:
            return 0
    
    def _calculate_sophisticated_microstructure(self, df, direction):
        """Sophisticated microstructure analysis with HIGH IMPACT (-6 to +10)"""
        try:
            bonus = 0
            
            # MUCH STRONGER price action quality impact (last 20 periods)
            if len(df) >= 20:
                recent_closes = df['close'].iloc[-20:]
                recent_highs = df['high'].iloc[-20:]
                recent_lows = df['low'].iloc[-20:]
                
                # ENHANCED trend consistency with higher impact
                if direction == "LONG":
                    higher_lows = sum(1 for i in range(1, len(recent_lows)) if recent_lows.iloc[i] > recent_lows.iloc[i-1])
                    if higher_lows >= 15:
                        bonus += 6  # Excellent trend structure
                    elif higher_lows >= 12:
                        bonus += 4
                    elif higher_lows >= 8:
                        bonus += 2
                    elif higher_lows <= 4:
                        bonus -= 4  # Poor trend structure
                    elif higher_lows <= 6:
                        bonus -= 2
                else:  # SHORT
                    lower_highs = sum(1 for i in range(1, len(recent_highs)) if recent_highs.iloc[i] < recent_highs.iloc[i-1])
                    if lower_highs >= 15:
                        bonus += 6
                    elif lower_highs >= 12:
                        bonus += 4
                    elif lower_highs >= 8:
                        bonus += 2
                    elif lower_highs <= 4:
                        bonus -= 4
                    elif lower_highs <= 6:
                        bonus -= 2
                
                # MUCH STRONGER volatility impact
                price_volatility = recent_closes.std() / recent_closes.mean() if recent_closes.mean() > 0 else 0
                if 0.008 <= price_volatility <= 0.03:  # Optimal volatility range
                    bonus += 4
                elif 0.03 < price_volatility <= 0.06:
                    bonus += 1
                elif price_volatility > 0.15:  # Extreme volatility penalty
                    bonus -= 6
                elif price_volatility > 0.10:
                    bonus -= 3
                elif price_volatility < 0.003:  # Stagnant market penalty
                    bonus -= 3
            
            # ENHANCED wick analysis with higher impact
            if len(df) >= 8:
                recent_wicks = []
                for i in range(-8, 0):
                    high = df['high'].iloc[i]
                    low = df['low'].iloc[i]
                    close = df['close'].iloc[i]
                    open_price = df['open'].iloc[i] if 'open' in df.columns else close
                    
                    body_size = abs(close - open_price)
                    upper_wick = high - max(close, open_price)
                    lower_wick = min(close, open_price) - low
                    
                    if body_size > 0:
                        upper_wick_ratio = upper_wick / body_size
                        lower_wick_ratio = lower_wick / body_size
                        recent_wicks.append((upper_wick_ratio, lower_wick_ratio))
                
                if recent_wicks:
                    avg_upper_wick = sum(w[0] for w in recent_wicks) / len(recent_wicks)
                    avg_lower_wick = sum(w[1] for w in recent_wicks) / len(recent_wicks)
                    
                    if direction == "LONG":
                        if avg_lower_wick > 2.0:  # Strong lower rejection
                            bonus += 4
                        elif avg_lower_wick > 1.2:
                            bonus += 2
                        if avg_upper_wick > 2.0:  # Upper rejection (bad for longs)
                            bonus -= 2
                    else:  # SHORT
                        if avg_upper_wick > 2.0:  # Strong upper rejection
                            bonus += 4
                        elif avg_upper_wick > 1.2:
                            bonus += 2
                        if avg_lower_wick > 2.0:  # Lower rejection (bad for shorts)
                            bonus -= 2
                        
            return max(-6, min(10, bonus))
            
        except Exception:
            return -1
    
    def _calculate_sophisticated_risk_reward(self, df, direction):
        """Sophisticated risk-reward analysis with HIGH IMPACT (-3 to +8)"""
        try:
            bonus = 0
            current_price = df['close'].iloc[-1]
            
            # MUCH STRONGER ATR-based risk assessment
            if 'atr' in df.columns:
                atr = df['atr'].iloc[-1]
                risk_pct = (atr / current_price) * 100
                
                # ENHANCED optimal risk zones for crypto
                if 1.2 <= risk_pct <= 2.0:  # Perfect sweet spot
                    bonus += 6
                elif 0.8 <= risk_pct <= 3.0:  # Good range
                    bonus += 3
                elif 3.0 < risk_pct <= 4.5:  # Acceptable
                    bonus += 1
                elif 4.5 < risk_pct <= 6.0:  # Higher risk
                    bonus -= 1
                elif risk_pct > 8:  # Very risky
                    bonus -= 4
                elif risk_pct < 0.5:  # Too tight (likely to get stopped)
                    bonus -= 3
            
            # MUCH STRONGER position in range assessment
            if len(df) >= 40:
                recent_high = df['high'].iloc[-40:].max()
                recent_low = df['low'].iloc[-40:].min()
                range_size = recent_high - recent_low
                
                if range_size > 0:
                    position_in_range = (current_price - recent_low) / range_size
                    
                    if direction == "LONG":
                        if position_in_range < 0.15:  # Very near range low - excellent R:R
                            bonus += 5
                        elif position_in_range < 0.3:  # Near range low - good R:R
                            bonus += 3
                        elif position_in_range < 0.45:
                            bonus += 1
                        elif position_in_range > 0.85:  # Very near range high - poor R:R
                            bonus -= 4
                        elif position_in_range > 0.7:
                            bonus -= 2
                    else:  # SHORT
                        if position_in_range > 0.85:  # Very near range high - excellent R:R
                            bonus += 5
                        elif position_in_range > 0.7:  # Near range high - good R:R
                            bonus += 3
                        elif position_in_range > 0.55:
                            bonus += 1
                        elif position_in_range < 0.15:  # Very near range low - poor R:R
                            bonus -= 4
                        elif position_in_range < 0.3:
                            bonus -= 2
                            
            return max(-3, min(8, bonus))
            
        except Exception:
            return 0
    
    def _calculate_sophisticated_momentum(self, df, direction):
        """Sophisticated momentum analysis with DRAMATICALLY HIGH IMPACT (-8 to +15)"""
        try:
            bonus = 0
            
            # RSI momentum with multiple timeframes - DRAMATICALLY INCREASED IMPACT
            if 'rsi14' in df.columns and len(df) >= 15:
                rsi = df['rsi14'].iloc[-1]
                rsi_5_avg = df['rsi14'].iloc[-5:].mean()
                rsi_10_avg = df['rsi14'].iloc[-10:-5].mean()
                
                if direction == "LONG":
                    # Oversold recovery - DRAMATICALLY INCREASED
                    if rsi < 25 and rsi_5_avg > rsi_10_avg:
                        bonus += 12  # Dramatically increased from 3
                    elif rsi < 35 and rsi_5_avg > rsi_10_avg + 3:
                        bonus += 8   # Dramatically increased from 2
                    elif rsi < 45 and rsi_5_avg > rsi_10_avg + 2:
                        bonus += 5   # Dramatically increased
                    elif rsi > 75:  # Overbought warning
                        bonus -= 6   # Dramatically increased penalty
                    elif rsi > 70:
                        bonus -= 3   # Dramatically increased penalty
                else:  # SHORT
                    # Overbought breakdown - DRAMATICALLY INCREASED
                    if rsi > 75 and rsi_5_avg < rsi_10_avg:
                        bonus += 12  # Dramatically increased from 3
                    elif rsi > 65 and rsi_5_avg < rsi_10_avg - 3:
                        bonus += 8   # Dramatically increased from 2
                    elif rsi > 55 and rsi_5_avg < rsi_10_avg - 2:
                        bonus += 5   # Dramatically increased
                    elif rsi < 25:  # Oversold warning
                        bonus -= 6   # Dramatically increased penalty
                    elif rsi < 30:
                        bonus -= 3   # Dramatically increased penalty
            
            # MACD momentum quality - DRAMATICALLY INCREASED IMPACT
            if all(col in df.columns for col in ['macd', 'macd_signal', 'macd_hist']):
                macd_hist = df['macd_hist'].iloc[-1]
                macd_hist_prev = df['macd_hist'].iloc[-2]
                macd_hist_trend = df['macd_hist'].iloc[-3:].mean() - df['macd_hist'].iloc[-6:-3].mean()
                
                momentum_aligned = ((direction == "LONG" and macd_hist > macd_hist_prev and macd_hist_trend > 0) or
                                  (direction == "SHORT" and macd_hist < macd_hist_prev and macd_hist_trend < 0))
                
                if momentum_aligned:
                    if abs(macd_hist_trend) > 0.002:  # Very strong momentum
                        bonus += 8   # Dramatically increased from 3
                    elif abs(macd_hist_trend) > 0.001:  # Strong momentum
                        bonus += 5   # Dramatically increased
                    else:
                        bonus += 2   # Dramatically increased from 1
                elif ((direction == "LONG" and macd_hist < 0 and macd_hist_trend < 0) or
                      (direction == "SHORT" and macd_hist > 0 and macd_hist_trend > 0)):
                    bonus -= 5   # Dramatically increased penalty from -1
                    
            return max(-8, min(15, bonus))  # Dramatically expanded range
            
        except Exception:
            return 0
    
    def _calculate_market_regime_factor(self, df, direction, trends):
        """Market regime alignment factor with DRAMATICALLY HIGH IMPACT (-10 to +20)"""
        try:
            bonus = 0
            
            # Overall market trend strength - DRAMATICALLY INCREASED IMPACT
            if trends and '1d' in trends:
                daily_trend = trends['1d']
                trend_strength = daily_trend.get('strength', 0)
                
                if direction == "LONG":
                    if daily_trend.get('trend') == 'BULLISH' and trend_strength > 70:
                        bonus += 15  # Dramatically increased from 3
                    elif daily_trend.get('trend') == 'BULLISH' and trend_strength > 50:
                        bonus += 10  # Dramatically increased from 2
                    elif daily_trend.get('trend') == 'BULLISH' and trend_strength > 30:
                        bonus += 5   # Dramatically increased
                    elif daily_trend.get('trend') == 'BEARISH' and trend_strength > 60:
                        bonus -= 8   # Dramatically increased penalty
                    elif daily_trend.get('trend') == 'BEARISH' and trend_strength > 40:
                        bonus -= 5   # Dramatically increased penalty
                else:  # SHORT
                    if daily_trend.get('trend') == 'BEARISH' and trend_strength > 70:
                        bonus += 15  # Dramatically increased from 3
                    elif daily_trend.get('trend') == 'BEARISH' and trend_strength > 50:
                        bonus += 10  # Dramatically increased from 2
                    elif daily_trend.get('trend') == 'BEARISH' and trend_strength > 30:
                        bonus += 5   # Dramatically increased
                    elif daily_trend.get('trend') == 'BULLISH' and trend_strength > 60:
                        bonus -= 8   # Dramatically increased penalty
                    elif daily_trend.get('trend') == 'BULLISH' and trend_strength > 40:
                        bonus -= 5   # Dramatically increased penalty
            
            # Volatility regime - DRAMATICALLY INCREASED IMPACT
            if 'atr_percent' in df.columns and len(df) >= 50:
                current_vol = df['atr_percent'].iloc[-10:].mean()
                historical_vol = df['atr_percent'].iloc[-50:].mean()
                
                if historical_vol > 0:
                    vol_ratio = current_vol / historical_vol
                    if 0.7 <= vol_ratio <= 1.4:  # Stable volatility
                        bonus += 5   # Dramatically increased from 2
                    elif vol_ratio > 2.5:  # Very high volatility regime
                        bonus -= 5   # Dramatically increased penalty
                    elif vol_ratio > 1.8:  # High volatility regime
                        bonus -= 3   # Dramatically increased penalty
                        
            return max(-10, min(20, bonus))  # Dramatically expanded range
            
        except Exception:
            return 0
    
    def _calculate_pattern_completion_quality(self, df, pattern_name):
        """Enhanced crypto-specific pattern completion quality assessment with ULTRA HIGH IMPACT (0 to +15)"""
        try:
            bonus = 0
            pattern_key = pattern_name.lower()
            current_price = df['close'].iloc[-1]
            
            # CRYPTO-SPECIFIC Time-based pattern maturity with volatility adjustment
            if any(word in pattern_key for word in ['head', 'shoulders', 'double', 'triangle']):
                # Chart patterns need proper development time - adjusted for crypto volatility
                pattern_age_bonus = 0
                if len(df) >= 60:
                    pattern_age_bonus = 5  # Extended time for pattern maturity
                elif len(df) >= 40:
                    pattern_age_bonus = 4
                elif len(df) >= 30:
                    pattern_age_bonus = 3
                elif len(df) >= 20:
                    pattern_age_bonus = 2
                
                # CRYPTO ENHANCEMENT: Adjust for volatility regime
                if 'atr_percent' in df.columns and len(df) >= 20:
                    volatility = df['atr_percent'].iloc[-10:].mean()
                    if volatility > 0.08:  # High volatility crypto market
                        pattern_age_bonus = int(pattern_age_bonus * 1.3)  # Patterns mature faster in high vol
                    elif volatility < 0.02:  # Low volatility
                        pattern_age_bonus = max(1, int(pattern_age_bonus * 0.7))  # Need more time
                
                bonus += pattern_age_bonus
            
            # CRYPTO-ENHANCED Pattern-specific quality checks with smart money integration
            if 'pullback' in pattern_key:
                pullback_quality = self._assess_crypto_pullback_quality(df, pattern_key)
                bonus += pullback_quality
                
            elif 'momentum' in pattern_key or 'macd' in pattern_key:
                momentum_quality = self._assess_crypto_momentum_quality(df, pattern_key)
                bonus += momentum_quality
            
            elif 'divergence' in pattern_key:
                divergence_quality = self._assess_crypto_divergence_quality(df, pattern_key)
                bonus += divergence_quality
                    
            elif 'breakout' in pattern_key or 'breakdown' in pattern_key:
                breakout_quality = self._assess_crypto_breakout_quality(df, pattern_key)
                bonus += breakout_quality
            
            # CRYPTO-SPECIFIC: Volume profile pattern integration
            volume_profile_bonus = self._get_volume_profile_pattern_bonus(df, pattern_key)
            bonus += volume_profile_bonus
            
            # CRYPTO-SPECIFIC: Smart money pattern confirmation
            smart_money_bonus = self._get_smart_money_pattern_confirmation(df, pattern_key)
            bonus += smart_money_bonus
            
            # CRYPTO-SPECIFIC: Whale activity pattern enhancement
            whale_activity_bonus = self._get_whale_activity_pattern_bonus(df, pattern_key)
            bonus += whale_activity_bonus
            
            return min(15, bonus)  # Dramatically expanded maximum for crypto-specific analysis
            
        except Exception:
            return 0
    
    def _assess_crypto_pullback_quality(self, df, pattern_key):
        """Crypto-specific pullback quality assessment"""
        try:
            bonus = 0
            if len(df) >= 20:
                recent_range = df['high'].iloc[-10:].max() - df['low'].iloc[-10:].min()
                longer_range = df['high'].iloc[-20:].max() - df['low'].iloc[-20:].min()
                
                if longer_range > 0:
                    pullback_ratio = recent_range / longer_range
                    
                    # CRYPTO-ENHANCED: More precise pullback quality for crypto volatility
                    if 0.20 <= pullback_ratio <= 0.50:  # Optimal crypto pullback
                        bonus += 5
                    elif 0.15 <= pullback_ratio <= 0.65:  # Good pullback range
                        bonus += 3
                    elif 0.10 <= pullback_ratio <= 0.75:  # Acceptable range
                        bonus += 1
                    elif pullback_ratio > 0.85:  # Too deep - likely trend change
                        bonus -= 2
                    elif pullback_ratio < 0.08:  # Too shallow - weak pullback
                        bonus -= 1
                    
                    # CRYPTO BONUS: Volume confirmation during pullback
                    if 'volume' in df.columns:
                        pullback_vol = df['volume'].iloc[-10:].mean()
                        trend_vol = df['volume'].iloc[-20:-10].mean()
                        if pullback_vol < trend_vol * 0.7:  # Lower volume on pullback (healthy)
                            bonus += 2
            
            return bonus
        except Exception:
            return 0
    
    def _assess_crypto_momentum_quality(self, df, pattern_key):
        """Crypto-specific momentum quality assessment"""
        try:
            bonus = 0
            
            # Enhanced momentum signal recency for crypto
            if 'fresh' in pattern_key or 'emerging' in pattern_key:
                bonus += 4  # Fresh signals are crucial in crypto
            elif 'recent' in pattern_key:
                bonus += 3
            elif 'developing' in pattern_key:
                bonus += 2
            
            # CRYPTO ENHANCEMENT: RSI momentum quality
            if 'rsi14' in df.columns and len(df) >= 15:
                rsi = df['rsi14'].iloc[-1]
                rsi_change = df['rsi14'].iloc[-1] - df['rsi14'].iloc[-5]
                
                if 'bullish' in pattern_key or 'long' in pattern_key:
                    if rsi < 40 and rsi_change > 5:  # Strong momentum from oversold
                        bonus += 3
                    elif rsi < 50 and rsi_change > 3:
                        bonus += 2
                elif 'bearish' in pattern_key or 'short' in pattern_key:
                    if rsi > 60 and rsi_change < -5:  # Strong momentum from overbought
                        bonus += 3
                    elif rsi > 50 and rsi_change < -3:
                        bonus += 2
            
            return bonus
        except Exception:
            return 0
    
    def _assess_crypto_divergence_quality(self, df, pattern_key):
        """Crypto-specific divergence quality assessment"""
        try:
            bonus = 0
            
            # Enhanced divergence classification for crypto
            if 'hidden' in pattern_key:
                bonus += 2  # Hidden divergences are actually strong in crypto trends
            elif 'regular' in pattern_key or 'classic' in pattern_key:
                bonus += 3  # Regular divergences for reversal signals
            else:
                bonus += 2  # Default divergence bonus
            
            # CRYPTO ENHANCEMENT: Multi-timeframe divergence strength
            if 'strong' in pattern_key or 'confirmed' in pattern_key:
                bonus += 2
            elif 'weak' in pattern_key or 'minor' in pattern_key:
                bonus -= 1
            
            return bonus
        except Exception:
            return 0
    
    def _assess_crypto_breakout_quality(self, df, pattern_key):
        """Crypto-specific breakout quality assessment"""
        try:
            bonus = 0
            
            if len(df) >= 10:
                # CRYPTO-ENHANCED: Volume surge analysis for breakouts
                recent_volume = df['volume'].iloc[-3:].mean()
                avg_volume = df['volume'].iloc[-20:].mean()
                
                if recent_volume > avg_volume * 3.0:  # Massive volume surge
                    bonus += 5
                elif recent_volume > avg_volume * 2.0:  # Strong volume confirmation
                    bonus += 4
                elif recent_volume > avg_volume * 1.5:  # Good volume confirmation
                    bonus += 3
                elif recent_volume > avg_volume * 1.2:  # Moderate confirmation
                    bonus += 1
                elif recent_volume < avg_volume * 0.8:  # Weak volume - concerning
                    bonus -= 2
                
                # CRYPTO ENHANCEMENT: Price momentum quality
                if len(df) >= 5:
                    price_momentum = (df['close'].iloc[-1] - df['close'].iloc[-5]) / df['close'].iloc[-5]
                    if 'breakout' in pattern_key and price_momentum > 0.03:  # 3%+ move
                        bonus += 2
                    elif 'breakdown' in pattern_key and price_momentum < -0.03:
                        bonus += 2
            
            return bonus
        except Exception:
            return 0
    
    def _get_volume_profile_pattern_bonus(self, df, pattern_key):
        """Get volume profile enhancement bonus for pattern quality"""
        try:
            bonus = 0
            
            # Get volume profile data for pattern confirmation
            volume_profile = self.get_professional_volume_profile(df)
            if not volume_profile:
                return 0
            
            poc_price = volume_profile.get('poc_price', 0)
            vah_price = volume_profile.get('vah_price', 0)
            val_price = volume_profile.get('val_price', 0)
            current_price = df['close'].iloc[-1]
            
            # Pattern-specific volume profile bonuses
            if any(word in pattern_key for word in ['support', 'bounce', 'long']):
                # For bullish patterns, proximity to volume support levels
                if poc_price > 0:
                    poc_distance = abs(current_price - poc_price) / current_price
                    if poc_distance < 0.01 and current_price >= poc_price * 0.998:  # Near POC from above
                        bonus += 3
                elif val_price > 0:
                    val_distance = abs(current_price - val_price) / current_price
                    if val_distance < 0.015 and current_price >= val_price * 0.995:  # Near VAL
                        bonus += 2
            
            elif any(word in pattern_key for word in ['resistance', 'rejection', 'short']):
                # For bearish patterns, proximity to volume resistance levels
                if poc_price > 0:
                    poc_distance = abs(current_price - poc_price) / current_price
                    if poc_distance < 0.01 and current_price <= poc_price * 1.002:  # Near POC from below
                        bonus += 3
                elif vah_price > 0:
                    vah_distance = abs(current_price - vah_price) / current_price
                    if vah_distance < 0.015 and current_price <= vah_price * 1.005:  # Near VAH
                        bonus += 2
            
            return bonus
        except Exception:
            return 0
    
    def _get_smart_money_pattern_confirmation(self, df, pattern_key):
        """Get smart money confirmation bonus for pattern quality"""
        try:
            bonus = 0
            
            # Use our enhanced smart money detection for pattern confirmation
            smart_money_score = self._detect_smart_money_activity(df)
            
            if smart_money_score > 70:  # Strong smart money activity
                if any(word in pattern_key for word in ['bullish', 'long', 'breakout', 'bounce']):
                    bonus += 3  # Smart money aligns with bullish pattern
                elif any(word in pattern_key for word in ['bearish', 'short', 'breakdown', 'rejection']):
                    bonus -= 1  # Smart money conflicts with bearish pattern
            elif smart_money_score < 30:  # Strong bearish smart money activity
                if any(word in pattern_key for word in ['bearish', 'short', 'breakdown', 'rejection']):
                    bonus += 3  # Smart money aligns with bearish pattern
                elif any(word in pattern_key for word in ['bullish', 'long', 'breakout', 'bounce']):
                    bonus -= 1  # Smart money conflicts with bullish pattern
            
            return bonus
        except Exception:
            return 0
    
    def _get_whale_activity_pattern_bonus(self, df, pattern_key):
        """Get whale activity bonus for pattern quality"""
        try:
            bonus = 0
            
            if 'volume' not in df.columns or len(df) < 20:
                return 0
            
            # Detect potential whale activity through volume analysis
            recent_volume = df['volume'].iloc[-5:]
            avg_volume = df['volume'].iloc[-20:].mean()
            
            # Look for whale accumulation/distribution patterns
            whale_activity_detected = False
            for vol in recent_volume:
                if vol > avg_volume * 4:  # Potential whale transaction
                    whale_activity_detected = True
                    break
            
            if whale_activity_detected:
                recent_price_change = (df['close'].iloc[-1] - df['close'].iloc[-5]) / df['close'].iloc[-5]
                
                # Whale accumulation (large volume + price stability/increase)
                if recent_price_change > -0.02:  # Price stable or up despite large volume
                    if any(word in pattern_key for word in ['bullish', 'long', 'support', 'bounce']):
                        bonus += 2  # Whale accumulation supports bullish pattern
                
                # Whale distribution (large volume + price decrease)
                elif recent_price_change < -0.02:
                    if any(word in pattern_key for word in ['bearish', 'short', 'resistance', 'rejection']):
                        bonus += 2  # Whale distribution supports bearish pattern
            
            return bonus
        except Exception:
            return 0
    
    def _apply_pattern_specific_adjustments(self, pattern_name, confidence, df):
        """Apply pattern-specific confidence adjustments (-3 to +3)"""
        try:
            adjustment = 0
            pattern_key = pattern_name.lower()
            
            # Pattern-specific refinements
            if 'divergence' in pattern_key:
                # Divergence patterns need strong confirmation
                if 'hidden' in pattern_key:
                    adjustment -= 1  # Hidden divergences are less reliable
                if 'emerging' in pattern_key:
                    adjustment -= 1  # Emerging signals need more confirmation
                    
            elif 'breakout' in pattern_key:
                # Breakout patterns need volume confirmation
                if 'volume' in df.columns:
                    recent_vol = df['volume'].iloc[-1]
                    avg_vol = df['volume'].iloc[-20:].mean()
                    if recent_vol > avg_vol * 1.5:
                        adjustment += 2
                    else:
                        adjustment -= 2  # Breakout without volume is weak
                        
            elif 'ath' in pattern_key:
                # ATH patterns are inherently riskier
                adjustment -= 2
                
            elif any(word in pattern_key for word in ['uptrend', 'momentum']):
                # Trend following gets slight boost in crypto
                adjustment += 1
                
            return max(-3, min(3, adjustment))
            
        except Exception:
            return 0
    
    def _calculate_market_condition_modifier(self, df, trends):
        """Calculate market condition modifier (0.85 to 1.15)"""
        try:
            modifier = 1.0
            
            # General market strength assessment
            if trends:
                bullish_count = sum(1 for tf_data in trends.values() if tf_data.get('trend') == 'BULLISH')
                bearish_count = sum(1 for tf_data in trends.values() if tf_data.get('trend') == 'BEARISH')
                
                if bullish_count >= 3:  # Strong bull market
                    modifier += 0.05
                elif bearish_count >= 3:  # Strong bear market
                    modifier -= 0.05
            
            # Volatility environment impact
            if 'atr_percent' in df.columns and len(df) >= 30:
                current_vol = df['atr_percent'].iloc[-5:].mean()
                avg_vol = df['atr_percent'].iloc[-30:].mean()
                
                if avg_vol > 0:
                    vol_ratio = current_vol / avg_vol
                    if vol_ratio > 1.8:  # High volatility environment
                        modifier -= 0.10  # Reduce confidence in volatile markets
                    elif vol_ratio < 0.6:  # Low volatility
                        modifier -= 0.05  # Also reduce for stagnant markets
                        
            return max(0.85, min(1.15, modifier))
            
        except Exception:
            return 1.0

    def _calculate_professional_trend_alignment(self, direction, trends):
        """Calculate trend alignment bonus with professional weighting"""
        if not trends:
            return 0
            
        bonus = 0
        aligned_count = 0
        
        # Timeframe weights (higher timeframes more important)
        weights = {"1h": 2, "4h": 4, "1d": 6, "1w": 8}
        
        for tf, weight in weights.items():
            if tf in trends:
                trend_data = trends[tf]
                expected_trend = "BULLISH" if direction == "LONG" else "BEARISH"
                
                if trend_data.get("trend") == expected_trend:
                    bonus += weight
                    aligned_count += 1
                elif trend_data.get("trend") == "NEUTRAL":
                    bonus += weight // 2  # Half points for neutral
        
        # Perfect alignment bonus
        if aligned_count == 4:
            bonus += 12
        elif aligned_count >= 3:
            bonus += 6
            
        return min(20, bonus)
    
    def _calculate_volume_confluence(self, df, direction):
        """Analyze volume for professional confirmation"""
        try:
            if 'volume' not in df.columns:
                return 0
                
            bonus = 0
            recent_vol = df['volume'].iloc[-1]
            avg_vol_20 = df['volume'].iloc[-20:].mean()
            avg_vol_5 = df['volume'].iloc[-5:].mean()
            
            # Volume surge confirmation
            vol_ratio = recent_vol / avg_vol_20 if avg_vol_20 > 0 else 1
            if vol_ratio > 2.0:
                bonus += 10
            elif vol_ratio > 1.5:
                bonus += 6
            elif vol_ratio > 1.2:
                bonus += 3
                
            # Volume trend consistency
            if avg_vol_5 > avg_vol_20:
                bonus += 3
                
            # On-Balance Volume confirmation
            if 'obv' in df.columns and len(df) >= 10:
                obv_trend = df['obv'].iloc[-5:].mean() > df['obv'].iloc[-10:-5].mean()
                direction_aligned = (direction == "LONG" and obv_trend) or (direction == "SHORT" and not obv_trend)
                if direction_aligned:
                    bonus += 5
                    
            return min(12, bonus)
            
        except Exception:
            return 0
    
    def _calculate_sr_confluence_professional(self, df, direction, support_levels, resistance_levels):
        """ENHANCED: Professional S/R confluence analysis with volume profile and Fibonacci integration"""
        try:
            if not support_levels and not resistance_levels:
                return 0
                
            bonus = 0
            current_price = df['close'].iloc[-1]
            
            # Get enhanced volume profile data
            volume_profile = self.get_professional_volume_profile(df)
            
            # Get enhanced Fibonacci levels 
            fibonacci_levels = self._calculate_fibonacci_levels(df)
            
            # CRYPTO-ENHANCED: Multi-layer confluence analysis
            confluence_factors = 0
            total_strength = 0
            
            if direction == "LONG" and support_levels:
                # Check proximity to strong support with multiple confluence factors
                for level, strength in support_levels[:5]:  # Check top 5 levels
                    distance_pct = abs(current_price - level) / current_price
                    
                    if distance_pct < 0.025:  # Within 2.5% for crypto volatility
                        # Base support strength
                        level_bonus = min(12, strength // 2)
                        
                        # ENHANCEMENT 1: Volume profile confluence
                        if volume_profile and volume_profile.get('volume_nodes'):
                            for node in volume_profile['volume_nodes'][:5]:
                                node_distance = abs(level - node['price']) / level
                                if node_distance < 0.01:  # Volume node within 1% of S/R level
                                    level_bonus += min(6, int(node['strength']))
                                    confluence_factors += 1
                                    break
                        
                        # ENHANCEMENT 2: Fibonacci confluence
                        for fib_level in fibonacci_levels:
                            fib_distance = abs(level - fib_level) / level
                            if fib_distance < 0.01:  # Fib level within 1% of S/R level
                                level_bonus += 4
                                confluence_factors += 1
                                break
                        
                        # ENHANCEMENT 3: Market structure confluence
                        if volume_profile and volume_profile.get('market_structure'):
                            market_structure = volume_profile['market_structure']
                            if market_structure.get('phase') in ['support_below', 'accumulation_above']:
                                level_bonus += 3
                                confluence_factors += 1
                        
                        # ENHANCEMENT 4: Smart money confluence
                        if hasattr(self, '_last_smart_money_signals'):
                            smart_signals = getattr(self, '_last_smart_money_signals', {})
                            if smart_signals.get('stealth_buying') or smart_signals.get('accumulation'):
                                level_bonus += 5
                                confluence_factors += 1
                        
                        # ENHANCEMENT 5: Distance-based weighting (closer = more important)
                        distance_weight = max(0.5, 1.0 - (distance_pct * 20))  # Closer levels get higher weight
                        level_bonus = int(level_bonus * distance_weight)
                        
                        bonus += level_bonus
                        total_strength += strength
                        
                        # Break after first significant level to avoid over-counting
                        if distance_pct < 0.015:  # Very close level
                            break
                            
            elif direction == "SHORT" and resistance_levels:
                # Check proximity to strong resistance with multiple confluence factors
                for level, strength in resistance_levels[:5]:  # Check top 5 levels
                    distance_pct = abs(current_price - level) / current_price
                    
                    if distance_pct < 0.025:  # Within 2.5%
                        # Base resistance strength
                        level_bonus = min(12, strength // 2)
                        
                        # ENHANCEMENT 1: Volume profile confluence
                        if volume_profile and volume_profile.get('volume_nodes'):
                            for node in volume_profile['volume_nodes'][:5]:
                                node_distance = abs(level - node['price']) / level
                                if node_distance < 0.01:
                                    # Distribution nodes are more bearish
                                    if node.get('node_type') == 'distribution':
                                        level_bonus += min(8, int(node['strength']))
                                    else:
                                        level_bonus += min(6, int(node['strength']))
                                    confluence_factors += 1
                                    break
                        
                        # ENHANCEMENT 2: Fibonacci confluence
                        for fib_level in fibonacci_levels:
                            fib_distance = abs(level - fib_level) / level
                            if fib_distance < 0.01:
                                level_bonus += 4
                                confluence_factors += 1
                                break
                        
                        # ENHANCEMENT 3: Market structure confluence
                        if volume_profile and volume_profile.get('market_structure'):
                            market_structure = volume_profile['market_structure']
                            if market_structure.get('phase') in ['resistance_above', 'distribution_below']:
                                level_bonus += 3
                                confluence_factors += 1
                        
                        # ENHANCEMENT 4: Smart money confluence
                        if hasattr(self, '_last_smart_money_signals'):
                            smart_signals = getattr(self, '_last_smart_money_signals', {})
                            if smart_signals.get('distribution') or smart_signals.get('whale_activity'):
                                level_bonus += 5
                                confluence_factors += 1
                        
                        # ENHANCEMENT 5: Distance-based weighting
                        distance_weight = max(0.5, 1.0 - (distance_pct * 20))
                        level_bonus = int(level_bonus * distance_weight)
                        
                        bonus += level_bonus
                        total_strength += strength
                        
                        # Break after first significant level
                        if distance_pct < 0.015:
                            break
            
            # CRYPTO-SPECIFIC: Multi-confluence bonus
            if confluence_factors >= 3:
                bonus += 8  # Strong multi-factor confluence
            elif confluence_factors >= 2:
                bonus += 4  # Good confluence
            
            # CRYPTO-SPECIFIC: Point of Control (POC) proximity bonus
            if volume_profile and volume_profile.get('poc'):
                poc_distance = abs(current_price - volume_profile['poc']) / current_price
                if poc_distance < 0.02:  # Within 2% of POC
                    poc_strength = volume_profile.get('poc_strength', 1)
                    if poc_strength > 2.0:  # Strong POC
                        bonus += 6
                    elif poc_strength > 1.5:  # Moderate POC
                        bonus += 3
            
            # Cap the bonus to maintain balance
            return min(18, bonus)  # Increased cap for enhanced analysis
            
        except Exception as e:
            print(f"Error in enhanced confluence analysis: {e}")
            return 0
    
    def _calculate_microstructure_quality(self, df):
        """Analyze market microstructure for quality assessment"""
        try:
            bonus = 0
            
            # Price action clarity (trending vs choppy)
            if len(df) >= 20:
                recent_closes = df['close'].iloc[-10:]
                price_trend_quality = abs(recent_closes.iloc[-1] - recent_closes.iloc[0]) / recent_closes.std()
                
                if price_trend_quality > 2:  # Clear directional move
                    bonus += 4
                elif price_trend_quality > 1:
                    bonus += 2
                    
            # Volatility environment assessment
            if 'atr_percent' in df.columns:
                current_vol = df['atr_percent'].iloc[-5:].mean()
                historical_vol = df['atr_percent'].iloc[-50:].mean()
                
                if historical_vol > 0:
                    vol_ratio = current_vol / historical_vol
                    if 0.8 <= vol_ratio <= 1.3:  # Normal volatility regime
                        bonus += 3
                    elif vol_ratio > 2:  # High volatility - reduce confidence
                        bonus -= 4
                        
            return max(-5, min(6, bonus))
            
        except Exception:
            return 0
    
    def _calculate_risk_reward_factor(self, df, direction):
        """Calculate risk-reward optimization factor"""
        try:
            bonus = 0
            current_price = df['close'].iloc[-1]
            
            # ATR-based risk assessment
            if 'atr' in df.columns:
                atr = df['atr'].iloc[-1]
                risk_pct = (atr / current_price) * 100
                
                # Optimal risk range for crypto (1-3%)
                if 1 <= risk_pct <= 3:
                    bonus += 4
                elif 3 < risk_pct <= 5:
                    bonus += 2
                elif risk_pct > 6:  # High risk environment
                    bonus -= 3
                    
            # Position in recent range (better R:R near extremes)
            if len(df) >= 30:
                recent_high = df['high'].iloc[-30:].max()
                recent_low = df['low'].iloc[-30:].min()
                range_size = recent_high - recent_low
                
                if range_size > 0:
                    position_in_range = (current_price - recent_low) / range_size
                    
                    if direction == "LONG" and position_in_range < 0.3:
                        bonus += 3  # Near support, good R:R for longs
                    elif direction == "SHORT" and position_in_range > 0.7:
                        bonus += 3  # Near resistance, good R:R for shorts
                        
            return min(5, bonus)
            
        except Exception:
            return 0
    
    def _calculate_momentum_divergence_factor(self, df, direction):
        """Calculate momentum divergence quality factor"""
        try:
            bonus = 0
            
            # RSI momentum analysis
            if 'rsi14' in df.columns and len(df) >= 10:
                rsi_current = df['rsi14'].iloc[-1]
                rsi_trend = df['rsi14'].iloc[-5:].mean() - df['rsi14'].iloc[-10:-5].mean()
                
                if direction == "LONG":
                    # Look for oversold conditions with momentum shift
                    if rsi_current < 40 and rsi_trend > 0:
                        bonus += 5
                    elif rsi_current < 50 and rsi_trend > 2:
                        bonus += 3
                elif direction == "SHORT":
                    # Look for overbought conditions with momentum shift
                    if rsi_current > 60 and rsi_trend < 0:
                        bonus += 5
                    elif rsi_current > 50 and rsi_trend < -2:
                        bonus += 3
                        
            # MACD momentum confirmation
            if all(col in df.columns for col in ['macd', 'macd_signal', 'macd_hist']):
                macd_hist_current = df['macd_hist'].iloc[-1]
                macd_hist_prev = df['macd_hist'].iloc[-2]
                
                momentum_shift = macd_hist_current > macd_hist_prev
                
                if (direction == "LONG" and momentum_shift) or (direction == "SHORT" and not momentum_shift):
                    bonus += 3
                    
            return min(6, bonus)
            
        except Exception:
            return 0
        volume_score = 0

        # Get volume slices for each part of the pattern
        volume_head = df['volume'].iloc[ls['idx']:rs['idx']].mean()
        volume_rs = df['volume'].iloc[head['idx']:rs['idx']].mean()
        breakout_volume = df['volume'].iloc[-1]
        avg_volume_20d = df['volume'].iloc[-21:-1].mean()

        # Reward for diminishing volume into the right shoulder (waning buying pressure)
        if volume_rs < volume_head * 0.8:
            volume_score += 15

        # Reward heavily for a high-volume breakout
        if breakout_volume > avg_volume_20d * 1.75:
            volume_score += 20

        return volume_score

    def _score_hs_momentum(self, df: pd.DataFrame, recent_df: pd.DataFrame, head: dict, rs: dict, is_inverse: bool) -> int:
        momentum_score = 0
        if 'rsi14' not in df.columns:
            return 0

        # Translate the index from recent_df to the main df before looking up RSI
        main_df_start_index = df.index.get_loc(recent_df.index[0])

        rsi_head = df['rsi14'].iloc[main_df_start_index + head['idx']]
        rsi_rs = df['rsi14'].iloc[main_df_start_index + rs['idx']]

        if not is_inverse: # Standard Head & Shoulders
            # Check for classic bearish divergence
            if rs['price'] <= head['price'] and rsi_rs < rsi_head:
                momentum_score += 20
        else: # Inverse Head & Shoulders
            # Check for classic bullish divergence
            if rs['price'] >= head['price'] and rsi_rs > rsi_head:
                momentum_score += 20

        return momentum_score

    def _calculate_hs_measured_move(self, head_price: float, neckline_price: float, breakout_price: float, direction: str) -> List[float]:
        """
        Calculate the measured move targets for a Head and Shoulders pattern.
        This version explicitly handles LONG vs SHORT to prevent calculation errors.
        """
        height = abs(neckline_price - head_price)

        # For a LONG signal (Inverse H&S), targets must be *above* the breakout price.
        if direction == "LONG":
            # Bullish pattern (Inverse H&S), targets are ADDED to the breakout price.
            return [
                breakout_price + (height * 0.5),
                breakout_price + height,
                breakout_price + (height * 1.618)
            ]

        # For a SHORT signal (Standard H&S), targets must be *below* the breakout price.
        elif direction == "SHORT":
            # Bearish pattern (Standard H&S), targets are SUBTRACTED
            return [
                breakout_price - (height * 0.5),
                breakout_price - height,
                breakout_price - (height * 1.618)
            ]

        # Return an empty list if the direction is not recognized
        else:
            return []


    def calculate_signal_confidence(self, pattern_name: str, direction: str, df: pd.DataFrame,
                                   support_levels: List[Tuple[float, int]] = None,
                                   resistance_levels: List[Tuple[float, int]] = None,
                                   trends: Dict[str, Dict] = None,
                                   base_confidence_override: float = None) -> float:
        """
        Compatibility wrapper for the enhanced confidence calculation
        """
        return self.calculate_enhanced_confidence(
            pattern_name, direction, df, support_levels, resistance_levels, trends, base_confidence_override
        )

    def _analyze_price_gaps(self, df: pd.DataFrame) -> Dict[str, Union[str, float]]:
        """
        Analyzes the recent price action for significant gaps between candles.

        Args:
            df: DataFrame with OHLCV data.

        Returns:
            A dictionary containing the gap type ('up', 'down', or 'none') and a
            score reflecting the gap's significance.
        """
        result = {"gap_type": "none", "score": 0}
        if len(df) < 2:
            return result

        try:
            # Look for a gap between the previous candle and the one before it.
            # A gap is confirmed by the current candle's price action.
            current = df.iloc[-1]
            prev = df.iloc[-2]
            prev2 = df.iloc[-3]

            # --- Gap Up Detection (Bullish) ---
            # A gap up occurs when the previous low is higher than the high of the candle before it.
            is_gap_up = prev['low'] > prev2['high']

            if is_gap_up:
                # Calculate gap size as a percentage of the previous price.
                gap_size_pct = ((prev['low'] - prev2['high']) / prev2['high']) * 100

                # Confirmation: The current candle should continue the upward momentum.
                is_confirmed = current['close'] > prev['high']

                if is_confirmed and gap_size_pct > 0.5: # Require at least a 0.5% gap
                    score = 0
                    # Score based on gap size
                    if gap_size_pct > 3: score += 40 # Very significant gap
                    elif gap_size_pct > 1.5: score += 25
                    else: score += 15

                    # Add score for volume confirmation
                    if 'volume' in df.columns and prev['volume'] > df['volume'].iloc[-10:-2].mean() * 1.5:
                        score += 20

                    result = {"gap_type": "up", "score": min(100, score)}

            # --- Gap Down Detection (Bearish) ---
            # A gap down occurs when the previous high is lower than the low of the candle before it.
            is_gap_down = prev['high'] < prev2['low']

            if is_gap_down:
                # Calculate gap size
                gap_size_pct = ((prev2['low'] - prev['high']) / prev2['low']) * 100

                # Confirmation: The current candle should continue the downward momentum.
                is_confirmed = current['close'] < prev['low']

                if is_confirmed and gap_size_pct > 0.5: # Require at least a 0.5% gap
                    score = 0
                    # Score based on gap size
                    if gap_size_pct > 3: score += 40
                    elif gap_size_pct > 1.5: score += 25
                    else: score += 15

                    # Add score for volume confirmation
                    if 'volume' in df.columns and prev['volume'] > df['volume'].iloc[-10:-2].mean() * 1.5:
                        score += 20

                    result = {"gap_type": "down", "score": min(100, score)}

            return result

        except Exception as e:
            print(f"Error in _analyze_price_gaps: {e}")
            return {"gap_type": "none", "score": 0}

    def run_menu(self):
        """Main menu interface for the program"""
        while True:
            print("\n" + "=" * 80)
            print("CRYPTO TRADING SIGNAL ANALYZER")
            print("=" * 80)
            print("1. Analyze All Symbols (Normal)")
            print("2. Analyze Single Symbol")
            print("3. Check Custom Strategy on Symbol")
            print("4. Update Correlation Data")
            print("5. View Recent Signals")
            print("6. Analyze All Symbols at Past Date/Time")
            print("7. Analyze All Symbols (Smart Money Concepts)")
            print("8. Exit")
            print("-" * 80)

            choice = input("Select an option (1-8): ")
            
            if choice == '1':
                print("\nAnalyzing all symbols. This may take several minutes...\n")
                results = self.analyze_all_symbols()
                if results:
                    print("\nTop signals found:")
                    # Output is already handled in analyze_all_symbols
                else:
                    print("\nNo significant signals detected.")
            
            elif choice == '2':
                symbol = input("\nEnter symbol to analyze (e.g., BTC, ETH): ").upper()

                print("\nSelect analysis type for single symbol:")
                print("1. Normal Analysis")
                print("2. Smart Money Concepts (SMC) Analysis")
                analysis_choice = input("Select an option (1-2): ")

                result = None
                if analysis_choice == '1':
                    print(f"\nPerforming Normal analysis for {symbol}...\n")
                    result = self.analyze_single_symbol(symbol)
                elif analysis_choice == '2':
                    print(f"\nPerforming Smart Money Concepts analysis for {symbol}...\n")
                    result = self.analyze_single_symbol_smc(symbol)
                else:
                    print("Invalid choice.")

                if result:
                    if "error" not in result:
                        print(self.format_signal_output(result))
                        save = input("\nSave this signal to CSV? (y/n): ").lower()
                        if save == 'y':
                            self.save_signal_to_csv(result)
                    else:
                        print(f"Error: {result['error']}")

            elif choice == '3':
                # This functionality remains as is
                symbol = input("\nEnter symbol to analyze (e.g., BTC, ETH): ").upper()
                print("\nSelect strategy type:")
                # ... (rest of the code for choice 3 is unchanged)
            
            elif choice == '4':
                print("\nUpdating correlation data. This may take several minutes...\n")
                self.update_correlations()
            
            elif choice == '5':
                # This functionality remains as is
                # ... (code for choice 5 is unchanged)
                pass # Placeholder to avoid syntax error in this example

            elif choice == '6':
                print("\nSelect analysis type for past date:")
                print("1. Normal Analysis")
                print("2. Smart Money Concepts (SMC) Analysis")
                analysis_choice = input("Select an option (1-2): ")

                past_date_str = input("Enter the past date (YYYY-MM-DD): ")
                past_time_str = input("Enter the past time (HH:MM): ")
                try:
                    past_datetime = datetime.strptime(f"{past_date_str} {past_time_str}", "%Y-%m-%d %H:%M")

                    if analysis_choice == '1':
                        print(f"\nAnalyzing all symbols with Normal analysis at {past_datetime}...\n")
                        self.analyze_all_symbols_at_past_date(past_datetime)
                    elif analysis_choice == '2':
                        print(f"\nAnalyzing all symbols with SMC analysis at {past_datetime}...\n")
                        self.analyze_all_symbols_smc(end_date=past_datetime)
                    else:
                        print("Invalid choice.")
                except ValueError:
                    print("Invalid date/time format. Please use YYYY-MM-DD and HH:MM.")

            elif choice == '7':
                verbose_choice = input("Enable verbose logging for SMC analysis? (y/n): ").lower()
                verbose_enabled = verbose_choice == 'y'

                try:
                    threshold_str = input("Enter minimum quality score threshold (0-100, default is 70): ")
                    score_threshold = int(threshold_str) if threshold_str else 70
                    if not (0 <= score_threshold <= 100):
                        print("Invalid threshold. Using default of 70.")
                        score_threshold = 70
                except ValueError:
                    print("Invalid input. Using default threshold of 70.")
                    score_threshold = 70

                print("\nAnalyzing all symbols with Smart Money Concepts. This may take several minutes...\n")
                self.analyze_all_symbols_smc(score_threshold=score_threshold, verbose=verbose_enabled)

            elif choice == '8':
                print("\nExiting program. Goodbye!")
                break
            
            else:
                print("\nInvalid choice. Please try again.")


if __name__ == "__main__":
    # Initialize the analyzer
    analyzer = CryptoTradeAnalyzer()

    # Run the main menu
    analyzer.run_menu()